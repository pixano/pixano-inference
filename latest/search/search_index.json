{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#documentation-of-pixano-inference","title":"Documentation of Pixano-Inference","text":""},{"location":"#mkdocs","title":"Mkdocs","text":"<p>Pixano-Inference documentation relies on MkDocs which offers a great set of features to generate documentations using Markdown.</p>"},{"location":"#locally-serve","title":"Locally serve","text":"<p>To serve locally the doc you need to install the dependencies:</p> <pre><code>cd pixano-inference\n\npip install \".[docs]\"\n</code></pre> <p>Then you can serve it using MkDocs:</p> <pre><code>mkdocs serve -a localhost:8000\n</code></pre> <p>It will listen to modifications in the <code>docs/</code> folder and the <code>mkdocs.yml</code> configuration file.</p>"},{"location":"api_reference/","title":"index","text":""},{"location":"api_reference/#pixano-inference-api-reference","title":"Pixano Inference API reference","text":""},{"location":"api_reference/#client-module","title":"Client module","text":"<p>The client module contains the class for the API client. It is responsible for making requests to the API endpoints.</p>"},{"location":"api_reference/#data-module","title":"Data module","text":"<p>The data module contains the functions to read (and later write) data from/to a database or file.</p>"},{"location":"api_reference/#model-registry-module","title":"Model registry module","text":"<p>The model registry module contains the functions to register a model to the application.</p>"},{"location":"api_reference/#models-module","title":"Models module","text":"<p>The models module contains the inference models to perform the tasks Pixano Inference API is designed for.</p> <p>The models include:</p> <ul> <li><code>BaseInferenceModel</code>: Base class for all Pixano Inference API models.</li> <li><code>Sam2Model</code>: Model used to detect and segment objects in images and videos.</li> <li><code>TransformerModel</code>: Model instantiated from Transformers.</li> <li><code>VLLMModel</code>: Model instantiated from VLLM.</li> </ul>"},{"location":"api_reference/#providers-module","title":"Providers module","text":"<p>The providers module contains the functions to load a model and to perform inference either from a model provider or from an API provider.</p> <p>The providers include:</p> <ul> <li><code>BaseProvider</code>: Base class for all Pixano Inference API providers.</li> <li><code>Sam2Provider</code>: Provider used to instantiate a <code>Sam2Model</code> and call its methods.</li> <li><code>TransformersProvider</code>: Provider used to instantiate a <code>TransformerModel</code> and call its methods.</li> <li><code>VLLMProvider</code>: Provider used to instantiate a <code>VLLMModel</code> and call its methods.</li> </ul>"},{"location":"api_reference/#pydantic-module","title":"Pydantic module","text":"<p>The pydantic module contains the classes for data validation. It is used by the models, providers, and the application itself to validate the input/output of the API.</p>"},{"location":"api_reference/#routers-module","title":"Routers module","text":"<p>The routers module contains the routers for the API. Each router has a path prefix that defines the endpoint where it will be mounted in the API.</p> <p>The routers swagger is accessible at at the <code>/docs</code> endpoint.</p>"},{"location":"api_reference/#settings-module","title":"Settings module","text":"<p>The settings module contains the configuration of the application.</p>"},{"location":"api_reference/#tasks-module","title":"Tasks module","text":"<p>The tasks module contains the enums used to define the task that a model can perform.</p>"},{"location":"api_reference/#utils-module","title":"Utils module","text":"<p>The utils module contains the functions and classes used by the other modules.</p>"},{"location":"api_reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>index</li> <li>client</li> <li>data<ul> <li>read_vector_databases</li> </ul> </li> <li>main</li> <li>models<ul> <li>base</li> <li>sam2</li> <li>transformers</li> <li>vllm</li> </ul> </li> <li>models_registry</li> <li>providers<ul> <li>base</li> <li>registry</li> <li>sam2</li> <li>transformers</li> <li>vllm</li> </ul> </li> <li>pydantic<ul> <li>base</li> <li>data<ul> <li>vector_database</li> </ul> </li> <li>models</li> <li>nd_array</li> <li>tasks<ul> <li>image<ul> <li>mask_generation</li> <li>utils</li> </ul> </li> <li>multimodal<ul> <li>conditional_generation</li> </ul> </li> <li>video<ul> <li>mask_generation</li> </ul> </li> </ul> </li> </ul> </li> <li>routers<ul> <li>app</li> <li>image</li> <li>multimodal</li> <li>nlp</li> <li>providers</li> <li>utils</li> <li>video</li> </ul> </li> <li>settings</li> <li>tasks<ul> <li>image</li> <li>multimodal</li> <li>nlp</li> <li>task</li> <li>utils</li> <li>video</li> </ul> </li> <li>utils<ul> <li>image</li> <li>package</li> <li>url</li> <li>vector</li> </ul> </li> </ul>"},{"location":"api_reference/client/","title":"client","text":""},{"location":"api_reference/client/#pixano_inference.client","title":"<code>pixano_inference.client</code>","text":"<p>Pixano inference client.</p>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient","title":"<code>PixanoInferenceClient(**data)</code>","text":"<p>               Bases: <code>Settings</code></p> <p>Pixano Inference Client.</p> Source code in <code>pixano_inference/settings.py</code> <pre><code>def __init__(self, **data: Any):\n    \"\"\"Initialize the settings.\"\"\"\n    if \"num_cpus\" not in data:\n        data[\"num_cpus\"] = os.cpu_count()\n    if \"num_gpus\" not in data:\n        if is_torch_installed():\n            if torch.cuda.is_available():\n                data[\"num_gpus\"] = torch.cuda.device_count()\n            else:\n                data[\"num_gpus\"] = 0\n        else:\n            data[\"num_gpus\"] = 0\n\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient._rest_call","title":"<code>_rest_call(path, method, **kwargs)</code>","text":"<p>Perform a REST call to the pixano inference server.</p> Source code in <code>pixano_inference/client.py</code> <pre><code>def _rest_call(self, path: str, method: Literal[\"GET\", \"POST\", \"PUT\", \"DELETE\"], **kwargs) -&gt; Response:\n    \"\"\"Perform a REST call to the pixano inference server.\"\"\"\n    match method:\n        case \"GET\":\n            request_fn = requests.get\n        case \"POST\":\n            request_fn = requests.post\n        case \"PUT\":\n            request_fn = requests.put\n        case \"DELETE\":\n            request_fn = requests.delete\n        case _:\n            raise ValueError(\n                f\"Invalid REST call method. Expected one of ['GET', 'POST', 'PUT', 'DELETE'], but got '{method}'.\"\n            )\n\n    url = f\"{self.url}/{path}\"\n    response = request_fn(url, **kwargs)\n    raise_if_error(response)\n\n    return response\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.connect","title":"<code>connect(url)</code>  <code>staticmethod</code>","text":"<p>Connect to pixano inference.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the pixano inference server.</p> required Source code in <code>pixano_inference/client.py</code> <pre><code>@staticmethod\ndef connect(url: str) -&gt; \"PixanoInferenceClient\":\n    \"\"\"Connect to pixano inference.\n\n    Args:\n        url: The URL of the pixano inference server.\n    \"\"\"\n    settings = Settings.model_validate(requests.get(f\"{url}/app/settings\").json())\n    client = PixanoInferenceClient(url=url, **settings.model_dump())\n    return client\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.delete","title":"<code>delete(path)</code>","text":"<p>Perform a DELETE request to the pixano inference server.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the request.</p> required Source code in <code>pixano_inference/client.py</code> <pre><code>def delete(self, path: str) -&gt; Response:\n    \"\"\"Perform a DELETE request to the pixano inference server.\n\n    Args:\n        path: The path of the request.\n    \"\"\"\n    return self._rest_call(path=path, method=\"DELETE\")\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.delete_model","title":"<code>delete_model(model_name)</code>","text":"<p>Delete a model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model.</p> required Source code in <code>pixano_inference/client.py</code> <pre><code>def delete_model(self, model_name: str) -&gt; None:\n    \"\"\"Delete a model.\n\n    Args:\n        model_name: The name of the model.\n    \"\"\"\n    self.delete(f\"providers/model/{model_name}\")\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.get","title":"<code>get(path)</code>","text":"<p>Perform a GET request to the pixano inference server.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the request.</p> required Source code in <code>pixano_inference/client.py</code> <pre><code>def get(self, path: str) -&gt; Response:\n    \"\"\"Perform a GET request to the pixano inference server.\n\n    Args:\n        path: The path of the request.\n    \"\"\"\n    return self._rest_call(path=path, method=\"GET\")\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.get_settings","title":"<code>get_settings()</code>","text":"<p>Get the settings for the pixano inference server.</p> Source code in <code>pixano_inference/client.py</code> <pre><code>def get_settings(self) -&gt; Settings:\n    \"\"\"Get the settings for the pixano inference server.\"\"\"\n    response = self.get(\"app/settings\")\n    raise_if_error(response)\n    return Settings(**response.json())\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.image_mask_generation","title":"<code>image_mask_generation(request)</code>","text":"<p>Perform an inference to perform image mask generation.</p> Source code in <code>pixano_inference/client.py</code> <pre><code>def image_mask_generation(self, request: ImageMaskGenerationRequest) -&gt; ImageMaskGenerationResponse:\n    \"\"\"Perform an inference to perform image mask generation.\"\"\"\n    return self.inference(\n        url=\"/tasks/image/mask_generation\", request=request, response_type=ImageMaskGenerationResponse\n    )\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.inference","title":"<code>inference(url, request, response_type)</code>","text":"<p>Perform a POST request to the pixano inference server.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The path of the request.</p> required <code>request</code> <code>BaseRequest</code> <p>The request of the model.</p> required <code>response_type</code> <code>type[BaseResponse]</code> <p>The type of the response.</p> required <p>Returns:</p> Type Description <code>BaseResponse</code> <p>A response from the pixano inference server.</p> Source code in <code>pixano_inference/client.py</code> <pre><code>def inference(self, url: str, request: BaseRequest, response_type: type[BaseResponse]) -&gt; BaseResponse:\n    \"\"\"Perform a POST request to the pixano inference server.\n\n    Args:\n        url: The path of the request.\n        request: The request of the model.\n        response_type: The type of the response.\n\n    Returns:\n        A response from the pixano inference server.\n    \"\"\"\n    return response_type.model_validate(self.post(url, json=request.model_dump()).json())\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.instantiate_model","title":"<code>instantiate_model(provider, config)</code>","text":"<p>Instantiate a model.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>The model provider.</p> required <code>config</code> <code>ModelConfig</code> <p>The configuration of the model.</p> required Source code in <code>pixano_inference/client.py</code> <pre><code>def instantiate_model(self, provider: str, config: ModelConfig) -&gt; None:\n    \"\"\"Instantiate a model.\n\n    Args:\n        provider: The model provider.\n        config: The configuration of the model.\n    \"\"\"\n    json_content = jsonable_encoder({\"provider\": provider, \"config\": config})\n    self.post(\"providers/instantiate\", json=json_content)\n    return\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.list_models","title":"<code>list_models()</code>","text":"<p>List all models.</p> Source code in <code>pixano_inference/client.py</code> <pre><code>def list_models(self) -&gt; list[ModelInfo]:\n    \"\"\"List all models.\"\"\"\n    response = self.get(\"app/models\")\n    return [ModelInfo.model_validate(model) for model in response.json()]\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.post","title":"<code>post(path, **kwargs)</code>","text":"<p>Perform a POST request to the pixano inference server.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the request.</p> required <code>kwargs</code> <code>Any</code> <p>The keyword arguments to pass to the request.</p> <code>{}</code> Source code in <code>pixano_inference/client.py</code> <pre><code>def post(self, path: str, **kwargs: Any) -&gt; Response:\n    \"\"\"Perform a POST request to the pixano inference server.\n\n    Args:\n        path: The path of the request.\n        kwargs: The keyword arguments to pass to the request.\n    \"\"\"\n    return self._rest_call(path=path, method=\"POST\", **kwargs)\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.put","title":"<code>put(path, **kwargs)</code>","text":"<p>Perform a PUT request to the pixano inference server.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the request.</p> required <code>kwargs</code> <code>Any</code> <p>The keyword arguments to pass to the request.</p> <code>{}</code> Source code in <code>pixano_inference/client.py</code> <pre><code>def put(self, path: str, **kwargs: Any) -&gt; Response:\n    \"\"\"Perform a PUT request to the pixano inference server.\n\n    Args:\n        path: The path of the request.\n        kwargs: The keyword arguments to pass to the request.\n    \"\"\"\n    return self._rest_call(path=path, method=\"PUT\", **kwargs)\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.text_image_conditional_generation","title":"<code>text_image_conditional_generation(request)</code>","text":"<p>Perform an inference to perform text-image conditional generation.</p> Source code in <code>pixano_inference/client.py</code> <pre><code>def text_image_conditional_generation(\n    self, request: TextImageConditionalGenerationRequest\n) -&gt; TextImageConditionalGenerationResponse:\n    \"\"\"Perform an inference to perform text-image conditional generation.\"\"\"\n    return self.inference(\n        url=\"/tasks/multimodal/image-text/conditional_generation/\",\n        request=request,\n        response_type=TextImageConditionalGenerationResponse,\n    )\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.video_mask_generation","title":"<code>video_mask_generation(request)</code>","text":"<p>Perform an inference to perform video mask generation.</p> Source code in <code>pixano_inference/client.py</code> <pre><code>def video_mask_generation(self, request: VideoMaskGenerationRequest) -&gt; VideoMaskGenerationResponse:\n    \"\"\"Perform an inference to perform video mask generation.\"\"\"\n    return self.inference(\n        url=\"/tasks/video/mask_generation\", request=request, response_type=VideoMaskGenerationResponse\n    )\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.raise_if_error","title":"<code>raise_if_error(response)</code>","text":"<p>Raise an error from a response.</p> Source code in <code>pixano_inference/client.py</code> <pre><code>def raise_if_error(response: Response) -&gt; None:\n    \"\"\"Raise an error from a response.\"\"\"\n    if response.ok:\n        return\n    error_out = f\"HTTP {response.status_code}: {response.reason}\"\n    try:\n        json_detail = response.json()\n    except Exception:\n        json_detail = {}\n\n    detail = json_detail.get(\"detail\", None)\n    if detail is not None:\n        error_out += f\" - {detail}\"\n    error = json_detail.get(\"error\", None)\n    if error is not None:\n        error_out += f\" - {error}\"\n    raise HTTPException(response.status_code, detail=error_out)\n</code></pre>"},{"location":"api_reference/main/","title":"main","text":""},{"location":"api_reference/main/#pixano_inference.main","title":"<code>pixano_inference.main</code>","text":"<p>Main application entry point.</p>"},{"location":"api_reference/main/#pixano_inference.main.create_app","title":"<code>create_app()</code>","text":"<p>Create the FastAPI application.</p> Source code in <code>pixano_inference/main.py</code> <pre><code>def create_app() -&gt; FastAPI:\n    \"\"\"Create the FastAPI application.\"\"\"\n    app = FastAPI()\n    app.include_router(routers.image.router)\n    app.include_router(routers.nlp.router)\n    app.include_router(routers.multimodal.router)\n    app.include_router(routers.providers.router)\n    app.include_router(routers.video.router)\n    app.include_router(routers.app.router)\n    return app\n</code></pre>"},{"location":"api_reference/main/#pixano_inference.main.serve","title":"<code>serve(host, port)</code>","text":"<p>Main application entry point.</p> Source code in <code>pixano_inference/main.py</code> <pre><code>@click.command(context_settings={\"auto_envvar_prefix\": \"UVICORN\"})\n@click.option(\n    \"--host\",\n    type=str,\n    default=\"127.0.0.1\",\n    help=\"Pixano Inference app URL host\",\n    show_default=True,\n)\n@click.option(\n    \"--port\",\n    type=int,\n    default=80,\n    help=\"Pixano Inference app URL port\",\n    show_default=True,\n)\ndef serve(host: str, port: int):\n    \"\"\"Main application entry point.\"\"\"\n    app = create_app()\n    config = uvicorn.Config(app, host=host, port=port)\n    server = uvicorn.Server(config)\n    server.run()\n</code></pre>"},{"location":"api_reference/models_registry/","title":"models_registry","text":""},{"location":"api_reference/models_registry/#pixano_inference.models_registry","title":"<code>pixano_inference.models_registry</code>","text":"<p>Registry for inference models.</p>"},{"location":"api_reference/models_registry/#pixano_inference.models_registry.get_model_from_registry","title":"<code>get_model_from_registry(name)</code>","text":"<p>Get a model from the registry.</p> Source code in <code>pixano_inference/models_registry.py</code> <pre><code>def get_model_from_registry(name: str) -&gt; BaseInferenceModel:\n    \"\"\"Get a model from the registry.\"\"\"\n    if name not in REGISTERED_MODELS:\n        raise KeyError(f\"Model {name} is not registered.\")\n    return REGISTERED_MODELS[name][0]\n</code></pre>"},{"location":"api_reference/models_registry/#pixano_inference.models_registry.get_model_provider_from_registry","title":"<code>get_model_provider_from_registry(name)</code>","text":"<p>Get the provider of a model from the registry.</p> Source code in <code>pixano_inference/models_registry.py</code> <pre><code>def get_model_provider_from_registry(name: str) -&gt; BaseProvider:\n    \"\"\"Get the provider of a model from the registry.\"\"\"\n    if name not in REGISTERED_MODELS:\n        raise KeyError(f\"Model {name} is not registered.\")\n    return REGISTERED_MODELS[name][1]\n</code></pre>"},{"location":"api_reference/models_registry/#pixano_inference.models_registry.get_model_task_from_registry","title":"<code>get_model_task_from_registry(name)</code>","text":"<p>Get the task of a model from the registry.</p> Source code in <code>pixano_inference/models_registry.py</code> <pre><code>def get_model_task_from_registry(name: str) -&gt; Task:\n    \"\"\"Get the task of a model from the registry.\"\"\"\n    if name not in REGISTERED_MODELS:\n        raise KeyError(f\"Model {name} is not registered.\")\n    return REGISTERED_MODELS[name][2]\n</code></pre>"},{"location":"api_reference/models_registry/#pixano_inference.models_registry.get_values_from_model_registry","title":"<code>get_values_from_model_registry(name)</code>","text":"<p>Get the values of a model from the registry.</p> Source code in <code>pixano_inference/models_registry.py</code> <pre><code>def get_values_from_model_registry(name: str) -&gt; tuple[BaseInferenceModel, BaseProvider, Task]:\n    \"\"\"Get the values of a model from the registry.\"\"\"\n    if name not in REGISTERED_MODELS:\n        raise KeyError(f\"Model {name} is not registered.\")\n    return REGISTERED_MODELS[name]\n</code></pre>"},{"location":"api_reference/models_registry/#pixano_inference.models_registry.list_models","title":"<code>list_models()</code>","text":"<p>List all models in the registry.</p> Source code in <code>pixano_inference/models_registry.py</code> <pre><code>def list_models() -&gt; list[tuple[str, tuple[BaseInferenceModel, BaseProvider, Task]]]:\n    \"\"\"List all models in the registry.\"\"\"\n    return list(REGISTERED_MODELS.items())\n</code></pre>"},{"location":"api_reference/models_registry/#pixano_inference.models_registry.register_model","title":"<code>register_model(model, provider, task)</code>","text":"<p>Register a model in the registry.</p> Source code in <code>pixano_inference/models_registry.py</code> <pre><code>def register_model(model: BaseInferenceModel, provider: BaseProvider, task: Task | str):\n    \"\"\"Register a model in the registry.\"\"\"\n    if model.name in REGISTERED_MODELS:\n        raise ValueError(f\"Model {model.name} is already registered.\")\n    task = task if isinstance(task, Task) else str_to_task(task)\n    REGISTERED_MODELS[model.name] = (model, provider, task)\n</code></pre>"},{"location":"api_reference/models_registry/#pixano_inference.models_registry.unregister_model","title":"<code>unregister_model(model)</code>","text":"<p>Unregister a model from the registry.</p> Source code in <code>pixano_inference/models_registry.py</code> <pre><code>def unregister_model(model: BaseInferenceModel):\n    \"\"\"Unregister a model from the registry.\"\"\"\n    if model.name not in REGISTERED_MODELS:\n        raise ValueError(f\"Model {model.name} is not registered.\")\n    del REGISTERED_MODELS[model.name]\n</code></pre>"},{"location":"api_reference/settings/","title":"settings","text":""},{"location":"api_reference/settings/#pixano_inference.settings","title":"<code>pixano_inference.settings</code>","text":"<p>Settings for the Pixano Inference API.</p>"},{"location":"api_reference/settings/#pixano_inference.settings.Settings","title":"<code>Settings(**data)</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Application settings.</p> <p>Attributes:</p> Name Type Description <code>app_name</code> <code>str</code> <p>The name of the application.</p> <code>app_version</code> <code>str</code> <p>The version of the application.</p> <code>app_description</code> <code>str</code> <p>A description of the application.</p> <code>num_cpus</code> <code>int</code> <p>The number of CPUs accessible to the application.</p> <code>num_gpus</code> <code>int</code> <p>The number of GPUs available for inference.</p> <code>num_nodes</code> <code>int</code> <p>The number of nodes available for inference.</p> <code>gpus_used</code> <code>list[int]</code> <p>The list of GPUs used by the application.</p> Source code in <code>pixano_inference/settings.py</code> <pre><code>def __init__(self, **data: Any):\n    \"\"\"Initialize the settings.\"\"\"\n    if \"num_cpus\" not in data:\n        data[\"num_cpus\"] = os.cpu_count()\n    if \"num_gpus\" not in data:\n        if is_torch_installed():\n            if torch.cuda.is_available():\n                data[\"num_gpus\"] = torch.cuda.device_count()\n            else:\n                data[\"num_gpus\"] = 0\n        else:\n            data[\"num_gpus\"] = 0\n\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/settings/#pixano_inference.settings.Settings.gpus_available","title":"<code>gpus_available</code>  <code>property</code>","text":"<p>Return the available GPUs.</p>"},{"location":"api_reference/settings/#pixano_inference.settings.get_pixano_inference_settings","title":"<code>get_pixano_inference_settings()</code>","text":"<p>Return the settings.</p> Source code in <code>pixano_inference/settings.py</code> <pre><code>def get_pixano_inference_settings() -&gt; Settings:\n    \"\"\"Return the settings.\"\"\"\n    return PIXANO_INFERENCE_SETTINGS\n</code></pre>"},{"location":"api_reference/data/read_vector_databases/","title":"read_vector_databases","text":""},{"location":"api_reference/data/read_vector_databases/#pixano_inference.data.read_vector_databases","title":"<code>pixano_inference.data.read_vector_databases</code>","text":"<p>This module contains functions for reading data from Vector databases.</p>"},{"location":"api_reference/data/read_vector_databases/#pixano_inference.data.read_vector_databases.read_lance_vector","title":"<code>read_lance_vector(path, column, indice=None, where=None, shape=None, return_type='tensor')</code>","text":"<p>Reads a vector from a Lance dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the Lance dataset.</p> required <code>column</code> <code>str</code> <p>The vector column to read.</p> required <code>indice</code> <code>int | None</code> <p>The index of the row to read.</p> <code>None</code> <code>where</code> <code>str | None</code> <p>The filter to apply. If specified, only one row should be returned.</p> <code>None</code> <code>shape</code> <code>list[int] | None</code> <p>The shape of the vector.</p> <code>None</code> <code>return_type</code> <code>Literal['tensor', 'numpy']</code> <p>The type of the return value. Either 'tensor' or 'numpy'.</p> <code>'tensor'</code> <p>Returns:</p> Type Description <code>'Tensor' | ndarray</code> <p>The value of the column.</p> Source code in <code>pixano_inference/data/read_vector_databases.py</code> <pre><code>def read_lance_vector(\n    path: Path | str,\n    column: str,\n    indice: int | None = None,\n    where: str | None = None,\n    shape: list[int] | None = None,\n    return_type: Literal[\"tensor\", \"numpy\"] = \"tensor\",\n) -&gt; \"Tensor\" | np.ndarray:\n    \"\"\"Reads a vector from a Lance dataset.\n\n    Args:\n        path: The path to the Lance dataset.\n        column: The vector column to read.\n        indice: The index of the row to read.\n        where: The filter to apply. If specified, only one row should be returned.\n        shape: The shape of the vector.\n        return_type: The type of the return value. Either 'tensor' or 'numpy'.\n\n    Returns:\n        The value of the column.\n    \"\"\"\n    assert_lance_installed()\n    if return_type not in [\"tensor\", \"numpy\"]:\n        raise ValueError(\"return_type must be either 'tensor' or 'numpy'.\")\n    elif return_type == \"tensor\":\n        assert_torch_installed()\n    if indice is not None and where is not None:\n        raise ValueError(\"Only one of 'index' and 'where' can be specified.\")\n    elif indice is None and where is None:\n        raise ValueError(\"One of 'index' and 'where' must be specified.\")\n    elif indice is not None and not isinstance(indice, int):\n        raise ValueError(\"index must be an integer.\")\n    elif where is not None and not isinstance(where, str):\n        raise ValueError(\"where must be a string.\")\n    elif not isinstance(path, (Path, str)):\n        raise ValueError(\"lance_path must be a Path or a string.\")\n    elif shape is not None and not isinstance(shape, list) and not all(isinstance(i, int) for i in shape):\n        raise ValueError(\"shape must be a list of integers.\")\n\n    if isinstance(path, str):\n        path = Path(path)\n\n    lance_dataset = LanceDataset(path)\n    if indice is not None:\n        pa_table = lance_dataset.take(indices=[indice], columns=[column])\n    else:\n        pa_table = lance_dataset.scanner(columns=[column], filter=where).to_table()\n    polar_df = pl.DataFrame(pa_table)\n    if polar_df.shape[0] != 1:\n        raise ValueError(f\"Expected one row to be returned but found {polar_df.shape[0]} rows.\")\n    vector: np.ndarray = polar_df[column].to_numpy()\n    vector = vector.reshape(shape)\n    if return_type == \"tensor\":\n        return torch.from_numpy(vector)\n    return vector\n</code></pre>"},{"location":"api_reference/models/base/","title":"base","text":""},{"location":"api_reference/models/base/#pixano_inference.models.base","title":"<code>pixano_inference.models.base</code>","text":"<p>Base class for inference models.</p>"},{"location":"api_reference/models/base/#pixano_inference.models.base.BaseInferenceModel","title":"<code>BaseInferenceModel(name, provider)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for inference models.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>provider</code> <code>str</code> <p>Provider of the model.</p> required Source code in <code>pixano_inference/models/base.py</code> <pre><code>def __init__(self, name: str, provider: str):\n    \"\"\"Initialize the model.\n\n    Args:\n        name: Name of the model.\n        provider: Provider of the model.\n    \"\"\"\n    self.name = name\n    self.provider = provider\n    self._status = ModelStatus.IDLE\n</code></pre>"},{"location":"api_reference/models/base/#pixano_inference.models.base.BaseInferenceModel.metadata","title":"<code>metadata</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return the metadata of the model.</p>"},{"location":"api_reference/models/base/#pixano_inference.models.base.BaseInferenceModel.status","title":"<code>status</code>  <code>property</code> <code>writable</code>","text":"<p>Get the status of the model.</p>"},{"location":"api_reference/models/base/#pixano_inference.models.base.BaseInferenceModel.delete","title":"<code>delete()</code>  <code>abstractmethod</code>","text":"<p>Delete the model.</p> Source code in <code>pixano_inference/models/base.py</code> <pre><code>@abstractmethod\ndef delete(self):\n    \"\"\"Delete the model.\"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/base/#pixano_inference.models.base.BaseInferenceModel.image_mask_generation","title":"<code>image_mask_generation(*args, **kwargs)</code>","text":"<p>Generate a mask from the image.</p> Source code in <code>pixano_inference/models/base.py</code> <pre><code>def image_mask_generation(self, *args: Any, **kwargs) -&gt; ImageMaskGenerationOutput:\n    \"\"\"Generate a mask from the image.\"\"\"\n    raise NotImplementedError(\"This model does not support image mask generation.\")\n</code></pre>"},{"location":"api_reference/models/base/#pixano_inference.models.base.BaseInferenceModel.text_image_conditional_generation","title":"<code>text_image_conditional_generation(*args, **kwargs)</code>","text":"<p>Generate text from an image and a prompt.</p> Source code in <code>pixano_inference/models/base.py</code> <pre><code>def text_image_conditional_generation(self, *args: Any, **kwargs) -&gt; TextImageConditionalGenerationOutput:\n    \"\"\"Generate text from an image and a prompt.\"\"\"\n    raise NotImplementedError(\"This model does not support text-image conditional generation.\")\n</code></pre>"},{"location":"api_reference/models/base/#pixano_inference.models.base.BaseInferenceModel.video_mask_generation","title":"<code>video_mask_generation(*args, **kwargs)</code>","text":"<p>Generate a mask from the video.</p> Source code in <code>pixano_inference/models/base.py</code> <pre><code>def video_mask_generation(self, *args: Any, **kwargs) -&gt; VideoMaskGenerationOutput:\n    \"\"\"Generate a mask from the video.\"\"\"\n    raise NotImplementedError(\"This model does not support video mask generation.\")\n</code></pre>"},{"location":"api_reference/models/base/#pixano_inference.models.base.ModelStatus","title":"<code>ModelStatus</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Current status of the model.</p> <p>Attributes: - IDLE: waiting for an input. - RUNNING: computing.</p>"},{"location":"api_reference/models/sam2/","title":"sam2","text":""},{"location":"api_reference/models/sam2/#pixano_inference.models.sam2","title":"<code>pixano_inference.models.sam2</code>","text":"<p>Inference model for the SAM2 model.</p>"},{"location":"api_reference/models/sam2/#pixano_inference.models.sam2.Sam2Model","title":"<code>Sam2Model(name, provider, predictor, torch_dtype='bfloat16', config={})</code>","text":"<p>               Bases: <code>BaseInferenceModel</code></p> <p>Inference model for the SAM2 model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>provider</code> <code>str</code> <p>Provider of the model.</p> required <code>predictor</code> <code>Any</code> <p>The SAM2 image predictor.</p> required <code>torch_dtype</code> <code>Literal['float32', 'float16', 'bfloat16']</code> <p>The torch data type to use for inference.</p> <code>'bfloat16'</code> <code>config</code> <code>dict[str, Any]</code> <p>Configuration for the model.</p> <code>{}</code> Source code in <code>pixano_inference/models/sam2.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    provider: str,\n    predictor: Any,\n    torch_dtype: Literal[\"float32\", \"float16\", \"bfloat16\"] = \"bfloat16\",\n    config: dict[str, Any] = {},\n):\n    \"\"\"Initialize the model.\n\n    Args:\n        name: Name of the model.\n        provider: Provider of the model.\n        predictor: The SAM2 image predictor.\n        torch_dtype: The torch data type to use for inference.\n        config: Configuration for the model.\n    \"\"\"\n    assert_sam2_installed()\n\n    super().__init__(name, provider)\n    match torch_dtype:\n        case \"float32\":\n            self.torch_dtype = torch.float32\n        case \"float16\":\n            self.torch_dtype = torch.float16\n        case \"bfloat16\":\n            self.torch_dtype = torch.bfloat16\n        case _:\n            raise ValueError(f\"Invalid torch_dtype: {torch_dtype}\")\n    self.predictor: SAM2ImagePredictor | SAM2VideoPredictor = predictor\n    self.config = config\n</code></pre>"},{"location":"api_reference/models/sam2/#pixano_inference.models.sam2.Sam2Model.metadata","title":"<code>metadata</code>  <code>property</code>","text":"<p>Return the metadata of the model.</p>"},{"location":"api_reference/models/sam2/#pixano_inference.models.sam2.Sam2Model.delete","title":"<code>delete()</code>","text":"<p>Delete the model.</p> Source code in <code>pixano_inference/models/sam2.py</code> <pre><code>def delete(self) -&gt; None:\n    \"\"\"Delete the model.\"\"\"\n    del self.predictor\n    unregister_model(self)\n    gc.collect()\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"api_reference/models/sam2/#pixano_inference.models.sam2.Sam2Model.image_mask_generation","title":"<code>image_mask_generation(image, points, labels, boxes, multimask_output=True, num_multimask_outputs=3, return_image_embedding=False, **kwargs)</code>","text":"<p>Generate masks from the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray | Image</code> <p>Image for the generation.</p> required <code>points</code> <code>list[list[list[int]]] | None</code> <p>Points for the mask generation. The first dimension is the number of prompts, the second the number of points per mask and the third the coordinates of the points.</p> required <code>labels</code> <code>list[list[int]] | None</code> <p>Labels for the mask generation. The first dimension is the number of prompts, the second the number of labels per mask.</p> required <code>boxes</code> <code>list[list[int]] | None</code> <p>Boxes for the mask generation. The first dimension is the number of prompts, the second the coordinates of the boxes.</p> required <code>multimask_output</code> <code>bool</code> <p>Whether to generate multiple masks per prediction.</p> <code>True</code> <code>num_multimask_outputs</code> <code>int</code> <p>Number of masks to generate per prediction.</p> <code>3</code> <code>return_image_embedding</code> <code>bool</code> <p>Whether to return the image embedding and high-resolution features.</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>pixano_inference/models/sam2.py</code> <pre><code>def image_mask_generation(\n    self,\n    image: np.ndarray | Image,\n    points: list[list[list[int]]] | None,\n    labels: list[list[int]] | None,\n    boxes: list[list[int]] | None,\n    multimask_output: bool = True,\n    num_multimask_outputs: int = 3,\n    return_image_embedding: bool = False,\n    **kwargs: Any,\n) -&gt; ImageMaskGenerationOutput:\n    \"\"\"Generate masks from the image.\n\n    Args:\n        image: Image for the generation.\n        points: Points for the mask generation. The first dimension is the number of prompts, the\n            second the number of points per mask and the third the coordinates of the points.\n        labels: Labels for the mask generation. The first dimension is the number of prompts, the second\n            the number of labels per mask.\n        boxes: Boxes for the mask generation. The first dimension is the number of prompts, the second\n            the coordinates of the boxes.\n        multimask_output: Whether to generate multiple masks per prediction.\n        num_multimask_outputs: Number of masks to generate per prediction.\n        return_image_embedding: Whether to return the image embedding and high-resolution features.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    # Check the input list types\n    with torch.inference_mode():\n        if not isinstance(image, (np.ndarray, Image)):\n            raise ValueError(\"The image should be an numpy array or a PIL image.\")\n        if (\n            points is not None\n            and not isinstance(points, list)\n            and not all(isinstance(point, list) for point in points)\n        ):\n            raise ValueError(\"The points should be a list of lists.\")\n        if (\n            labels is not None\n            and not isinstance(labels, list)\n            and not all(isinstance(label, list) for label in labels)\n        ):\n            raise ValueError(\"The labels should be a list of lists.\")\n        if boxes is not None and not isinstance(boxes, list):\n            if not all(isinstance(box, list) for box in boxes):\n                raise ValueError(\"The boxes should be a list of lists.\")\n\n        if multimask_output and not (num_multimask_outputs == 3):\n            raise ValueError(\"The number of multimask outputs is not configurable for Sam2 and must be 3.\")\n\n        # Check the input batch size\n        if points is None and labels is not None:\n            raise ValueError(\"Labels are not supported without points.\")\n        if points is not None and labels is not None:\n            if len(points) != len(labels):\n                raise ValueError(\"The number of points and labels should match.\")\n        if points is not None and boxes is not None:\n            if len(points) != len(boxes):\n                raise ValueError(\"The number of points and boxes should match.\")\n\n        # Check the input shapes and value types\n        if points is not None:\n            for prompt_point in points:\n                for points_in_mask in prompt_point:\n                    if len(points_in_mask) != 2:\n                        raise ValueError(\"Each point should have 2 coordinates.\")\n                    if not all(isinstance(point, int) for point in points_in_mask):\n                        raise ValueError(\"Each point should be an integer.\")\n        if labels is not None:\n            for i, prompt_label in enumerate(labels):\n                if len(prompt_label) != len(points[i]):  # type: ignore[index]\n                    raise ValueError(\"The number of labels should match the number of points.\")\n\n                if not all(isinstance(label, int) for label in prompt_label):\n                    raise ValueError(\"Each label should be an integer.\")\n        if boxes is not None:\n            for prompt_box in boxes:\n                if len(prompt_box) != 4:\n                    raise ValueError(\"Each box should have 4 coordinates.\")\n                if not all(isinstance(box, int) for box in prompt_box):\n                    raise ValueError(\"Each box should be an integer.\")\n\n        # Convert inputs to numpy arrays\n        if points is not None:\n            np_points = [np.array(prompt_points, dtype=np.int32) for prompt_points in points]\n        if labels is not None:\n            np_labels = [np.array(prompt_labels, dtype=np.int32) for prompt_labels in labels]\n        if boxes is not None:\n            boxes = np.array(boxes, dtype=np.int32)\n\n        if points is not None:\n            # Pad the points and labels to the same length\n            # =============================================================================\n            # From Hugging Face's implementation (Apache-2.0 License):\n            # https://github.com/huggingface/transformers/blob/v4.45.2/src/transformers/models/sam/processing_sam.py#L36\n            expected_nb_points = max([point.shape[0] for point in np_points])\n            processed_points = []\n            for i, point in enumerate(np_points):\n                if point.shape[0] != expected_nb_points:\n                    point = np.concatenate(\n                        [point, np.zeros((expected_nb_points - point.shape[0], 2)) + -10], axis=0\n                    )\n                    if labels is not None:\n                        np_labels[i] = np.append(np_labels[i], [-10])\n                processed_points.append(point)\n            # =============================================================================\n            np_points = np.array(processed_points)\n\n        input_points = np_points if points is not None else None\n        input_labels = np.array(np_labels) if labels is not None else None\n\n        with torch.autocast(self.predictor.device.type, dtype=self.torch_dtype):\n            if not self.predictor._is_image_set:\n                self.predictor.set_image(image)\n\n            masks, scores, _ = self.predictor.predict(\n                point_coords=input_points,\n                point_labels=input_labels,\n                box=boxes,\n                mask_input=None,\n                multimask_output=multimask_output,\n                return_logits=False,\n            )\n\n            if len(masks.shape) == 3:\n                masks = np.expand_dims(masks, 0)\n                scores = np.expand_dims(scores, 0)\n\n            if return_image_embedding:\n                image_embedding: Tensor = self.predictor._features[\"image_embed\"]\n                image_embedding_list = image_embedding.to(torch.float32).flatten().tolist()\n                high_resolution_features: list[Tensor] = self.predictor._features[\"high_res_feats\"]\n                high_resolution_features_list = [\n                    features.to(torch.float32).flatten().tolist() for features in high_resolution_features\n                ]\n                image_embedding_ndarray = NDArrayFloat(\n                    values=image_embedding_list, shape=image_embedding.shape[1:]\n                )\n                high_resolution_features_ndarray = [\n                    NDArrayFloat(values=features_list, shape=features.shape[1:])\n                    for features, features_list in zip(high_resolution_features, high_resolution_features_list)\n                ]\n            else:\n                image_embedding_ndarray = None\n                high_resolution_features_ndarray = None\n\n            masks = ImageMaskGenerationOutput(\n                masks=[\n                    [CompressedRLE.from_mask(mask.astype(np.uint8)) for mask in prediction_masks]\n                    for prediction_masks in masks\n                ],\n                scores=NDArrayFloat.from_numpy(scores),\n                image_embedding=image_embedding_ndarray,\n                high_resolution_features=high_resolution_features_ndarray,\n            )\n            return masks\n</code></pre>"},{"location":"api_reference/models/sam2/#pixano_inference.models.sam2.Sam2Model.set_image_embeddings","title":"<code>set_image_embeddings(image, image_embedding, high_resolution_features)</code>","text":"<p>Calculates the image embeddings for the provided image.</p> <p>Adapted from https://github.com/facebookresearch/sam2/blob/main/sam2/sam2_image_predictor.py (Apache-2.0 License).</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray | 'Tensor' | Image</code> <p>The input image to embed in RGB format. The image should be in HWC format if np.ndarray, or WHC format if PIL Image with or CHW format if torch.Tensor.</p> required <code>image_embedding</code> <code>Tensor</code> <p>The image embedding tensor.</p> required <code>high_resolution_features</code> <code>Tensor</code> <p>The high-resolution features tensor.</p> required Source code in <code>pixano_inference/models/sam2.py</code> <pre><code>def set_image_embeddings(\n    self,\n    image: np.ndarray | \"Tensor\" | Image,\n    image_embedding: Tensor,\n    high_resolution_features: Tensor,\n) -&gt; None:\n    \"\"\"Calculates the image embeddings for the provided image.\n\n    Adapted from https://github.com/facebookresearch/sam2/blob/main/sam2/sam2_image_predictor.py\n    (Apache-2.0 License).\n\n    Args:\n        image: The input image to embed in RGB format. The image should be in HWC format if np.ndarray, or WHC\n            format if PIL Image with or CHW format if torch.Tensor.\n        image_embedding: The image embedding tensor.\n        high_resolution_features: The high-resolution features tensor.\n    \"\"\"\n    with torch.inference_mode():\n        self.predictor.reset_predictor()\n        # Transform the image to the form expected by the model\n        if isinstance(image, np.ndarray):\n            self.predictor._orig_hw = [image.shape[:2]]\n        elif isinstance(image, Tensor):\n            self.predictor._orig_hw = [image.shape[-2:]]\n        elif isinstance(image, Image):\n            w, h = image.size\n            self.predictor._orig_hw = [(h, w)]\n        else:\n            raise NotImplementedError(\"Image format not supported\")\n\n        self.predictor._features = {\n            \"image_embed\": image_embedding.unsqueeze(0).to(device=self.predictor.model.device),\n            \"high_res_feats\": [\n                features.unsqueeze(0).to(device=self.predictor.model.device)\n                for features in high_resolution_features\n            ],\n        }\n        self.predictor._is_image_set = True\n</code></pre>"},{"location":"api_reference/models/sam2/#pixano_inference.models.sam2.Sam2Model.video_mask_generation","title":"<code>video_mask_generation(video_dir, objects_ids, frame_indexes, points=None, labels=None, boxes=None, propagate=False, **kwargs)</code>","text":"<p>Generate masks from the video.</p> <p>Parameters:</p> Name Type Description Default <code>video_dir</code> <code>Path</code> <p>Directory of the video.</p> required <code>objects_ids</code> <code>list[int]</code> <p>IDs of the objects to generate masks for.</p> required <code>frame_indexes</code> <code>list[int]</code> <p>Indexes of the frames where the objects are located.</p> required <code>points</code> <code>list[list[list[int]]] | None</code> <p>Points for the mask generation. The first fimension is the number of objects, the second the number of points for each object and the third the coordinates of the points.</p> <code>None</code> <code>labels</code> <code>list[list[int]] | None</code> <p>Labels for the mask generation. The first fimension is the number of objects, the second the number of labels for each object.</p> <code>None</code> <code>boxes</code> <code>list[list[int]] | None</code> <p>Boxes for the mask generation. The first fimension is the number of objects, the second the coordinates of the boxes.</p> <code>None</code> <code>propagate</code> <code>bool</code> <p>Whether to propagate the masks in the video.</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>VideoMaskGenerationOutput</code> <p>Output of the generation.</p> Source code in <code>pixano_inference/models/sam2.py</code> <pre><code>def video_mask_generation(\n    self,\n    video_dir: Path,\n    objects_ids: list[int],\n    frame_indexes: list[int],\n    points: list[list[list[int]]] | None = None,\n    labels: list[list[int]] | None = None,\n    boxes: list[list[int]] | None = None,\n    propagate: bool = False,\n    **kwargs: Any,\n) -&gt; VideoMaskGenerationOutput:\n    \"\"\"Generate masks from the video.\n\n    Args:\n        video_dir: Directory of the video.\n        objects_ids: IDs of the objects to generate masks for.\n        frame_indexes: Indexes of the frames where the objects are located.\n        points: Points for the mask generation. The first fimension is the number of objects, the\n            second the number of points for each object and the third the coordinates of the points.\n        labels: Labels for the mask generation. The first fimension is the number of objects, the second\n            the number of labels for each object.\n        boxes: Boxes for the mask generation. The first fimension is the number of objects, the second\n            the coordinates of the boxes.\n        propagate: Whether to propagate the masks in the video.\n        kwargs: Additional keyword arguments.\n\n    Returns:\n        Output of the generation.\n    \"\"\"\n    # Check the input list types\n    with torch.inference_mode():\n        if not isinstance(video_dir, (Path)) or not video_dir.exists():\n            raise ValueError(\"The video_dir should be a valid path.\")\n        if (\n            points is not None\n            and not isinstance(points, list)\n            and not all(isinstance(point, list) for point in points)\n        ):\n            raise ValueError(\"The points should be a list of lists.\")\n        if (\n            labels is not None\n            and not isinstance(labels, list)\n            and not all(isinstance(label, list) for label in labels)\n        ):\n            raise ValueError(\"The labels should be a list of lists.\")\n        if boxes is not None and not isinstance(boxes, list):\n            if not all(isinstance(box, list) for box in boxes):\n                raise ValueError(\"The boxes should be a list of lists.\")\n\n        # Check the input batch size\n        if points is None and labels is not None:\n            raise ValueError(\"Labels are not supported without points.\")\n        if points is not None and labels is not None:\n            if len(points) != len(labels):\n                raise ValueError(\"The number of points and labels should match.\")\n        if points is not None and boxes is not None:\n            if len(points) != len(boxes):\n                raise ValueError(\"The number of points and boxes should match.\")\n\n        # Check the input shapes and value types\n        if points is not None:\n            for prompt_point in points:\n                for points_in_mask in prompt_point:\n                    if len(points_in_mask) != 2:\n                        raise ValueError(\"Each point should have 2 coordinates.\")\n                    if not all(isinstance(point, int) for point in points_in_mask):\n                        raise ValueError(\"Each point should be an integer.\")\n        if labels is not None:\n            for i, prompt_label in enumerate(labels):\n                if len(prompt_label) != len(points[i]):  # type: ignore[index]\n                    raise ValueError(\"The number of labels should match the number of points.\")\n\n                if not all(isinstance(label, int) for label in prompt_label):\n                    raise ValueError(\"Each label should be an integer.\")\n        if boxes is not None:\n            for prompt_box in boxes:\n                if len(prompt_box) != 4:\n                    raise ValueError(\"Each box should have 4 coordinates.\")\n                if not all(isinstance(box, int) for box in prompt_box):\n                    raise ValueError(\"Each box should be an integer.\")\n\n        # Convert inputs to numpy arrays\n        if points is not None:\n            input_points = [np.array(prompt_points, dtype=np.int32) for prompt_points in points]\n        else:\n            input_points = [None for _ in objects_ids]\n        if labels is not None:\n            input_labels = [np.array(prompt_labels, dtype=np.int32) for prompt_labels in labels]\n        else:\n            input_labels = [None for _ in objects_ids]\n        if boxes is not None:\n            input_boxes = [np.array(box, dtype=np.int32) for box in boxes]\n        else:\n            input_boxes = [None for _ in objects_ids]\n\n        video_segments: dict[int, dict[int, np.ndarray]] = {}\n        with torch.autocast(self.predictor.device.type, dtype=self.torch_dtype):\n            inference_state = self.predictor.init_state(video_path=str(video_dir))\n            for object_id, object_frame_index, object_points, object_labels, object_boxes in zip(\n                objects_ids, frame_indexes, input_points, input_labels, input_boxes\n            ):\n                _, out_obj_ids, out_mask_logits = self.predictor.add_new_points_or_box(\n                    inference_state=inference_state,\n                    frame_idx=object_frame_index,\n                    obj_id=object_id,\n                    points=object_points,\n                    labels=object_labels,\n                    box=object_boxes,\n                )\n\n                if not propagate:\n                    video_segments[object_frame_index] = {\n                        out_obj_id: (out_mask_logits[i] &gt; 0.0).cpu().numpy()\n                        for i, out_obj_id in enumerate(out_obj_ids)\n                    }\n\n            if propagate:\n                video_segments = {}  # video_segments contains the per-frame segmentation results\n                for out_frame_idx, out_obj_ids, out_mask_logits in self.predictor.propagate_in_video(\n                    inference_state\n                ):\n                    video_segments[out_frame_idx] = {\n                        out_obj_id: (out_mask_logits[i] &gt; 0.0).cpu().numpy()\n                        for i, out_obj_id in enumerate(out_obj_ids)\n                    }\n\n        out_objects_ids = []\n        out_frame_indexes = []\n        out_masks = []\n\n        for frame_index, object_masks in video_segments.items():\n            for object_id, mask in object_masks.items():\n                out_objects_ids.append(object_id)\n                out_frame_indexes.append(frame_index)\n                out_masks.append(mask)\n\n        return VideoMaskGenerationOutput(\n            objects_ids=out_objects_ids,\n            frame_indexes=out_frame_indexes,\n            masks=[CompressedRLE.from_mask(mask.astype(np.uint8)) for mask in out_masks],\n        )\n</code></pre>"},{"location":"api_reference/models/transformers/","title":"transformers","text":""},{"location":"api_reference/models/transformers/#pixano_inference.models.transformers","title":"<code>pixano_inference.models.transformers</code>","text":"<p>Inference models for Transformers.</p>"},{"location":"api_reference/models/transformers/#pixano_inference.models.transformers.TransformerModel","title":"<code>TransformerModel(name, path, processor, model)</code>","text":"<p>               Bases: <code>BaseInferenceModel</code></p> <p>Inference model for transformers.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>path</code> <code>Path | str</code> <p>Path to the model or its Hugging Face hub's identifier.</p> required <code>processor</code> <code>'ProcessorMixin'</code> <p>Processor for the model.</p> required <code>model</code> <code>'PreTrainedModel'</code> <p>Model for the inference.</p> required Source code in <code>pixano_inference/models/transformers.py</code> <pre><code>def __init__(self, name: str, path: Path | str, processor: \"ProcessorMixin\", model: \"PreTrainedModel\"):\n    \"\"\"Initialize the model.\n\n    Args:\n        name: Name of the model.\n        path: Path to the model or its Hugging Face hub's identifier.\n        processor: Processor for the model.\n        model: Model for the inference.\n    \"\"\"\n    assert_transformers_installed()\n\n    super().__init__(name, provider=\"transformers\")\n    self.processor = processor\n    self.path = path\n    self.model = model.eval()\n</code></pre>"},{"location":"api_reference/models/transformers/#pixano_inference.models.transformers.TransformerModel.metadata","title":"<code>metadata</code>  <code>property</code>","text":"<p>Return the metadata of the model.</p>"},{"location":"api_reference/models/transformers/#pixano_inference.models.transformers.TransformerModel.delete","title":"<code>delete()</code>","text":"<p>Delete the model.</p> Source code in <code>pixano_inference/models/transformers.py</code> <pre><code>def delete(self):\n    \"\"\"Delete the model.\"\"\"\n    del self.model\n    del self.processor\n    unregister_model(self)\n    gc.collect()\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"api_reference/models/transformers/#pixano_inference.models.transformers.TransformerModel.image_mask_generation","title":"<code>image_mask_generation(image, image_embedding=None, points=None, labels=None, boxes=None, num_multimask_outputs=3, multimask_output=True, return_image_embedding=False, **kwargs)</code>","text":"<p>Generate a mask from the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>'Tensor' | Image</code> <p>Image for the generation.</p> required <code>image_embedding</code> <code>'Tensor' | None</code> <p>Image embeddings for the generation.</p> <code>None</code> <code>points</code> <code>list[list[list[int]]] | None</code> <p>Points for the mask generation. The first fimension is the number of prompts, the second the number of points per mask and the third the coordinates of the points.</p> <code>None</code> <code>labels</code> <code>list[list[int]] | None</code> <p>Labels for the mask generation. The first fimension is the number of prompts, the second the number of labels per mask.</p> <code>None</code> <code>boxes</code> <code>list[list[int]] | None</code> <p>Boxes for the mask generation. The first fimension is the number of prompts, the second the coordinates of the boxes.</p> <code>None</code> <code>num_multimask_outputs</code> <code>int</code> <p>Number of masks to generate per prediction.</p> <code>3</code> <code>multimask_output</code> <code>bool</code> <p>Whether to generate multiple masks per prediction.</p> <code>True</code> <code>return_image_embedding</code> <code>bool</code> <p>Whether to return the image embedding.</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>pixano_inference/models/transformers.py</code> <pre><code>def image_mask_generation(\n    self,\n    image: \"Tensor\" | Image,\n    image_embedding: \"Tensor\" | None = None,\n    points: list[list[list[int]]] | None = None,\n    labels: list[list[int]] | None = None,\n    boxes: list[list[int]] | None = None,\n    num_multimask_outputs: int = 3,\n    multimask_output: bool = True,\n    return_image_embedding: bool = False,\n    **kwargs: Any,\n) -&gt; ImageMaskGenerationOutput:\n    \"\"\"Generate a mask from the image.\n\n    Args:\n        image: Image for the generation.\n        image_embedding: Image embeddings for the generation.\n        points: Points for the mask generation. The first fimension is the number of prompts, the\n            second the number of points per mask and the third the coordinates of the points.\n        labels: Labels for the mask generation. The first fimension is the number of prompts, the second\n            the number of labels per mask.\n        boxes: Boxes for the mask generation. The first fimension is the number of prompts, the second\n            the coordinates of the boxes.\n        num_multimask_outputs: Number of masks to generate per prediction.\n        multimask_output: Whether to generate multiple masks per prediction.\n        return_image_embedding: Whether to return the image embedding.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    with torch.inference_mode():\n        inputs = self.processor(\n            image,\n            input_points=[points] if points is not None else None,\n            input_boxes=[boxes] if boxes is not None else None,\n            input_labels=[labels] if labels is not None else None,\n            return_tensors=\"pt\",\n        ).to(self.model.device, dtype=self.model.dtype)\n\n        if return_image_embedding:\n            if image_embedding is None:  # Compute image embeddings if not provided\n                image_embedding = self.model.get_image_embeddings(inputs[\"pixel_values\"])\n\n        if image_embedding is not None:\n            if image_embedding.ndim == 3:\n                image_embedding = image_embedding.unsqueeze(0)\n            inputs.pop(\"pixel_values\", None)\n            inputs.update({\"image_embeddings\": image_embedding.to(self.model.device, dtype=self.model.dtype)})\n\n        outputs = self.model(\n            **inputs, num_multimask_outputs=num_multimask_outputs, multimask_output=multimask_output, **kwargs\n        )\n\n        masks = (\n            self.processor.image_processor.post_process_masks(\n                outputs.pred_masks, inputs[\"original_sizes\"], inputs[\"reshaped_input_sizes\"]\n            )\n        )[0].cpu()\n        return ImageMaskGenerationOutput(\n            masks=[\n                [CompressedRLE(**encode_mask_to_rle(mask)) for mask in prediction_masks]\n                for prediction_masks in masks\n            ],\n            scores=NDArrayFloat.from_torch(outputs.iou_scores[0].cpu()),\n            image_embedding=(\n                NDArrayFloat.from_torch(image_embedding[0].cpu()) if return_image_embedding else None\n            ),\n        )\n</code></pre>"},{"location":"api_reference/models/transformers/#pixano_inference.models.transformers.TransformerModel.text_image_conditional_generation","title":"<code>text_image_conditional_generation(prompt, images, generation_config=None, **kwargs)</code>","text":"<p>Generate text from an image and a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str | list[dict[str, Any]]</code> <p>Prompt for the generation.</p> required <code>images</code> <code>list['Tensor']</code> <p>Images for the generation.</p> required <code>generation_config</code> <code>'GenerationConfig' | None</code> <p>Configuration for the generation as Hugging Face's GenerationConfig.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>pixano_inference/models/transformers.py</code> <pre><code>def text_image_conditional_generation(\n    self,\n    prompt: str | list[dict[str, Any]],\n    images: list[\"Tensor\"],\n    generation_config: \"GenerationConfig\" | None = None,\n    **kwargs: Any,\n) -&gt; TextImageConditionalGenerationOutput:\n    \"\"\"Generate text from an image and a prompt.\n\n    Args:\n        prompt: Prompt for the generation.\n        images: Images for the generation.\n        generation_config: Configuration for the generation as Hugging Face's GenerationConfig.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    with torch.inference_mode():\n        if generation_config is None:\n            generation_config = GenerationConfig()\n\n        generation_config = self._fill_generation_config(generation_config, **kwargs)\n\n        if isinstance(prompt, list):\n            prompt = self.processor.apply_chat_template(prompt, add_generation_prompt=True)\n\n        inputs = self.processor(prompt, images, return_tensors=\"pt\").to(self.model.device)\n        generate_ids = self.model.generate(**inputs, generation_config=generation_config)\n\n        total_tokens: int = generate_ids.shape[1]\n        prompt_tokens: int = inputs[\"input_ids\"].shape[1]\n        completion_tokens: int = total_tokens - prompt_tokens\n\n        output = self.processor.decode(\n            generate_ids[0, prompt_tokens:], skip_special_tokens=True, clean_up_tokenization_spaces=False\n        )\n\n        return TextImageConditionalGenerationOutput(\n            generated_text=output,\n            usage=UsageConditionalGeneration(\n                prompt_tokens=prompt_tokens,\n                completion_tokens=completion_tokens,\n                total_tokens=total_tokens,\n            ),\n            generation_config=generation_config.to_diff_dict(),\n        )\n</code></pre>"},{"location":"api_reference/models/vllm/","title":"vllm","text":""},{"location":"api_reference/models/vllm/#pixano_inference.models.vllm","title":"<code>pixano_inference.models.vllm</code>","text":"<p>Inference models for vLLM.</p>"},{"location":"api_reference/models/vllm/#pixano_inference.models.vllm.VLLMModel","title":"<code>VLLMModel(name, vllm_model, model_config, processor_config, device=None)</code>","text":"<p>               Bases: <code>BaseInferenceModel</code></p> <p>Inference model for vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>vllm_model</code> <code>str</code> <p>The model Hugging Face hub's identifier.</p> required <code>model_config</code> <code>dict[str, Any]</code> <p>Configuration for the model.</p> required <code>processor_config</code> <code>dict[str, Any]</code> <p>Configuration for the processor of the model.</p> required <code>device</code> <code>device | str | None</code> <p>The device to use for inference.</p> <code>None</code> Source code in <code>pixano_inference/models/vllm.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    vllm_model: str,\n    model_config: dict[str, Any],\n    processor_config: dict[str, Any],\n    device: torch.device | str | None = None,\n):\n    \"\"\"Initialize the model.\n\n    Args:\n        name: Name of the model.\n        vllm_model: The model Hugging Face hub's identifier.\n        model_config: Configuration for the model.\n        processor_config: Configuration for the processor of the model.\n        device: The device to use for inference.\n    \"\"\"\n    assert_vllm_installed()\n\n    super().__init__(name, provider=\"vllm\")\n    self.vllm_model = vllm_model\n    self.model_config = model_config\n    self.processor_config = processor_config\n    self.model = LLM(model=vllm_model, **model_config, **processor_config, device=device)\n</code></pre>"},{"location":"api_reference/models/vllm/#pixano_inference.models.vllm.VLLMModel.metadata","title":"<code>metadata</code>  <code>property</code>","text":"<p>Return the metadata of the model.</p>"},{"location":"api_reference/models/vllm/#pixano_inference.models.vllm.VLLMModel.delete","title":"<code>delete()</code>","text":"<p>Delete the model.</p> Source code in <code>pixano_inference/models/vllm.py</code> <pre><code>def delete(self):\n    \"\"\"Delete the model.\"\"\"\n    del self.model\n    unregister_model(self)\n    gc.collect()\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"api_reference/models/vllm/#pixano_inference.models.vllm.VLLMModel.text_image_conditional_generation","title":"<code>text_image_conditional_generation(prompt, temperature=1.0, max_new_tokens=16, **kwargs)</code>","text":"<p>Generate text from an image and a prompt from the vLLM's <code>LLM.chat</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>list[dict[str, Any]]</code> <p>Prompt for the generation.</p> required <code>temperature</code> <code>float</code> <p>Temperature for the generation.</p> <code>1.0</code> <code>max_new_tokens</code> <code>int</code> <p>Maximum number of tokens to generate.</p> <code>16</code> <code>kwargs</code> <code>Any</code> <p>Additional generation arguments.</p> <code>{}</code> Source code in <code>pixano_inference/models/vllm.py</code> <pre><code>def text_image_conditional_generation(\n    self,\n    prompt: list[dict[str, Any]],\n    temperature: float = 1.0,\n    max_new_tokens: int = 16,\n    **kwargs: Any,\n) -&gt; TextImageConditionalGenerationOutput:\n    \"\"\"Generate text from an image and a prompt from the vLLM's `LLM.chat` method.\n\n    Args:\n        prompt: Prompt for the generation.\n        temperature: Temperature for the generation.\n        max_new_tokens: Maximum number of tokens to generate.\n        kwargs: Additional generation arguments.\n    \"\"\"\n    sampling_params = SamplingParams(temperature=temperature, max_tokens=max_new_tokens, **kwargs)\n    with torch.inference_mode():\n        request_output: RequestOutput = self.model.chat(\n            messages=prompt, use_tqdm=False, sampling_params=sampling_params\n        )[0]\n        prompt = request_output.prompt\n        output: CompletionOutput = request_output.outputs[0]\n\n        prompt_tokens = len(request_output.prompt_token_ids)\n        completion_tokens = len(output.token_ids)\n        total_tokens = prompt_tokens + completion_tokens\n\n        return TextImageConditionalGenerationOutput(\n            generated_text=output.text,\n            usage=UsageConditionalGeneration(\n                prompt_tokens=prompt_tokens,\n                completion_tokens=completion_tokens,\n                total_tokens=total_tokens,\n            ),\n            generation_config=msgspec.to_builtins(sampling_params),\n        )\n</code></pre>"},{"location":"api_reference/providers/base/","title":"base","text":""},{"location":"api_reference/providers/base/#pixano_inference.providers.base","title":"<code>pixano_inference.providers.base</code>","text":"<p>Base classes for providers.</p>"},{"location":"api_reference/providers/base/#pixano_inference.providers.base.APIProvider","title":"<code>APIProvider(**kwargs)</code>","text":"<p>               Bases: <code>BaseProvider</code></p> <p>Base class for API providers.</p> Source code in <code>pixano_inference/providers/base.py</code> <pre><code>def __init__(self, **kwargs: Any):\n    \"\"\"Initialize the provider.\"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/providers/base/#pixano_inference.providers.base.APIProvider.api_url","title":"<code>api_url</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>URL of the API.</p>"},{"location":"api_reference/providers/base/#pixano_inference.providers.base.APIProvider.send_request","title":"<code>send_request(request)</code>  <code>abstractmethod</code>","text":"<p>Send a request to the API and return the response.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>APIRequest</code> <p>Request to send.</p> required <p>Returns:</p> Type Description <code>BaseResponse</code> <p>Response from the API.</p> Source code in <code>pixano_inference/providers/base.py</code> <pre><code>@abstractmethod\ndef send_request(self, request: APIRequest) -&gt; BaseResponse:\n    \"\"\"Send a request to the API and return the response.\n\n    Args:\n        request: Request to send.\n\n    Returns:\n        Response from the API.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/providers/base/#pixano_inference.providers.base.BaseProvider","title":"<code>BaseProvider(**kwargs)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for providers.</p> Source code in <code>pixano_inference/providers/base.py</code> <pre><code>def __init__(self, **kwargs: Any):\n    \"\"\"Initialize the provider.\"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/providers/base/#pixano_inference.providers.base.BaseProvider.image_mask_generation","title":"<code>image_mask_generation(request, model, *args, **kwargs)</code>","text":"<p>Generate a mask from the image.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ImageMaskGenerationRequest</code> <p>Request for the generation.</p> required <code>model</code> <code>BaseInferenceModel</code> <p>Model to use for the generation.</p> required <code>args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>pixano_inference/providers/base.py</code> <pre><code>@torch.inference_mode()\ndef image_mask_generation(\n    self, request: ImageMaskGenerationRequest, model: BaseInferenceModel, *args: Any, **kwargs: Any\n) -&gt; ImageMaskGenerationOutput:\n    \"\"\"Generate a mask from the image.\n\n    Args:\n        request: Request for the generation.\n        model: Model to use for the generation.\n        args: Additional arguments.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    raise NotImplementedError(\"This provider does not support image mask generation.\")\n</code></pre>"},{"location":"api_reference/providers/base/#pixano_inference.providers.base.BaseProvider.text_image_conditional_generation","title":"<code>text_image_conditional_generation(request, model, *args, **kwargs)</code>","text":"<p>Generate an image from the text and image.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>TextImageConditionalGenerationRequest</code> <p>Request for the generation.</p> required <code>model</code> <code>BaseInferenceModel</code> <p>Model to use for the generation.</p> required <code>args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextImageConditionalGenerationOutput</code> <p>Output of the generation</p> Source code in <code>pixano_inference/providers/base.py</code> <pre><code>@torch.inference_mode()\ndef text_image_conditional_generation(\n    self, request: TextImageConditionalGenerationRequest, model: BaseInferenceModel, *args: Any, **kwargs: Any\n) -&gt; TextImageConditionalGenerationOutput:\n    \"\"\"Generate an image from the text and image.\n\n    Args:\n        request: Request for the generation.\n        model: Model to use for the generation.\n        args: Additional arguments.\n        kwargs: Additional keyword arguments.\n\n    Returns:\n        Output of the generation\n    \"\"\"\n    raise NotImplementedError(\"This provider does not support text-image conditional generation.\")\n</code></pre>"},{"location":"api_reference/providers/base/#pixano_inference.providers.base.BaseProvider.video_mask_generation","title":"<code>video_mask_generation(request, model, *args, **kwargs)</code>","text":"<p>Generate a mask from the video.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>VideoMaskGenerationRequest</code> <p>Request for the generation.</p> required <code>model</code> <code>BaseInferenceModel</code> <p>Model to use for the generation.</p> required <code>args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>VideoMaskGenerationResponse</code> <p>Output of the generation</p> Source code in <code>pixano_inference/providers/base.py</code> <pre><code>@torch.inference_mode()\ndef video_mask_generation(\n    self, request: VideoMaskGenerationRequest, model: BaseInferenceModel, *args: Any, **kwargs: Any\n) -&gt; VideoMaskGenerationResponse:\n    \"\"\"Generate a mask from the video.\n\n    Args:\n        request: Request for the generation.\n        model: Model to use for the generation.\n        args: Additional arguments.\n        kwargs: Additional keyword arguments.\n\n    Returns:\n        Output of the generation\n    \"\"\"\n    raise NotImplementedError(\"This provider does not support video mask generation.\")\n</code></pre>"},{"location":"api_reference/providers/base/#pixano_inference.providers.base.ModelProvider","title":"<code>ModelProvider(**kwargs)</code>","text":"<p>               Bases: <code>BaseProvider</code></p> <p>Base class for model providers.</p> Source code in <code>pixano_inference/providers/base.py</code> <pre><code>def __init__(self, **kwargs: Any):\n    \"\"\"Initialize the provider.\"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/providers/base/#pixano_inference.providers.base.ModelProvider.load_model","title":"<code>load_model(name, task, settings=PIXANO_INFERENCE_SETTINGS, path=None, processor_config={}, config={})</code>  <code>abstractmethod</code>","text":"<p>Load the model from the provider.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>task</code> <code>Task | str</code> <p>Task of the model.</p> required <code>settings</code> <code>Settings</code> <p>Settings to use for the provider.</p> <code>PIXANO_INFERENCE_SETTINGS</code> <code>path</code> <code>Path | str | None</code> <p>Path to the model.</p> <code>None</code> <code>processor_config</code> <code>dict</code> <p>Processor configuration.</p> <code>{}</code> <code>config</code> <code>dict</code> <p>Configuration for the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The loaded model.</p> Source code in <code>pixano_inference/providers/base.py</code> <pre><code>@abstractmethod\ndef load_model(\n    self,\n    name: str,\n    task: Task | str,\n    settings: Settings = PIXANO_INFERENCE_SETTINGS,\n    path: Path | str | None = None,\n    processor_config: dict = {},\n    config: dict = {},\n) -&gt; Any:\n    \"\"\"Load the model from the provider.\n\n    Args:\n        name: Name of the model.\n        task: Task of the model.\n        settings: Settings to use for the provider.\n        path: Path to the model.\n        processor_config: Processor configuration.\n        config: Configuration for the model.\n\n    Returns:\n        The loaded model.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/providers/registry/","title":"registry","text":""},{"location":"api_reference/providers/registry/#pixano_inference.providers.registry","title":"<code>pixano_inference.providers.registry</code>","text":"<p>Registry for providers.</p>"},{"location":"api_reference/providers/registry/#pixano_inference.providers.registry.get_provider","title":"<code>get_provider(provider)</code>","text":"<p>Return the provider from the registry.</p> Source code in <code>pixano_inference/providers/registry.py</code> <pre><code>def get_provider(provider: str) -&gt; type[BaseProvider]:\n    \"\"\"Return the provider from the registry.\"\"\"\n    if (actual_provider := PROVIDERS_REGISTRY.get(provider)) is not None:\n        return actual_provider\n    raise ValueError(f\"Provider {provider} not found.\")\n</code></pre>"},{"location":"api_reference/providers/registry/#pixano_inference.providers.registry.get_provider_name","title":"<code>get_provider_name(provider)</code>","text":"<p>Return the name of the provider.</p> Source code in <code>pixano_inference/providers/registry.py</code> <pre><code>def get_provider_name(provider: BaseProvider) -&gt; str:\n    \"\"\"Return the name of the provider.\"\"\"\n    for name, provider_cls in PROVIDERS_REGISTRY.items():\n        if isinstance(provider, provider_cls):\n            return name\n    raise ValueError(f\"Provider {provider} not found.\")\n</code></pre>"},{"location":"api_reference/providers/registry/#pixano_inference.providers.registry.get_providers","title":"<code>get_providers()</code>","text":"<p>Return the list of providers in the registry.</p> Source code in <code>pixano_inference/providers/registry.py</code> <pre><code>def get_providers() -&gt; list[str]:\n    \"\"\"Return the list of providers in the registry.\"\"\"\n    return list(PROVIDERS_REGISTRY.keys())\n</code></pre>"},{"location":"api_reference/providers/registry/#pixano_inference.providers.registry.is_provider","title":"<code>is_provider(provider)</code>","text":"<p>Return True if the provider is in the registry.</p> Source code in <code>pixano_inference/providers/registry.py</code> <pre><code>def is_provider(provider: str) -&gt; bool:\n    \"\"\"Return True if the provider is in the registry.\"\"\"\n    return provider in PROVIDERS_REGISTRY\n</code></pre>"},{"location":"api_reference/providers/registry/#pixano_inference.providers.registry.register_provider","title":"<code>register_provider(provider)</code>","text":"<p>Return a decorator to register a provider in the registry.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Name of the provider.</p> required <p>Returns:</p> Type Description <p>The decorator.</p> Source code in <code>pixano_inference/providers/registry.py</code> <pre><code>def register_provider(provider: str):\n    \"\"\"Return a decorator to register a provider in the registry.\n\n    Args:\n        provider: Name of the provider.\n\n    Returns:\n        The decorator.\n    \"\"\"\n\n    def decorator(cls):\n        \"\"\"Register the provider in the registry.\n\n        Args:\n            cls: Class to register.\n\n        Returns:\n            The class.\n        \"\"\"\n        if provider in PROVIDERS_REGISTRY:\n            raise ValueError(f\"Provider {provider} already registered.\")\n        PROVIDERS_REGISTRY[provider] = cls\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api_reference/providers/sam2/","title":"sam2","text":""},{"location":"api_reference/providers/sam2/#pixano_inference.providers.sam2","title":"<code>pixano_inference.providers.sam2</code>","text":"<p>Provider for the SAM2 model.</p>"},{"location":"api_reference/providers/sam2/#pixano_inference.providers.sam2.Sam2Provider","title":"<code>Sam2Provider(**kwargs)</code>","text":"<p>               Bases: <code>ModelProvider</code></p> <p>Provider for the SAM2 model.</p> Source code in <code>pixano_inference/providers/sam2.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the SAM2 provider.\"\"\"\n    assert_sam2_installed()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api_reference/providers/sam2/#pixano_inference.providers.sam2.Sam2Provider.image_mask_generation","title":"<code>image_mask_generation(request, model, *args, **kwargs)</code>","text":"<p>Generate a mask from the image.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ImageMaskGenerationRequest</code> <p>Request for the generation.</p> required <code>model</code> <code>Sam2Model</code> <p>Model to use for the generation.</p> required <code>args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ImageMaskGenerationOutput</code> <p>Output of the generation</p> Source code in <code>pixano_inference/providers/sam2.py</code> <pre><code>def image_mask_generation(\n    self, request: ImageMaskGenerationRequest, model: Sam2Model, *args: Any, **kwargs: Any\n) -&gt; ImageMaskGenerationOutput:\n    \"\"\"Generate a mask from the image.\n\n    Args:\n        request: Request for the generation.\n        model: Model to use for the generation.\n        args: Additional arguments.\n        kwargs: Additional keyword arguments.\n\n    Returns:\n        Output of the generation\n    \"\"\"\n    request_input = request.to_input()\n    image = convert_string_to_image(request_input.image)\n\n    if request_input.image_embedding is not None and request_input.high_resolution_features is not None:\n        image_embedding = vector_to_tensor(request_input.image_embedding)\n        high_resolution_features = [vector_to_tensor(v) for v in request_input.high_resolution_features]\n        model.set_image_embeddings(image, image_embedding, high_resolution_features)\n    elif request_input.image_embedding is not None or request_input.high_resolution_features is not None:\n        raise ValueError(\"Both image_embedding and high_resolution_features must be provided.\")\n\n    model_input = request_input.model_dump(exclude=[\"image\", \"image_embedding\", \"high_resolution_features\"])\n    model_input[\"image\"] = image\n    output = model.image_mask_generation(**model_input)\n    model.predictor.reset_predictor()\n    return output\n</code></pre>"},{"location":"api_reference/providers/sam2/#pixano_inference.providers.sam2.Sam2Provider.load_model","title":"<code>load_model(name, task, settings=PIXANO_INFERENCE_SETTINGS, path=None, processor_config={}, config={})</code>","text":"<p>Load the model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>task</code> <code>Task | str</code> <p>Task of the model.</p> required <code>settings</code> <code>Settings</code> <p>Settings to use for the provider.</p> <code>PIXANO_INFERENCE_SETTINGS</code> <code>path</code> <code>Path | str | None</code> <p>Path to the model.</p> <code>None</code> <code>processor_config</code> <code>dict</code> <p>Processor configuration.</p> <code>{}</code> <code>config</code> <code>dict</code> <p>Configuration for the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Sam2Model</code> <p>The loaded model.</p> Source code in <code>pixano_inference/providers/sam2.py</code> <pre><code>def load_model(\n    self,\n    name: str,\n    task: Task | str,\n    settings: Settings = PIXANO_INFERENCE_SETTINGS,\n    path: Path | str | None = None,\n    processor_config: dict = {},\n    config: dict = {},\n) -&gt; Sam2Model:\n    \"\"\"Load the model.\n\n    Args:\n        name: Name of the model.\n        task: Task of the model.\n        settings: Settings to use for the provider.\n        path: Path to the model.\n        processor_config: Processor configuration.\n        config: Configuration for the model.\n\n    Returns:\n        The loaded model.\n    \"\"\"\n    from sam2.build_sam import build_sam2, build_sam2_hf, build_sam2_video_predictor, build_sam2_video_predictor_hf\n    from sam2.sam2_image_predictor import SAM2ImagePredictor\n\n    device = settings.gpus_available[0] if settings.gpus_available else \"cpu\"\n\n    task = str_to_task(task) if isinstance(task, str) else task\n    if task == ImageTask.MASK_GENERATION:\n        if path is not None and Path(path).exists():\n            model = build_sam2(ckpt_path=path, mode=\"eval\", device=device, **config)\n        else:\n            model = build_sam2_hf(model_id=path, mode=\"eval\", device=device, **config)\n        model = torch.compile(model)\n        predictor = SAM2ImagePredictor(model)\n    elif task == VideoTask.MASK_GENERATION:\n        if path is not None and Path(path).exists():\n            predictor = build_sam2_video_predictor(ckpt_path=path, mode=\"eval\", device=device, **config)\n        else:\n            predictor = build_sam2_video_predictor_hf(model_id=path, mode=\"eval\", device=device, **config)\n        predictor = torch.compile(predictor)\n    else:\n        raise ValueError(f\"Invalid task '{task}' for the SAM2 provider.\")\n\n    if device != \"cpu\":\n        device = cast(int, device)\n        settings.gpus_used.append(device)\n\n    our_model = Sam2Model(\n        name=name,\n        provider=\"sam2\",\n        predictor=predictor,\n        torch_dtype=config.get(\"torch_dtype\", \"bfloat16\"),\n        config=config,\n    )\n\n    register_model(our_model, self, task)\n\n    return our_model\n</code></pre>"},{"location":"api_reference/providers/sam2/#pixano_inference.providers.sam2.Sam2Provider.video_mask_generation","title":"<code>video_mask_generation(request, model, *args, **kwargs)</code>","text":"<p>Generate masks from the video.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>VideoMaskGenerationRequest</code> <p>Request for the generation.</p> required <code>model</code> <code>BaseInferenceModel</code> <p>Model to use for the generation.</p> required <code>args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>VideoMaskGenerationResponse</code> <p>Response of the generation.</p> Source code in <code>pixano_inference/providers/sam2.py</code> <pre><code>def video_mask_generation(\n    self, request: VideoMaskGenerationRequest, model: BaseInferenceModel, *args: Any, **kwargs: Any\n) -&gt; VideoMaskGenerationResponse:\n    \"\"\"Generate masks from the video.\n\n    Args:\n        request: Request for the generation.\n        model: Model to use for the generation.\n        args: Additional arguments.\n        kwargs: Additional keyword arguments.\n\n    Returns:\n        Response of the generation.\n    \"\"\"\n    request_input = request.to_input()\n    output = model.video_mask_generation(**request_input)\n    return output\n</code></pre>"},{"location":"api_reference/providers/transformers/","title":"transformers","text":""},{"location":"api_reference/providers/transformers/#pixano_inference.providers.transformers","title":"<code>pixano_inference.providers.transformers</code>","text":"<p>Provider for Hugging Face Transformers models.</p>"},{"location":"api_reference/providers/transformers/#pixano_inference.providers.transformers.TransformersProvider","title":"<code>TransformersProvider(*args, **kwargs)</code>","text":"<p>               Bases: <code>ModelProvider</code></p> <p>Provider for Hugging Face Transformers models.</p> Source code in <code>pixano_inference/providers/transformers.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the transformer provider.\"\"\"\n    assert_transformers_installed()\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api_reference/providers/transformers/#pixano_inference.providers.transformers.TransformersProvider.image_mask_generation","title":"<code>image_mask_generation(request, model, *args, **kwargs)</code>","text":"<p>Generate a mask from the image.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ImageMaskGenerationRequest</code> <p>Request for the generation.</p> required <code>model</code> <code>TransformerModel</code> <p>Model to use for the generation.</p> required <code>args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ImageMaskGenerationOutput</code> <p>Output of the generation</p> Source code in <code>pixano_inference/providers/transformers.py</code> <pre><code>def image_mask_generation(\n    self, request: ImageMaskGenerationRequest, model: TransformerModel, *args: Any, **kwargs: Any\n) -&gt; ImageMaskGenerationOutput:\n    \"\"\"Generate a mask from the image.\n\n    Args:\n        request: Request for the generation.\n        model: Model to use for the generation.\n        args: Additional arguments.\n        kwargs: Additional keyword arguments.\n\n    Returns:\n        Output of the generation\n    \"\"\"\n    request_input = request.to_input()\n    image = convert_string_to_image(request_input.image)\n\n    if request_input.image_embedding is not None:\n        image_embedding = vector_to_tensor(request_input.image_embedding)\n\n    model_input = request_input.model_dump(exclude=[\"image\", \"image_embedding\"])\n    model_input[\"image\"] = image\n    model_input[\"image_embedding\"] = image_embedding if request_input.image_embedding is not None else None\n    output = model.image_mask_generation(**model_input)\n    return output\n</code></pre>"},{"location":"api_reference/providers/transformers/#pixano_inference.providers.transformers.TransformersProvider.load_model","title":"<code>load_model(name, task, settings=PIXANO_INFERENCE_SETTINGS, path=None, processor_config={}, config={})</code>","text":"<p>Load a model from transformers.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>task</code> <code>Task | str</code> <p>Task of the model.</p> required <code>settings</code> <code>Settings</code> <p>Settings to use for the provider.</p> <code>PIXANO_INFERENCE_SETTINGS</code> <code>path</code> <code>Path | str | None</code> <p>Path to the model or its Hugging Face hub's identifier.</p> <code>None</code> <code>processor_config</code> <code>dict</code> <p>Configuration for the processor.</p> <code>{}</code> <code>config</code> <code>dict</code> <p>Configuration for the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>TransformerModel</code> <p>Loaded model.</p> Source code in <code>pixano_inference/providers/transformers.py</code> <pre><code>def load_model(\n    self,\n    name: str,\n    task: Task | str,\n    settings: Settings = PIXANO_INFERENCE_SETTINGS,\n    path: Path | str | None = None,\n    processor_config: dict = {},\n    config: dict = {},\n) -&gt; TransformerModel:\n    \"\"\"Load a model from transformers.\n\n    Args:\n        name: Name of the model.\n        task: Task of the model.\n        settings: Settings to use for the provider.\n        path: Path to the model or its Hugging Face hub's identifier.\n        processor_config: Configuration for the processor.\n        config: Configuration for the model.\n\n    Returns:\n        Loaded model.\n    \"\"\"\n    if path is None:\n        raise ValueError(\"Path is required to load a model from transformers.\")\n    if isinstance(task, str):\n        task = str_to_task(task)\n    processor = AutoProcessor.from_pretrained(path, **processor_config)\n\n    device = settings.gpus_available[0] if settings.gpus_available else \"cpu\"\n    quantization_config = config.pop(\"quantization_config\", {})\n    quantization_config = BitsAndBytesConfig(**quantization_config)\n    config[\"quantization_config\"] = quantization_config\n\n    model = get_transformer_automodel_from_pretrained(path, task, device_map=device, **config)\n    if model is None:\n        if task in [NLPTask.CONDITONAL_GENERATION, MultimodalImageNLPTask.CONDITIONAL_GENERATION]:\n            model = get_conditional_generation_transformer_from_pretrained(name, path, device_map=device, **config)\n\n    if device != \"cpu\":\n        device = cast(int, device)\n        settings.gpus_used.append(device)\n\n    model = model.eval()\n    model = torch.compile(model)\n\n    our_model = TransformerModel(name, path, processor, model)\n    register_model(our_model, self, task)\n    return model\n</code></pre>"},{"location":"api_reference/providers/transformers/#pixano_inference.providers.transformers.TransformersProvider.text_image_conditional_generation","title":"<code>text_image_conditional_generation(request, model)</code>","text":"<p>Generate text from an image and a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>TextImageConditionalGenerationRequest</code> <p>Request for text-image conditional generation.</p> required <code>model</code> <code>TransformerModel</code> <p>Model for text-image conditional generation</p> required <p>Returns:</p> Type Description <code>TextImageConditionalGenerationOutput</code> <p>Output of text-image conditional generation.</p> Source code in <code>pixano_inference/providers/transformers.py</code> <pre><code>def text_image_conditional_generation(\n    self, request: TextImageConditionalGenerationRequest, model: TransformerModel\n) -&gt; TextImageConditionalGenerationOutput:\n    \"\"\"Generate text from an image and a prompt.\n\n    Args:\n        request: Request for text-image conditional generation.\n        model: Model for text-image conditional generation\n\n    Returns:\n        Output of text-image conditional generation.\n    \"\"\"\n    model_input = request.to_input()\n\n    images: list[Image] | None\n    if model_input.images is None:\n        if isinstance(model_input.prompt, str):\n            raise ValueError(\"Images must be provided if the prompt is a string.\")\n        images = []\n        for message in model_input.prompt:\n            new_content = []\n            for content in message[\"content\"]:\n                if content[\"type\"] == \"image_url\":\n                    images.append(convert_string_to_image(content[\"image_url\"][\"url\"]))\n                    new_content.append({\"type\": \"image\"})\n                else:\n                    new_content.append(content)\n            message[\"content\"] = new_content\n\n    else:\n        images = (\n            [convert_string_to_image(image) for image in model_input[\"images\"]]\n            if len(model_input[\"images\"]) &gt; 0\n            else None\n        )\n\n    model_input_dump = model_input.model_dump()\n    model_input_dump[\"images\"] = images\n    output = model.text_image_conditional_generation(**model_input_dump)\n    return output\n</code></pre>"},{"location":"api_reference/providers/transformers/#pixano_inference.providers.transformers.get_conditional_generation_transformer_from_pretrained","title":"<code>get_conditional_generation_transformer_from_pretrained(name, path, **model_kwargs)</code>","text":"<p>Get a transformer model from transformers using automodel.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>path</code> <code>Path | str | None</code> <p>Path to the model or its Hugging Face hub's identifier.</p> required <code>model_kwargs</code> <code>Any</code> <p>Additional keyword arguments for the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>PreTrainedModel</code> <p>Model from Transformers.</p> Source code in <code>pixano_inference/providers/transformers.py</code> <pre><code>def get_conditional_generation_transformer_from_pretrained(\n    name: str, path: Path | str | None, **model_kwargs: Any\n) -&gt; PreTrainedModel:\n    \"\"\"Get a transformer model from transformers using automodel.\n\n    Args:\n        name: Name of the model.\n        path: Path to the model or its Hugging Face hub's identifier.\n        model_kwargs: Additional keyword arguments for the model.\n\n    Returns:\n        Model from Transformers.\n    \"\"\"\n    name = name.lower()\n    if \"llava\" in name:\n        if \"next\" in name:\n            if \"video\" in name:\n                from transformers import LlavaNextVideoForConditionalGeneration\n\n                model = LlavaNextVideoForConditionalGeneration.from_pretrained(path, **model_kwargs)\n            else:\n                from transformers import LlavaNextForConditionalGeneration\n\n                model = LlavaNextForConditionalGeneration.from_pretrained(path, **model_kwargs)\n        else:\n            from transformers import LlavaForConditionalGeneration\n\n            model = LlavaForConditionalGeneration.from_pretrained(path, **model_kwargs)\n    else:\n        raise ValueError(f\"Model {name} not supported.\")\n    return model\n</code></pre>"},{"location":"api_reference/providers/transformers/#pixano_inference.providers.transformers.get_transformer_automodel_from_pretrained","title":"<code>get_transformer_automodel_from_pretrained(pretrained_model_name_or_path, task, **model_kwargs)</code>","text":"<p>Get a transformer model from transformers using automodel.</p> <p>Parameters:</p> Name Type Description Default <code>pretrained_model_name_or_path</code> <code>str | Path</code> <p>Name or path of the pretrained model.</p> required <code>task</code> <code>Task</code> <p>Task of the model.</p> required <code>model_kwargs</code> <code>Any</code> <p>Additional keyword arguments for the model.</p> <code>{}</code> Source code in <code>pixano_inference/providers/transformers.py</code> <pre><code>def get_transformer_automodel_from_pretrained(\n    pretrained_model_name_or_path: str | Path, task: Task, **model_kwargs: Any\n):\n    \"\"\"Get a transformer model from transformers using automodel.\n\n    Args:\n        pretrained_model_name_or_path: Name or path of the pretrained model.\n        task: Task of the model.\n        model_kwargs: Additional keyword arguments for the model.\n    \"\"\"\n    assert_transformers_installed()\n    pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n    if isinstance(task, ImageTask):\n        match task:\n            case ImageTask.CLASSIFICATION:\n                from transformers import AutoModelForImageClassification\n\n                return AutoModelForImageClassification.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case ImageTask.DEPTH_ESTIMATION:\n                from transformers import AutoModelForDepthEstimation\n\n                return AutoModelForDepthEstimation.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case ImageTask.FEATURE_EXTRACTION:\n                from transformers import AutoModel\n\n                return AutoModel.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case ImageTask.KEYPOINT_DETECTION:\n                from transformers import AutoModelForKeypointDetection\n\n                return AutoModelForKeypointDetection.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case ImageTask.MASK_GENERATION:\n                from transformers import AutoModelForMaskGeneration\n\n                return AutoModelForMaskGeneration.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case ImageTask.OBJECT_DETECTION:\n                from transformers import AutoModelForObjectDetection\n\n                return AutoModelForObjectDetection.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case ImageTask.SEMANTIC_SEGMENTATION:\n                from transformers import AutoModelForSemanticSegmentation\n\n                return AutoModelForSemanticSegmentation.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case ImageTask.INSTANCE_SEGMENTATION:\n                from transformers import AutoModelForInstanceSegmentation\n\n                return AutoModelForInstanceSegmentation.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case ImageTask.UNIVERSAL_SEGMENTATION:\n                from transformers import AutoModelForUniversalSegmentation\n\n                return AutoModelForUniversalSegmentation.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case ImageTask.ZERO_SHOT_CLASSIFICATION:\n                from transformers import AutoModelForZeroShotImageClassification\n\n                return AutoModelForZeroShotImageClassification.from_pretrained(\n                    pretrained_model_name_or_path, **model_kwargs\n                )\n            case ImageTask.ZERO_SHOT_DETECTION:\n                from transformers import AutoModelForZeroShotObjectDetection\n\n                return AutoModelForZeroShotObjectDetection.from_pretrained(\n                    pretrained_model_name_or_path, **model_kwargs\n                )\n    elif isinstance(task, NLPTask):\n        match task:\n            case NLPTask.CAUSAL_LM:\n                from transformers import AutoModelForCausalLM\n\n                return AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case NLPTask.MASKED_LM:\n                from transformers import AutoModelForMaskedLM\n\n                return AutoModelForMaskedLM.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case NLPTask.MASK_GENERATION:\n                from transformers import AutoModelForMaskGeneration\n\n                return AutoModelForMaskGeneration.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case NLPTask.SEQUENCE_CLASSIFICATION:\n                from transformers import AutoModelForSequenceClassification\n\n                return AutoModelForSequenceClassification.from_pretrained(\n                    pretrained_model_name_or_path, **model_kwargs\n                )\n            case NLPTask.MULTIPLE_CHOICE:\n                from transformers import AutoModelForMultipleChoice\n\n                return AutoModelForMultipleChoice.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case NLPTask.NEXT_SENTENCE_PREDICTION:\n                from transformers import AutoModelForNextSentencePrediction\n\n                return AutoModelForNextSentencePrediction.from_pretrained(\n                    pretrained_model_name_or_path, **model_kwargs\n                )\n            case NLPTask.TOKEN_CLASSIFICATION:\n                from transformers import AutoModelForTokenClassification\n\n                return AutoModelForTokenClassification.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case NLPTask.QUESTION_ANSWERING:\n                from transformers import AutoModelForQuestionAnswering\n\n                return AutoModelForQuestionAnswering.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case NLPTask.TEXT_ENCODING:\n                from transformers import AutoModelForTextEncoding\n\n                return AutoModelForTextEncoding.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case _:\n                raise ValueError(f\"Task {task} not supported.\")\n</code></pre>"},{"location":"api_reference/providers/vllm/","title":"vllm","text":""},{"location":"api_reference/providers/vllm/#pixano_inference.providers.vllm","title":"<code>pixano_inference.providers.vllm</code>","text":"<p>Provider for vLLM models.</p>"},{"location":"api_reference/providers/vllm/#pixano_inference.providers.vllm.VLLMProvider","title":"<code>VLLMProvider(*args, **kwargs)</code>","text":"<p>               Bases: <code>ModelProvider</code></p> <p>Provider for vLLM models.</p> Source code in <code>pixano_inference/providers/vllm.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the vLLM provider.\"\"\"\n    assert_vllm_installed()\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api_reference/providers/vllm/#pixano_inference.providers.vllm.VLLMProvider.load_model","title":"<code>load_model(name, task, settings=PIXANO_INFERENCE_SETTINGS, path=None, processor_config={}, config={})</code>","text":"<p>Load a model from vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>task</code> <code>Task | str</code> <p>Task of the model.</p> required <code>settings</code> <code>Settings</code> <p>Settings to use for the provider.</p> <code>PIXANO_INFERENCE_SETTINGS</code> <code>path</code> <code>str | None</code> <p>Path to the model or its Hugging Face hub's identifier.</p> <code>None</code> <code>processor_config</code> <code>dict</code> <p>Configuration for the processor.</p> <code>{}</code> <code>config</code> <code>dict</code> <p>Configuration for the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>VLLMModel</code> <p>Loaded model.</p> Source code in <code>pixano_inference/providers/vllm.py</code> <pre><code>def load_model(\n    self,\n    name: str,\n    task: Task | str,\n    settings: Settings = PIXANO_INFERENCE_SETTINGS,\n    path: str | None = None,  # type: ignore[override]\n    processor_config: dict = {},\n    config: dict = {},\n) -&gt; VLLMModel:\n    \"\"\"Load a model from vLLM.\n\n    Args:\n        name: Name of the model.\n        task: Task of the model.\n        settings: Settings to use for the provider.\n        path: Path to the model or its Hugging Face hub's identifier.\n        processor_config: Configuration for the processor.\n        config: Configuration for the model.\n\n    Returns:\n        Loaded model.\n    \"\"\"\n    if path is None:\n        raise ValueError(\"Path is required to load a model from vLLm.\")\n    if isinstance(task, str):\n        task = str_to_task(task)\n\n    device = settings.gpus_available[0] if settings.gpus_available else \"cpu\"\n\n    our_model = VLLMModel(\n        name=name, vllm_model=path, model_config=config, processor_config=processor_config, device=device\n    )\n\n    if device != \"cpu\":\n        device = cast(int, device)\n        settings.gpus_used.append(device)\n\n    register_model(our_model, self, task)\n    return our_model\n</code></pre>"},{"location":"api_reference/providers/vllm/#pixano_inference.providers.vllm.VLLMProvider.text_image_conditional_generation","title":"<code>text_image_conditional_generation(request, model)</code>","text":"<p>Generate text from an image and a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>TextImageConditionalGenerationRequest</code> <p>Request for text-image conditional generation.</p> required <code>model</code> <code>VLLMModel</code> <p>Model for text-image conditional generation</p> required <p>Returns:</p> Type Description <code>TextImageConditionalGenerationOutput</code> <p>Output of text-image conditional generation.</p> Source code in <code>pixano_inference/providers/vllm.py</code> <pre><code>def text_image_conditional_generation(\n    self, request: TextImageConditionalGenerationRequest, model: VLLMModel\n) -&gt; TextImageConditionalGenerationOutput:\n    \"\"\"Generate text from an image and a prompt.\n\n    Args:\n        request: Request for text-image conditional generation.\n        model: Model for text-image conditional generation\n\n    Returns:\n        Output of text-image conditional generation.\n    \"\"\"\n    model_input = request.to_input()\n    if model_input.images is not None:\n        raise ValueError(\"images should be passed in the prompt for vLLM.\")\n    if isinstance(model_input.prompt, str):\n        raise ValueError(\"Pixano-inference only support a chat template for vLLM.\")\n\n    output = model.text_image_conditional_generation(**model_input.model_dump(exclude=\"images\"))\n    return output\n</code></pre>"},{"location":"api_reference/pydantic/base/","title":"base","text":""},{"location":"api_reference/pydantic/base/#pixano_inference.pydantic.base","title":"<code>pixano_inference.pydantic.base</code>","text":"<p>Pydantic base models for request and response.</p>"},{"location":"api_reference/pydantic/base/#pixano_inference.pydantic.base.APIRequest","title":"<code>APIRequest</code>","text":"<p>               Bases: <code>BaseRequest</code></p> <p>API request model.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>API key.</p> <code>secret_key</code> <code>str</code> <p>Secret key.</p>"},{"location":"api_reference/pydantic/base/#pixano_inference.pydantic.base.BaseRequest","title":"<code>BaseRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base request model.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>Name of the model.</p>"},{"location":"api_reference/pydantic/base/#pixano_inference.pydantic.base.BaseResponse","title":"<code>BaseResponse</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base response model.</p> <p>Attributes:</p> Name Type Description <code>timestamp</code> <code>datetime</code> <p>Timestamp of the response.</p> <code>processing_time</code> <code>float</code> <p>Processing time of the response.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Metadata of the response.</p> <code>data</code> <code>Any</code> <p>Data of the response.</p>"},{"location":"api_reference/pydantic/models/","title":"models","text":""},{"location":"api_reference/pydantic/models/#pixano_inference.pydantic.models","title":"<code>pixano_inference.pydantic.models</code>","text":"<p>Pydantic models for model configuration.</p>"},{"location":"api_reference/pydantic/models/#pixano_inference.pydantic.models.ModelConfig","title":"<code>ModelConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model configuration for instantiation.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the model.</p> <code>task</code> <code>str</code> <p>Task of the model.</p> <code>path</code> <code>Path | str | None</code> <p>Path to the model dump.</p> <code>config</code> <code>dict[str, Any]</code> <p>Configuration of the model.</p> <code>processor_config</code> <code>dict[str, Any]</code> <p>Configuration of the processor.</p>"},{"location":"api_reference/pydantic/models/#pixano_inference.pydantic.models.ModelInfo","title":"<code>ModelInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model Information.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the model.</p> <code>task</code> <code>str</code> <p>Task of the model.</p>"},{"location":"api_reference/pydantic/nd_array/","title":"nd_array","text":""},{"location":"api_reference/pydantic/nd_array/#pixano_inference.pydantic.nd_array","title":"<code>pixano_inference.pydantic.nd_array</code>","text":"<p>Pydantic models for N-dimensional arrays.</p>"},{"location":"api_reference/pydantic/nd_array/#pixano_inference.pydantic.nd_array.NDArray","title":"<code>NDArray</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[T]</code>, <code>ABC</code></p> <p>Represents an N-dimensional array.</p> <p>Attributes:</p> Name Type Description <code>values</code> <code>list[T]</code> <p>The list of values.</p> <code>shape</code> <code>list[int]</code> <p>The shape of the array, represented as a list of integers.</p> <code>np_dtype</code> <code>dtype</code> <p>The NumPy data type of the array.</p>"},{"location":"api_reference/pydantic/nd_array/#pixano_inference.pydantic.nd_array.NDArray.from_numpy","title":"<code>from_numpy(arr)</code>  <code>classmethod</code>","text":"<p>Create an instance of the class from a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>The NumPy array to convert.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>An instance of the class with values and shape derived from the input array.</p> Source code in <code>pixano_inference/pydantic/nd_array.py</code> <pre><code>@classmethod\ndef from_numpy(cls, arr: np.ndarray) -&gt; Self:\n    \"\"\"Create an instance of the class from a NumPy array.\n\n    Args:\n        arr: The NumPy array to convert.\n\n    Returns:\n        An instance of the class with values and shape derived from\n            the input array.\n    \"\"\"\n    shape = list(arr.shape)\n    arr = arr.astype(dtype=cls.np_dtype)\n    return cls(\n        values=arr.reshape(-1).tolist(),\n        shape=shape,\n    )\n</code></pre>"},{"location":"api_reference/pydantic/nd_array/#pixano_inference.pydantic.nd_array.NDArray.from_torch","title":"<code>from_torch(tensor)</code>  <code>classmethod</code>","text":"<p>Create an instance of the class from a PyTorch tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The PyTorch tensor to convert.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>An instance of the class with values and shape derived from the input tensor.</p> Source code in <code>pixano_inference/pydantic/nd_array.py</code> <pre><code>@classmethod\ndef from_torch(cls, tensor: \"Tensor\") -&gt; Self:\n    \"\"\"Create an instance of the class from a PyTorch tensor.\n\n    Args:\n        tensor: The PyTorch tensor to convert.\n\n    Returns:\n        An instance of the class with values and shape derived from\n            the input tensor.\n    \"\"\"\n    assert_torch_installed()\n    return cls.from_numpy(tensor.cpu().numpy())\n</code></pre>"},{"location":"api_reference/pydantic/nd_array/#pixano_inference.pydantic.nd_array.NDArray.to_numpy","title":"<code>to_numpy()</code>","text":"<p>Convert the instance to a NumPy array.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array with values and shape derived from the instance.</p> Source code in <code>pixano_inference/pydantic/nd_array.py</code> <pre><code>def to_numpy(self) -&gt; np.ndarray:\n    \"\"\"Convert the instance to a NumPy array.\n\n    Returns:\n        A NumPy array with values and shape derived from the instance.\n    \"\"\"\n    array = np.array(self.values, dtype=self.np_dtype).reshape(self.shape)\n    return array\n</code></pre>"},{"location":"api_reference/pydantic/nd_array/#pixano_inference.pydantic.nd_array.NDArray.to_torch","title":"<code>to_torch()</code>","text":"<p>Convert the instance to a PyTorch tensor.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A PyTorch tensor with values and shape derived from the instance.</p> Source code in <code>pixano_inference/pydantic/nd_array.py</code> <pre><code>def to_torch(self) -&gt; \"Tensor\":\n    \"\"\"Convert the instance to a PyTorch tensor.\n\n    Returns:\n        A PyTorch tensor with values and shape derived from the instance.\n    \"\"\"\n    assert_torch_installed()\n    return torch.from_numpy(self.to_numpy())\n</code></pre>"},{"location":"api_reference/pydantic/nd_array/#pixano_inference.pydantic.nd_array.NDArrayFloat","title":"<code>NDArrayFloat</code>","text":"<p>               Bases: <code>NDArray[float]</code></p> <p>Represents an N-dimensional array of 32-bit floating-point values.</p> <p>Attributes:</p> Name Type Description <code>values</code> <code>list[T]</code> <p>The list of 32-bit floating-point values in the array.</p> <code>shape</code> <code>list[int]</code> <p>The shape of the array, represented as a list of integers.</p> <code>np_dtype</code> <code>dtype</code> <p>The NumPy data type of the array.</p>"},{"location":"api_reference/pydantic/data/vector_database/","title":"vector_database","text":""},{"location":"api_reference/pydantic/data/vector_database/#pixano_inference.pydantic.data.vector_database","title":"<code>pixano_inference.pydantic.data.vector_database</code>","text":"<p>Pydantic model for the Vector databases.</p>"},{"location":"api_reference/pydantic/data/vector_database/#pixano_inference.pydantic.data.vector_database.LanceVector","title":"<code>LanceVector</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Lance vector model.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> <p>The path to the LANCE dataset.</p> <code>column</code> <code>str</code> <p>The column to read.</p> <code>indice</code> <code>int | None</code> <p>The index of the row to read.</p> <code>where</code> <code>str | None</code> <p>The filter to apply. If specified, only one row should be returned.</p> <code>shape</code> <code>list[int] | None</code> <p>The shape of the vector.</p>"},{"location":"api_reference/pydantic/data/vector_database/#pixano_inference.pydantic.data.vector_database.LanceVector.read_vector","title":"<code>read_vector(return_type='tensor')</code>","text":"<p>Reads a vector from a Lance dataset.</p> <p>Parameters:</p> Name Type Description Default <code>return_type</code> <code>Literal['tensor', 'numpy']</code> <p>The type of the return value. Either 'tensor' or 'numpy'.</p> <code>'tensor'</code> <p>Returns:</p> Type Description <code>'Tensor' | ndarray</code> <p>The vector.</p> Source code in <code>pixano_inference/pydantic/data/vector_database.py</code> <pre><code>def read_vector(self, return_type: Literal[\"tensor\", \"numpy\"] = \"tensor\") -&gt; \"Tensor\" | np.ndarray:\n    \"\"\"Reads a vector from a Lance dataset.\n\n    Args:\n        return_type: The type of the return value. Either 'tensor' or 'numpy'.\n\n    Returns:\n        The vector.\n    \"\"\"\n    return read_lance_vector(self.path, self.column, self.indice, self.where, self.shape, return_type)\n</code></pre>"},{"location":"api_reference/pydantic/data/vector_database/#pixano_inference.pydantic.data.vector_database.LanceVector.validate_indice_or_where","title":"<code>validate_indice_or_where()</code>","text":"<p>Validates that only one of 'indice' and 'where' is specified.</p> Source code in <code>pixano_inference/pydantic/data/vector_database.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_indice_or_where(self) -&gt; Self:\n    \"\"\"Validates that only one of 'indice' and 'where' is specified.\"\"\"\n    if self.indice is not None and self.where is not None:\n        raise ValueError(\"Only one of 'indice' and 'where' can be specified.\")\n    elif self.indice is None and self.where is None:\n        raise ValueError(\"One of 'indice' and 'where' must be specified.\")\n    return self\n</code></pre>"},{"location":"api_reference/pydantic/tasks/image/mask_generation/","title":"mask_generation","text":""},{"location":"api_reference/pydantic/tasks/image/mask_generation/#pixano_inference.pydantic.tasks.image.mask_generation","title":"<code>pixano_inference.pydantic.tasks.image.mask_generation</code>","text":"<p>Pydantic models for mask generation task.</p>"},{"location":"api_reference/pydantic/tasks/image/mask_generation/#pixano_inference.pydantic.tasks.image.mask_generation.ImageMaskGenerationInput","title":"<code>ImageMaskGenerationInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input for mask generation.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>str | Path</code> <p>Image for mask generation.</p> <code>image_embedding</code> <code>NDArrayFloat | LanceVector | None</code> <p>Image embedding for the mask generation.</p> <code>high_resolution_features</code> <code>list[NDArrayFloat] | list[LanceVector] | None</code> <p>High resolution features for the mask generation.</p> <code>points</code> <code>list[list[list[int]]] | None</code> <p>Points for the mask generation. The first fimension is the number of prompts the second the number of points per mask and the third the coordinates of the points.</p> <code>labels</code> <code>list[list[int]] | None</code> <p>Labels for the mask generation. The first fimension is the number of prompts, the second the number of labels per mask.</p> <code>boxes</code> <code>list[list[int]] | None</code> <p>Boxes for the mask generation. The first fimension is the number of prompts, the second the coordinates of the boxes.</p> <code>num_multimask_outputs</code> <code>int</code> <p>Number of masks to generate per prediction.</p> <code>multimask_output</code> <code>bool</code> <p>Whether to generate multiple masks per prediction.</p> <code>return_image_embedding</code> <code>bool</code> <p>Whether to return the image embeddings.</p>"},{"location":"api_reference/pydantic/tasks/image/mask_generation/#pixano_inference.pydantic.tasks.image.mask_generation.ImageMaskGenerationOutput","title":"<code>ImageMaskGenerationOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output for mask generation.</p> <p>Attributes:</p> Name Type Description <code>masks</code> <code>list[list[CompressedRLE]]</code> <p>Generated masks. The first dimension is the number of predictions, the second the number of masks per prediction.</p> <code>scores</code> <code>NDArrayFloat</code> <p>Scores of the masks. The first dimension is the number of predictions, the second the number of masks per prediction.</p> <code>image_embedding</code> <code>NDArrayFloat | None</code> <p>Image embeddings.</p> <code>high_resolution_features</code> <code>list[NDArrayFloat] | None</code> <p>High resolution features.</p>"},{"location":"api_reference/pydantic/tasks/image/mask_generation/#pixano_inference.pydantic.tasks.image.mask_generation.ImageMaskGenerationRequest","title":"<code>ImageMaskGenerationRequest</code>","text":"<p>               Bases: <code>BaseRequest</code>, <code>ImageMaskGenerationInput</code></p> <p>Request for mask generation.</p>"},{"location":"api_reference/pydantic/tasks/image/mask_generation/#pixano_inference.pydantic.tasks.image.mask_generation.ImageMaskGenerationRequest.to_input","title":"<code>to_input()</code>","text":"<p>Convert the request to the input.</p> Source code in <code>pixano_inference/pydantic/tasks/image/mask_generation.py</code> <pre><code>def to_input(self) -&gt; ImageMaskGenerationInput:\n    \"\"\"Convert the request to the input.\"\"\"\n    return ImageMaskGenerationInput(\n        image=self.image,\n        points=self.points,\n        boxes=self.boxes,\n        num_multimask_outputs=self.num_multimask_outputs,\n        multimask_output=self.multimask_output,\n        image_embedding=self.image_embedding,\n        high_resolution_features=self.high_resolution_features,\n        return_image_embedding=self.return_image_embedding,\n    )\n</code></pre>"},{"location":"api_reference/pydantic/tasks/image/mask_generation/#pixano_inference.pydantic.tasks.image.mask_generation.ImageMaskGenerationResponse","title":"<code>ImageMaskGenerationResponse</code>","text":"<p>               Bases: <code>BaseResponse</code></p> <p>Response for mask generation.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>ImageMaskGenerationOutput</code> <p>Output of the generation.</p>"},{"location":"api_reference/pydantic/tasks/image/utils/","title":"utils","text":""},{"location":"api_reference/pydantic/tasks/image/utils/#pixano_inference.pydantic.tasks.image.utils","title":"<code>pixano_inference.pydantic.tasks.image.utils</code>","text":"<p>Pydantic models for image tasks.</p>"},{"location":"api_reference/pydantic/tasks/image/utils/#pixano_inference.pydantic.tasks.image.utils.CompressedRLE","title":"<code>CompressedRLE</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Compressed RLE mask type.</p> <p>Attributes:</p> Name Type Description <code>size</code> <code>list[int]</code> <p>Mask size.</p> <code>counts</code> <code>bytes</code> <p>Mask RLE encoding.</p>"},{"location":"api_reference/pydantic/tasks/image/utils/#pixano_inference.pydantic.tasks.image.utils.CompressedRLE.from_mask","title":"<code>from_mask(mask, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a compressed RLE mask from a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Image | ndarray</code> <p>The mask as a NumPy array.</p> required <code>kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CompressedRLE</code> <p>The compressed RLE mask.</p> Source code in <code>pixano_inference/pydantic/tasks/image/utils.py</code> <pre><code>@staticmethod\ndef from_mask(mask: Image | np.ndarray, **kwargs: Any) -&gt; \"CompressedRLE\":\n    \"\"\"Create a compressed RLE mask from a NumPy array.\n\n    Args:\n        mask: The mask as a NumPy array.\n        kwargs: Additional arguments.\n\n    Returns:\n        The compressed RLE mask.\n    \"\"\"\n    rle = mask_to_rle(mask)\n    return CompressedRLE(size=rle[\"size\"], counts=rle[\"counts\"], **kwargs)\n</code></pre>"},{"location":"api_reference/pydantic/tasks/image/utils/#pixano_inference.pydantic.tasks.image.utils.CompressedRLE.to_mask","title":"<code>to_mask()</code>","text":"<p>Convert the compressed RLE mask to a NumPy array.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>The mask as a NumPy array.</p> Source code in <code>pixano_inference/pydantic/tasks/image/utils.py</code> <pre><code>def to_mask(self) -&gt; np.ndarray:\n    \"\"\"Convert the compressed RLE mask to a NumPy array.\n\n    Returns:\n        The mask as a NumPy array.\n    \"\"\"\n    return rle_to_mask({\"size\": self.size, \"counts\": self.counts})\n</code></pre>"},{"location":"api_reference/pydantic/tasks/image/utils/#pixano_inference.pydantic.tasks.image.utils.mask_to_rle","title":"<code>mask_to_rle(mask)</code>","text":"<p>Encode mask from Pillow or NumPy array to RLE.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Image | ndarray</code> <p>Mask as Pillow or NumPy array.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Mask as RLE.</p> Source code in <code>pixano_inference/pydantic/tasks/image/utils.py</code> <pre><code>def mask_to_rle(mask: Image | np.ndarray) -&gt; dict:\n    \"\"\"Encode mask from Pillow or NumPy array to RLE.\n\n    Args:\n        mask: Mask as Pillow or NumPy array.\n\n    Returns:\n        Mask as RLE.\n    \"\"\"\n    mask_array = np.asfortranarray(mask)\n    return mask_api.encode(mask_array)\n</code></pre>"},{"location":"api_reference/pydantic/tasks/image/utils/#pixano_inference.pydantic.tasks.image.utils.rle_to_mask","title":"<code>rle_to_mask(rle)</code>","text":"<p>Decode mask from RLE to NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>rle</code> <code>dict[str, list[int] | bytes]</code> <p>Mask as RLE.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Mask as NumPy array.</p> Source code in <code>pixano_inference/pydantic/tasks/image/utils.py</code> <pre><code>def rle_to_mask(rle: dict[str, list[int] | bytes]) -&gt; np.ndarray:\n    \"\"\"Decode mask from RLE to NumPy array.\n\n    Args:\n        rle: Mask as RLE.\n\n    Returns:\n        Mask as NumPy array.\n    \"\"\"\n    return mask_api.decode(rle)\n</code></pre>"},{"location":"api_reference/pydantic/tasks/multimodal/conditional_generation/","title":"conditional_generation","text":""},{"location":"api_reference/pydantic/tasks/multimodal/conditional_generation/#pixano_inference.pydantic.tasks.multimodal.conditional_generation","title":"<code>pixano_inference.pydantic.tasks.multimodal.conditional_generation</code>","text":"<p>Pydantic models for text-image conditional generation task.</p>"},{"location":"api_reference/pydantic/tasks/multimodal/conditional_generation/#pixano_inference.pydantic.tasks.multimodal.conditional_generation.TextImageConditionalGenerationInput","title":"<code>TextImageConditionalGenerationInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input for text-image conditional generation.</p> <p>Attributes:</p> Name Type Description <code>prompt</code> <code>str | list[dict[str, Any]]</code> <p>Prompt for the generation. Can be a string or a list of dictionaries to apply a chat template.</p> <code>images</code> <code>list[str | Path] | None</code> <p>Images for the generation. Can be None if images are passed in the prompt.</p> <code>max_new_tokens</code> <code>int</code> <p>Maximum number of new tokens to generate.</p> <code>temperature</code> <code>float</code> <p>Temperature for the generation.</p>"},{"location":"api_reference/pydantic/tasks/multimodal/conditional_generation/#pixano_inference.pydantic.tasks.multimodal.conditional_generation.TextImageConditionalGenerationOutput","title":"<code>TextImageConditionalGenerationOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output for text-image conditional generation.</p> <p>Attributes:</p> Name Type Description <code>generated_text</code> <code>str</code> <p>Generated text.</p> <code>usage</code> <code>UsageConditionalGeneration</code> <p>Usage of the model for the generation.</p> <code>generation_config</code> <code>dict[str, Any]</code> <p>Configuration for the generation.</p>"},{"location":"api_reference/pydantic/tasks/multimodal/conditional_generation/#pixano_inference.pydantic.tasks.multimodal.conditional_generation.TextImageConditionalGenerationRequest","title":"<code>TextImageConditionalGenerationRequest</code>","text":"<p>               Bases: <code>BaseRequest</code>, <code>TextImageConditionalGenerationInput</code></p> <p>Request for text-image conditional generation.</p>"},{"location":"api_reference/pydantic/tasks/multimodal/conditional_generation/#pixano_inference.pydantic.tasks.multimodal.conditional_generation.TextImageConditionalGenerationRequest.to_input","title":"<code>to_input()</code>","text":"<p>Convert the request to the input.</p> Source code in <code>pixano_inference/pydantic/tasks/multimodal/conditional_generation.py</code> <pre><code>def to_input(self) -&gt; TextImageConditionalGenerationInput:\n    \"\"\"Convert the request to the input.\"\"\"\n    return TextImageConditionalGenerationInput(\n        prompt=self.prompt,\n        images=self.images,\n        max_new_tokens=self.max_new_tokens,\n        temperature=self.temperature,\n    )\n</code></pre>"},{"location":"api_reference/pydantic/tasks/multimodal/conditional_generation/#pixano_inference.pydantic.tasks.multimodal.conditional_generation.TextImageConditionalGenerationResponse","title":"<code>TextImageConditionalGenerationResponse</code>","text":"<p>               Bases: <code>BaseResponse</code></p> <p>Response for text-image conditional generation.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>TextImageConditionalGenerationOutput</code> <p>Output of the generation.</p>"},{"location":"api_reference/pydantic/tasks/multimodal/conditional_generation/#pixano_inference.pydantic.tasks.multimodal.conditional_generation.UsageConditionalGeneration","title":"<code>UsageConditionalGeneration</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Usage metadata of the model for text-image conditional generation.</p> <p>Attributes:</p> Name Type Description <code>prompt_tokens</code> <code>int</code> <p>Number of tokens in the prompt.</p> <code>completion_tokens</code> <code>int</code> <p>Number of tokens in the completion.</p> <code>total_tokens</code> <code>int</code> <p>Total number of tokens.</p>"},{"location":"api_reference/pydantic/tasks/video/mask_generation/","title":"mask_generation","text":""},{"location":"api_reference/pydantic/tasks/video/mask_generation/#pixano_inference.pydantic.tasks.video.mask_generation","title":"<code>pixano_inference.pydantic.tasks.video.mask_generation</code>","text":"<p>Pydantic models for mask generation task.</p>"},{"location":"api_reference/pydantic/tasks/video/mask_generation/#pixano_inference.pydantic.tasks.video.mask_generation.VideoMaskGenerationInput","title":"<code>VideoMaskGenerationInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input for mask generation.</p> <p>Attributes:</p> Name Type Description <code>frames</code> <p>Frames for mask generation.</p> <code>points</code> <code>list[list[list[int]]] | None</code> <p>Points for the mask generation. The first fimension is the number of objects the second the number of points for each object and the third the coordinates of the points.</p> <code>labels</code> <code>list[list[int]] | None</code> <p>Labels for the mask generation. The first fimension is the number of objects, the second the number of labels for each object.</p> <code>boxes</code> <code>list[list[int]] | None</code> <p>Boxes for the mask generation. The first fimension is the number of objects, the second the coordinates of the boxes.</p> <code>objects_ids</code> <code>list[int]</code> <p>IDs of the objects to generate masks for.</p> <code>frame_indexes</code> <code>list[int]</code> <p>Indexes of the frames where the objects are located.</p>"},{"location":"api_reference/pydantic/tasks/video/mask_generation/#pixano_inference.pydantic.tasks.video.mask_generation.VideoMaskGenerationOutput","title":"<code>VideoMaskGenerationOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output for mask generation.</p> <p>Attributes:</p> Name Type Description <code>objects_ids</code> <code>list[int]</code> <p>IDs of the objects.</p> <code>frame_indexes</code> <code>list[int]</code> <p>Indexes of the frames where the objects are located.</p> <code>masks</code> <code>list[CompressedRLE]</code> <p>Masks for the objects.</p>"},{"location":"api_reference/pydantic/tasks/video/mask_generation/#pixano_inference.pydantic.tasks.video.mask_generation.VideoMaskGenerationRequest","title":"<code>VideoMaskGenerationRequest</code>","text":"<p>               Bases: <code>BaseRequest</code>, <code>VideoMaskGenerationInput</code></p> <p>Request for mask generation.</p>"},{"location":"api_reference/pydantic/tasks/video/mask_generation/#pixano_inference.pydantic.tasks.video.mask_generation.VideoMaskGenerationRequest.to_input","title":"<code>to_input()</code>","text":"<p>Convert the request to the input.</p> Source code in <code>pixano_inference/pydantic/tasks/video/mask_generation.py</code> <pre><code>def to_input(self) -&gt; VideoMaskGenerationInput:\n    \"\"\"Convert the request to the input.\"\"\"\n    return VideoMaskGenerationInput(\n        video=self.video,\n        points=self.points,\n        labels=self.labels,\n        boxes=self.boxes,\n        objects_ids=self.objects_ids,\n        frame_indexes=self.frame_indexes,\n    )\n</code></pre>"},{"location":"api_reference/pydantic/tasks/video/mask_generation/#pixano_inference.pydantic.tasks.video.mask_generation.VideoMaskGenerationResponse","title":"<code>VideoMaskGenerationResponse</code>","text":"<p>               Bases: <code>BaseResponse</code></p> <p>Response for mask generation.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>VideoMaskGenerationOutput</code> <p>Output of the generation.</p>"},{"location":"api_reference/routers/app/","title":"app","text":""},{"location":"api_reference/routers/app/#pixano_inference.routers.app","title":"<code>pixano_inference.routers.app</code>","text":"<p>API routes for the app.</p>"},{"location":"api_reference/routers/app/#pixano_inference.routers.app.get_list_models","title":"<code>get_list_models()</code>  <code>async</code>","text":"<p>List all models available in the app.</p> Source code in <code>pixano_inference/routers/app.py</code> <pre><code>@router.get(\"/models/\", response_model=list[ModelInfo])\nasync def get_list_models() -&gt; list[ModelInfo]:\n    \"\"\"List all models available in the app.\"\"\"\n    models = [ModelInfo(name=model_name, task=task.value) for model_name, (model, provider, task) in list_models()]\n    return models\n</code></pre>"},{"location":"api_reference/routers/app/#pixano_inference.routers.app.get_settings","title":"<code>get_settings()</code>  <code>async</code>","text":"<p>Get the current settings of the app.</p> Source code in <code>pixano_inference/routers/app.py</code> <pre><code>@router.get(\"/settings/\", response_model=Settings)\nasync def get_settings() -&gt; Settings:\n    \"\"\"Get the current settings of the app.\"\"\"\n    return PIXANO_INFERENCE_SETTINGS\n</code></pre>"},{"location":"api_reference/routers/image/","title":"image","text":""},{"location":"api_reference/routers/image/#pixano_inference.routers.image","title":"<code>pixano_inference.routers.image</code>","text":"<p>API routes for Image tasks.</p>"},{"location":"api_reference/routers/image/#pixano_inference.routers.image.mask_generation","title":"<code>mask_generation(request)</code>  <code>async</code>","text":"<p>Generate mask from an image and optionnaly points and bboxes.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ImageMaskGenerationRequest</code> <p>Request for mask generation.</p> required <p>Returns:</p> Type Description <code>ImageMaskGenerationResponse</code> <p>Response for mask generation.</p> Source code in <code>pixano_inference/routers/image.py</code> <pre><code>@router.post(\"/mask_generation/\", response_model=ImageMaskGenerationResponse)\nasync def mask_generation(\n    request: ImageMaskGenerationRequest,\n) -&gt; ImageMaskGenerationResponse:\n    \"\"\"Generate mask from an image and optionnaly points and bboxes.\n\n    Args:\n        request: Request for mask generation.\n\n    Returns:\n        Response for mask generation.\n    \"\"\"\n    return await execute_task_request(\n        request=request, task=ImageTask.MASK_GENERATION, response_type=ImageMaskGenerationResponse\n    )\n</code></pre>"},{"location":"api_reference/routers/multimodal/","title":"multimodal","text":""},{"location":"api_reference/routers/multimodal/#pixano_inference.routers.multimodal","title":"<code>pixano_inference.routers.multimodal</code>","text":"<p>API routes for NLP tasks.</p>"},{"location":"api_reference/routers/multimodal/#pixano_inference.routers.multimodal.image_text_conditional_generation","title":"<code>image_text_conditional_generation(request)</code>  <code>async</code>","text":"<p>Generate text from an image and a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>TextImageConditionalGenerationRequest</code> <p>Request for text-image conditional generation.</p> required <p>Returns:</p> Type Description <code>TextImageConditionalGenerationResponse</code> <p>Response for text-image conditional generation.</p> Source code in <code>pixano_inference/routers/multimodal.py</code> <pre><code>@router.post(\"/conditional_generation/\", response_model=TextImageConditionalGenerationResponse)\nasync def image_text_conditional_generation(\n    request: TextImageConditionalGenerationRequest,\n) -&gt; TextImageConditionalGenerationResponse:\n    \"\"\"Generate text from an image and a prompt.\n\n    Args:\n        request: Request for text-image conditional generation.\n\n    Returns:\n        Response for text-image conditional generation.\n    \"\"\"\n    return await execute_task_request(\n        request=request,\n        task=MultimodalImageNLPTask.CONDITIONAL_GENERATION,\n        response_type=TextImageConditionalGenerationResponse,\n    )\n</code></pre>"},{"location":"api_reference/routers/nlp/","title":"nlp","text":""},{"location":"api_reference/routers/nlp/#pixano_inference.routers.nlp","title":"<code>pixano_inference.routers.nlp</code>","text":"<p>API routes for NLP tasks.</p>"},{"location":"api_reference/routers/providers/","title":"providers","text":""},{"location":"api_reference/routers/providers/#pixano_inference.routers.providers","title":"<code>pixano_inference.routers.providers</code>","text":"<p>API routes for model providers.</p>"},{"location":"api_reference/routers/providers/#pixano_inference.routers.providers.delete_model","title":"<code>delete_model(model_name)</code>  <code>async</code>","text":"<p>Delete a model from the system.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to be deleted.</p> required Source code in <code>pixano_inference/routers/providers.py</code> <pre><code>@router.delete(\"/model/{model_name}\")\nasync def delete_model(model_name: str):\n    \"\"\"Delete a model from the system.\n\n    Args:\n        model_name: The name of the model to be deleted.\n    \"\"\"\n    try:\n        model = get_model_from_registry(model_name)\n    except KeyError as e:\n        raise HTTPException(status_code=404, detail=f\"Model {model_name} not found\") from e\n    model.delete()\n    return\n</code></pre>"},{"location":"api_reference/routers/providers/#pixano_inference.routers.providers.instantiate_model","title":"<code>instantiate_model(config, provider, settings)</code>  <code>async</code>","text":"<p>Instantiate a model from a provider.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ModelConfig</code> <p>Model configuration for instantiation.</p> required <code>provider</code> <code>Annotated[str, Body()]</code> <p>The model provider.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_pixano_inference_settings)]</code> <p>Settings for the instantiation.</p> required Source code in <code>pixano_inference/routers/providers.py</code> <pre><code>@router.post(\"/instantiate\")\nasync def instantiate_model(\n    config: ModelConfig,\n    provider: Annotated[str, Body()],\n    settings: Annotated[Settings, Depends(get_pixano_inference_settings)],\n):\n    \"\"\"Instantiate a model from a provider.\n\n    Args:\n        config: Model configuration for instantiation.\n        provider: The model provider.\n        settings: Settings for the instantiation.\n    \"\"\"\n    if not is_provider(provider):\n        raise HTTPException(status_code=404, detail=f\"Provider {provider} does not exist.\")\n    elif provider == \"transformers\":\n        try:\n            p: ModelProvider = get_provider(\"transformers\")()  # type: ignore[assignment]\n        except ImportError:\n            raise HTTPException(status_code=500, detail=\"Transformers library is not installed.\")\n    elif provider == \"sam2\":\n        try:\n            p = get_provider(\"sam2\")()  # type: ignore[assignment]\n        except ImportError:\n            raise HTTPException(status_code=500, detail=\"Sam2 is not installed.\")\n    elif provider == \"vllm\":\n        try:\n            p = get_provider(\"vllm\")()  # type: ignore[assignment]\n        except ImportError:\n            raise HTTPException(status_code=500, detail=\"vLLM library is not installed.\")\n    else:\n        p = get_provider(provider)()  # type: ignore[assignment]\n\n    p.load_model(settings=settings, **config.model_dump())\n    return\n</code></pre>"},{"location":"api_reference/routers/providers/#pixano_inference.routers.providers.instantiate_sam2_model","title":"<code>instantiate_sam2_model(config, settings)</code>  <code>async</code>","text":"<p>Instantiate a model from sam2.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ModelConfig</code> <p>Model configuration for instantiation.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_pixano_inference_settings)]</code> <p>Settings for the instantiation.</p> required Source code in <code>pixano_inference/routers/providers.py</code> <pre><code>@router.post(\"/sam2/instantiate/\")\nasync def instantiate_sam2_model(\n    config: ModelConfig, settings: Annotated[Settings, Depends(get_pixano_inference_settings)]\n):\n    \"\"\"Instantiate a model from sam2.\n\n    Args:\n        config: Model configuration for instantiation.\n        settings: Settings for the instantiation.\n    \"\"\"\n    return instantiate_model(config=config, provider=\"sam2\", settings=settings)\n</code></pre>"},{"location":"api_reference/routers/providers/#pixano_inference.routers.providers.instantiate_transformer_model","title":"<code>instantiate_transformer_model(config, settings)</code>  <code>async</code>","text":"<p>Instantiate a model from transformers.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ModelConfig</code> <p>Model configuration for instantiation.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_pixano_inference_settings)]</code> <p>Settings for the instantiation.</p> required Source code in <code>pixano_inference/routers/providers.py</code> <pre><code>@router.post(\"/transformers/instantiate/\")\nasync def instantiate_transformer_model(\n    config: ModelConfig, settings: Annotated[Settings, Depends(get_pixano_inference_settings)]\n):\n    \"\"\"Instantiate a model from transformers.\n\n    Args:\n        config: Model configuration for instantiation.\n        settings: Settings for the instantiation.\n    \"\"\"\n    return instantiate_model(config=config, provider=\"transformers\", settings=settings)\n</code></pre>"},{"location":"api_reference/routers/utils/","title":"utils","text":""},{"location":"api_reference/routers/utils/#pixano_inference.routers.utils","title":"<code>pixano_inference.routers.utils</code>","text":"<p>Utils for the routers.</p>"},{"location":"api_reference/routers/utils/#pixano_inference.routers.utils.execute_task_request","title":"<code>execute_task_request(request, task, response_type)</code>  <code>async</code>","text":"<p>Execute a task request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>BaseRequest</code> <p>Request to execute.</p> required <code>task</code> <code>Task</code> <p>Task to execute</p> required <code>response_type</code> <code>type[T]</code> <p>Type of the response to return.</p> required <p>Returns:</p> Type Description <code>T</code> <p>Response of the request</p> Source code in <code>pixano_inference/routers/utils.py</code> <pre><code>async def execute_task_request(request: BaseRequest, task: Task, response_type: type[T]) -&gt; T:\n    \"\"\"Execute a task request.\n\n    Args:\n        request: Request to execute.\n        task: Task to execute\n        response_type: Type of the response to return.\n\n    Returns:\n        Response of the request\n    \"\"\"\n    start_time = time.time()\n    model_str = request.model\n    try:\n        model, provider, model_task = get_values_from_model_registry(model_str)\n    except KeyError:\n        raise HTTPException(404, detail=f\"Model {model_str} is not registered.\")\n    if task != model_task:\n        raise HTTPException(400, detail=f\"Model {model} does not support the {task.value} task.\")\n\n    if model.status == ModelStatus.RUNNING:\n        raise HTTPException(503, \"Model is running please wait for it to finish and try again.\")\n\n    model.status = ModelStatus.RUNNING\n    try:\n        match task:\n            case ImageTask.MASK_GENERATION:\n                output = provider.image_mask_generation(request, model)\n            case MultimodalImageNLPTask.CONDITIONAL_GENERATION:\n                output = provider.text_image_conditional_generation(request, model)\n            case _:\n                raise HTTPException(400, f\"Task {task.value} is not yet supported.\")\n    except Exception as e:\n        model.status = ModelStatus.IDLE\n        raise e\n    else:\n        model.status = ModelStatus.IDLE\n\n    response = response_type(\n        timestamp=datetime.now(),\n        processing_time=time.time() - start_time,\n        metadata=model.metadata,\n        data=output,\n    )\n    return response\n</code></pre>"},{"location":"api_reference/routers/video/","title":"video","text":""},{"location":"api_reference/routers/video/#pixano_inference.routers.video","title":"<code>pixano_inference.routers.video</code>","text":"<p>API routes for Image tasks.</p>"},{"location":"api_reference/routers/video/#pixano_inference.routers.video.mask_generation","title":"<code>mask_generation(request)</code>  <code>async</code>","text":"<p>Generate mask from a video and optionnaly points and bboxes.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>VideoMaskGenerationRequest</code> <p>Request for mask generation.</p> required <p>Returns:</p> Type Description <code>VideoMaskGenerationResponse</code> <p>Response for mask generation.</p> Source code in <code>pixano_inference/routers/video.py</code> <pre><code>@router.post(\"/mask_generation/\", response_model=VideoMaskGenerationResponse)\nasync def mask_generation(\n    request: VideoMaskGenerationRequest,\n) -&gt; VideoMaskGenerationResponse:\n    \"\"\"Generate mask from a video and optionnaly points and bboxes.\n\n    Args:\n        request: Request for mask generation.\n\n    Returns:\n        Response for mask generation.\n    \"\"\"\n    return await execute_task_request(\n        request=request, task=VideoTask.MASK_GENERATION, response_type=VideoMaskGenerationResponse\n    )\n</code></pre>"},{"location":"api_reference/tasks/image/","title":"image","text":""},{"location":"api_reference/tasks/image/#pixano_inference.tasks.image","title":"<code>pixano_inference.tasks.image</code>","text":"<p>Image tasks.</p>"},{"location":"api_reference/tasks/image/#pixano_inference.tasks.image.ImageTask","title":"<code>ImageTask</code>","text":"<p>               Bases: <code>Task</code></p> <p>Image tasks.</p>"},{"location":"api_reference/tasks/multimodal/","title":"multimodal","text":""},{"location":"api_reference/tasks/multimodal/#pixano_inference.tasks.multimodal","title":"<code>pixano_inference.tasks.multimodal</code>","text":"<p>Multimodal tasks.</p>"},{"location":"api_reference/tasks/multimodal/#pixano_inference.tasks.multimodal.MultimodalImageNLPTask","title":"<code>MultimodalImageNLPTask</code>","text":"<p>               Bases: <code>Task</code></p> <p>Multimodal tasks.</p>"},{"location":"api_reference/tasks/nlp/","title":"nlp","text":""},{"location":"api_reference/tasks/nlp/#pixano_inference.tasks.nlp","title":"<code>pixano_inference.tasks.nlp</code>","text":"<p>Natural Language Processing tasks.</p>"},{"location":"api_reference/tasks/nlp/#pixano_inference.tasks.nlp.NLPTask","title":"<code>NLPTask</code>","text":"<p>               Bases: <code>Task</code></p> <p>Natural Language Processing tasks.</p>"},{"location":"api_reference/tasks/task/","title":"task","text":""},{"location":"api_reference/tasks/task/#pixano_inference.tasks.task","title":"<code>pixano_inference.tasks.task</code>","text":"<p>Tasks module.</p>"},{"location":"api_reference/tasks/task/#pixano_inference.tasks.task.Task","title":"<code>Task</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Task base class.</p>"},{"location":"api_reference/tasks/utils/","title":"utils","text":""},{"location":"api_reference/tasks/utils/#pixano_inference.tasks.utils","title":"<code>pixano_inference.tasks.utils</code>","text":"<p>Task utilities.</p>"},{"location":"api_reference/tasks/utils/#pixano_inference.tasks.utils.get_tasks","title":"<code>get_tasks()</code>","text":"<p>Get all tasks.</p> Source code in <code>pixano_inference/tasks/utils.py</code> <pre><code>def get_tasks() -&gt; set[str]:\n    \"\"\"Get all tasks.\"\"\"\n    return {task for set in [STR_IMAGE_TASKS, STR_NLP_TASKS, STR_MULTIMODAL_TASKS, STR_VIDEO_TASKS] for task in set}\n</code></pre>"},{"location":"api_reference/tasks/utils/#pixano_inference.tasks.utils.is_task","title":"<code>is_task(task)</code>","text":"<p>Check if a task is valid.</p> Source code in <code>pixano_inference/tasks/utils.py</code> <pre><code>def is_task(task: str) -&gt; bool:\n    \"\"\"Check if a task is valid.\"\"\"\n    return task in get_tasks()\n</code></pre>"},{"location":"api_reference/tasks/utils/#pixano_inference.tasks.utils.str_to_task","title":"<code>str_to_task(task)</code>","text":"<p>Convert a task string to its task.</p> Source code in <code>pixano_inference/tasks/utils.py</code> <pre><code>def str_to_task(task: str) -&gt; Task:\n    \"\"\"Convert a task string to its task.\"\"\"\n    if task in STR_IMAGE_TASKS:\n        return ImageTask(task)\n    elif task in STR_NLP_TASKS:\n        return NLPTask(task)\n    elif task in STR_MULTIMODAL_TASKS:\n        return MultimodalImageNLPTask(task)\n    elif task in STR_VIDEO_TASKS:\n        return VideoTask(task)\n    raise ValueError(f\"Invalid task '{task}'\")\n</code></pre>"},{"location":"api_reference/tasks/video/","title":"video","text":""},{"location":"api_reference/tasks/video/#pixano_inference.tasks.video","title":"<code>pixano_inference.tasks.video</code>","text":"<p>Video tasks.</p>"},{"location":"api_reference/tasks/video/#pixano_inference.tasks.video.VideoTask","title":"<code>VideoTask</code>","text":"<p>               Bases: <code>Task</code></p> <p>Video tasks.</p>"},{"location":"api_reference/utils/image/","title":"image","text":""},{"location":"api_reference/utils/image/#pixano_inference.utils.image","title":"<code>pixano_inference.utils.image</code>","text":"<p>Image utilities.</p>"},{"location":"api_reference/utils/image/#pixano_inference.utils.image.compress_rle","title":"<code>compress_rle(rle)</code>","text":"<p>Compress an RLE encoded mask.</p> <p>Parameters:</p> Name Type Description Default <code>rle</code> <code>dict[str, Any]</code> <p>RLE encoded mask as a dictionary.</p> required <p>Returns:</p> Type Description <code>dict[str, Any | str]</code> <p>Compressed RLE encoded mask as a string.</p> Source code in <code>pixano_inference/utils/image.py</code> <pre><code>def compress_rle(rle: dict[str, Any]) -&gt; dict[str, Any | str]:\n    \"\"\"Compress an RLE encoded mask.\n\n    Args:\n        rle: RLE encoded mask as a dictionary.\n\n    Returns:\n        Compressed RLE encoded mask as a string.\n    \"\"\"\n    counts = np.array(rle[\"counts\"], dtype=np.uint32).tobytes()\n    rle[\"counts\"] = base64.b64encode(counts).decode(\"utf-8\")\n    return rle\n</code></pre>"},{"location":"api_reference/utils/image/#pixano_inference.utils.image.convert_string_to_image","title":"<code>convert_string_to_image(str_image)</code>","text":"<p>Convert a string or path to an image.</p> <p>Parameters:</p> Name Type Description Default <code>str_image</code> <code>str | Path</code> <p>Image as a string or path.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>Image.</p> Source code in <code>pixano_inference/utils/image.py</code> <pre><code>def convert_string_to_image(str_image: str | Path) -&gt; Image.Image:\n    \"\"\"Convert a string or path to an image.\n\n    Args:\n        str_image: Image as a string or path.\n\n    Returns:\n        Image.\n    \"\"\"\n    if isinstance(str_image, str):\n        if is_url(str_image):\n            image_pil = Image.open(requests.get(str_image, stream=True).raw)\n        else:\n            if is_base64_image(str_image):\n                image_bytes = base64.b64decode(extract_image_from_base64(str_image))\n                image_pil = Image.open(BytesIO(image_bytes))\n            elif Path(str_image).exists():\n                image_pil = Image.open(str_image)\n            else:\n                raise ValueError(\"The image is not a valid path, URL or base64 string.\")\n    elif isinstance(str_image, Path):\n        image_pil = Image.open(str_image)\n    else:\n        raise ValueError(\"The image is not a valid path, URL or base64 string.\")\n    image_converted = image_pil.convert(\"RGB\")\n    return image_converted\n</code></pre>"},{"location":"api_reference/utils/image/#pixano_inference.utils.image.decode_rle_to_mask","title":"<code>decode_rle_to_mask(rle)</code>","text":"<p>Decode an RLE encoded mask.</p> <p>Parameters:</p> Name Type Description Default <code>rle</code> <code>dict</code> <p>RLE encoded mask as a dictionary.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Decoded binary mask of shape (height, width).</p> Source code in <code>pixano_inference/utils/image.py</code> <pre><code>def decode_rle_to_mask(rle: dict) -&gt; np.ndarray:\n    \"\"\"Decode an RLE encoded mask.\n\n    Args:\n        rle: RLE encoded mask as a dictionary.\n\n    Returns:\n        Decoded binary mask of shape (height, width).\n    \"\"\"\n    height, width = rle[\"size\"]\n    mask = np.empty(height * width, dtype=bool)\n    idx = 0\n    parity = False\n    for count in rle[\"counts\"]:\n        mask[idx : idx + count] = parity\n        idx += count\n        parity = not parity\n    mask = mask.reshape(width, height)\n    return mask.transpose()  # Reshape to original shape\n</code></pre>"},{"location":"api_reference/utils/image/#pixano_inference.utils.image.decompress_rle","title":"<code>decompress_rle(rle)</code>","text":"<p>Decompress a compressed RLE encoded mask.</p> <p>Parameters:</p> Name Type Description Default <code>rle</code> <code>dict[str, Any]</code> <p>Compressed RLE encoded mask as a string.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Decompressed RLE encoded mask as a dictionary.</p> Source code in <code>pixano_inference/utils/image.py</code> <pre><code>def decompress_rle(rle: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Decompress a compressed RLE encoded mask.\n\n    Args:\n        rle: Compressed RLE encoded mask as a string.\n\n    Returns:\n        Decompressed RLE encoded mask as a dictionary.\n    \"\"\"\n    rle[\"counts\"] = np.frombuffer(base64.b64decode(rle[\"counts\"]), dtype=np.uint32).tolist()\n    return rle\n</code></pre>"},{"location":"api_reference/utils/image/#pixano_inference.utils.image.encode_mask_to_rle","title":"<code>encode_mask_to_rle(mask)</code>","text":"<p>Encode a binary mask using RLE.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Tensor</code> <p>A binary mask of shape (height, width).</p> required <p>Returns:</p> Type Description <code>dict[str, list[int]]</code> <p>RLE encoded mask as a dictionary.</p> Source code in <code>pixano_inference/utils/image.py</code> <pre><code>def encode_mask_to_rle(mask: \"Tensor\") -&gt; dict[str, list[int]]:\n    \"\"\"Encode a binary mask using RLE.\n\n    Args:\n        mask: A binary mask of shape (height, width).\n\n    Returns:\n        RLE encoded mask as a dictionary.\n    \"\"\"\n    assert_torch_installed()\n    rle = {\"counts\": [], \"size\": list(mask.shape)}\n    mask = mask.permute(1, 0).flatten()\n    diff_arr = torch.diff(mask)\n    nonzero_indices = torch.where(diff_arr != 0)[0] + 1\n    lengths = torch.diff(torch.concatenate((torch.tensor([0]), nonzero_indices, torch.tensor([len(mask)]))))\n\n    # note that the odd counts are always the numbers of zeros\n    if mask[0] == 1:\n        lengths = torch.concatenate(([0], lengths))\n\n    rle[\"counts\"] = lengths.tolist()\n\n    return rle\n</code></pre>"},{"location":"api_reference/utils/image/#pixano_inference.utils.image.extract_image_from_base64","title":"<code>extract_image_from_base64(image)</code>","text":"<p>Extract from a base64 image the actual base64 part.</p> Source code in <code>pixano_inference/utils/image.py</code> <pre><code>def extract_image_from_base64(image: str):\n    \"\"\"Extract from a base64 image the actual base64 part.\"\"\"\n    match = re.match(regex_base64, image)\n    if match is None:\n        return \"\"\n    return image[len(match.group(1)) :]\n</code></pre>"},{"location":"api_reference/utils/image/#pixano_inference.utils.image.is_base64_image","title":"<code>is_base64_image(image)</code>","text":"<p>Check if a string is a base64 image.</p> <p>The expected format is \"data:image/{image_format};base64,{base64}\".</p> Source code in <code>pixano_inference/utils/image.py</code> <pre><code>def is_base64_image(image: str) -&gt; bool:\n    \"\"\"Check if a string is a base64 image.\n\n    The expected format is \"data:image/{image_format};base64,{base64}\".\n    \"\"\"\n    match = re.match(regex_base64, image)\n    return match is not None\n</code></pre>"},{"location":"api_reference/utils/package/","title":"package","text":""},{"location":"api_reference/utils/package/#pixano_inference.utils.package","title":"<code>pixano_inference.utils.package</code>","text":"<p>Utility functions for working with Python packages.</p>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.assert_lance_installed","title":"<code>assert_lance_installed()</code>","text":"<p>Assert that the lance package is installed.</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def assert_lance_installed() -&gt; None:\n    \"\"\"Assert that the lance package is installed.\"\"\"\n    assert_package_installed(\n        \"lance\", \"lance is not installed. Please install it using 'pip install pixano-inference[data]'.\"\n    )\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.assert_package_installed","title":"<code>assert_package_installed(package_name, error_message=None)</code>","text":"<p>Assert that a Python package is installed.</p> <p>Parameters:</p> Name Type Description Default <code>package_name</code> <code>str</code> <p>The name of the package to check.</p> required <code>error_message</code> <code>str | None</code> <p>The error message to raise if the package is not installed.</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the package is not installed</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def assert_package_installed(package_name: str, error_message: str | None = None) -&gt; None:\n    \"\"\"Assert that a Python package is installed.\n\n    Args:\n        package_name: The name of the package to check.\n        error_message: The error message to raise if the package is not installed.\n\n    Raises:\n        ImportError: If the package is not installed\n    \"\"\"\n    if not is_package_installed(package_name):\n        message = error_message or f\"Package '{package_name}' is not installed.\"\n        raise ImportError(message)\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.assert_sam2_installed","title":"<code>assert_sam2_installed()</code>","text":"<p>Assert that the sam2 package is installed.</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def assert_sam2_installed() -&gt; None:\n    \"\"\"Assert that the sam2 package is installed.\"\"\"\n    assert_package_installed(\n        \"sam2\",\n        \"sam2 is not installed. Please install it using 'pip install git+https://github.com/facebookresearch/sam2.git'.\",\n    )\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.assert_torch_installed","title":"<code>assert_torch_installed()</code>","text":"<p>Assert that the torch package is installed.</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def assert_torch_installed() -&gt; None:\n    \"\"\"Assert that the torch package is installed.\"\"\"\n    assert_package_installed(\n        \"torch\", \"torch is not installed. Please install it using 'pip install pixano-inference[torch]'.\"\n    )\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.assert_transformers_installed","title":"<code>assert_transformers_installed()</code>","text":"<p>Assert that the transformers package is installed.</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def assert_transformers_installed() -&gt; None:\n    \"\"\"Assert that the transformers package is installed.\"\"\"\n    assert_package_installed(\n        \"transformers\",\n        \"transformers is not installed. Please install it using 'pip install pixano-inference[transformers]'.\",\n    )\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.assert_vllm_installed","title":"<code>assert_vllm_installed()</code>","text":"<p>Assert that the vllm package is installed.</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def assert_vllm_installed() -&gt; None:\n    \"\"\"Assert that the vllm package is installed.\"\"\"\n    assert_package_installed(\n        \"vllm\", \"vLLM is not installed. Please install it using 'pip install pixano-inference[vllm]'.\"\n    )\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.is_lance_installed","title":"<code>is_lance_installed()</code>","text":"<p>Check if the lance package is installed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the lance package is installed, False otherwise</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def is_lance_installed() -&gt; bool:\n    \"\"\"Check if the lance package is installed.\n\n    Returns:\n        True if the lance package is installed, False otherwise\n    \"\"\"\n    return is_package_installed(\"lance\")\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.is_package_installed","title":"<code>is_package_installed(package_name)</code>","text":"<p>Check if a Python package is installed.</p> <p>Parameters:</p> Name Type Description Default <code>package_name</code> <code>str</code> <p>The name of the package to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the package is installed, False otherwise</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def is_package_installed(package_name: str) -&gt; bool:\n    \"\"\"Check if a Python package is installed.\n\n    Args:\n        package_name: The name of the package to check.\n\n    Returns:\n        True if the package is installed, False otherwise\n    \"\"\"\n    package_spec = importlib.util.find_spec(package_name)\n    return package_spec is not None\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.is_sam2_installed","title":"<code>is_sam2_installed()</code>","text":"<p>Check if the sam2 package is installed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the sam2 package is installed, False otherwise</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def is_sam2_installed() -&gt; bool:\n    \"\"\"Check if the sam2 package is installed.\n\n    Returns:\n        True if the sam2 package is installed, False otherwise\n    \"\"\"\n    return is_package_installed(\"sam2\")\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.is_torch_installed","title":"<code>is_torch_installed()</code>","text":"<p>Check if the torch package is installed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the torch package is installed, False otherwise</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def is_torch_installed() -&gt; bool:\n    \"\"\"Check if the torch package is installed.\n\n    Returns:\n        True if the torch package is installed, False otherwise\n    \"\"\"\n    return is_package_installed(\"torch\")\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.is_transformers_installed","title":"<code>is_transformers_installed()</code>","text":"<p>Check if the transformers package is installed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the transformers package is installed, False otherwise</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def is_transformers_installed() -&gt; bool:\n    \"\"\"Check if the transformers package is installed.\n\n    Returns:\n        True if the transformers package is installed, False otherwise\n    \"\"\"\n    return is_package_installed(\"transformers\")\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.is_vllm_installed","title":"<code>is_vllm_installed()</code>","text":"<p>Check if the vllm package is installed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the vLLM package is installed, False otherwise</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def is_vllm_installed() -&gt; bool:\n    \"\"\"Check if the vllm package is installed.\n\n    Returns:\n        True if the vLLM package is installed, False otherwise\n    \"\"\"\n    return is_package_installed(\"vllm\")\n</code></pre>"},{"location":"api_reference/utils/url/","title":"url","text":""},{"location":"api_reference/utils/url/#pixano_inference.utils.url","title":"<code>pixano_inference.utils.url</code>","text":"<p>URL utils.</p>"},{"location":"api_reference/utils/url/#pixano_inference.utils.url.is_url","title":"<code>is_url(url)</code>","text":"<p>Check if a string is a valid URL.</p> Source code in <code>pixano_inference/utils/url.py</code> <pre><code>def is_url(url: str) -&gt; bool:\n    \"\"\"Check if a string is a valid URL.\"\"\"\n    return re.match(url_validation_regex, url) is not None\n</code></pre>"},{"location":"api_reference/utils/vector/","title":"vector","text":""},{"location":"api_reference/utils/vector/#pixano_inference.utils.vector","title":"<code>pixano_inference.utils.vector</code>","text":"<p>Utility functions for vector operations.</p>"},{"location":"api_reference/utils/vector/#pixano_inference.utils.vector.vector_to_tensor","title":"<code>vector_to_tensor(vector)</code>","text":"<p>Convert a vector to a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>vector</code> <code>NDArrayFloat | LanceVector | 'Tensor' | None</code> <p>Vector to convert.</p> required Source code in <code>pixano_inference/utils/vector.py</code> <pre><code>def vector_to_tensor(vector: NDArrayFloat | LanceVector | \"Tensor\" | None) -&gt; \"Tensor\":\n    \"\"\"Convert a vector to a tensor.\n\n    Args:\n        vector: Vector to convert.\n    \"\"\"\n    assert_torch_installed()\n    if not isinstance(vector, (NDArrayFloat, LanceVector, Tensor)) and vector is not None:\n        raise ValueError(f\"Unsupported vector type: {type(vector)}\")\n\n    if isinstance(vector, LanceVector):\n        return vector.read_vector()\n    elif isinstance(vector, NDArrayFloat):\n        return vector.to_torch()\n    return vector\n</code></pre>"},{"location":"getting_started/","title":"Index","text":""},{"location":"getting_started/#getting-started-with-pixano-inference","title":"Getting started with Pixano-Inference","text":""},{"location":"getting_started/#installation","title":"Installation","text":"<p>To install the library, simply execute the following command</p> <pre><code>pip install pixano-inference\n</code></pre> <p>If you want to dynamically make changes to the library to develop and test, make a dev install by cloning the repo and executing the following commands</p> <pre><code>cd pixano-inference\npip install -e .\n</code></pre> <p>To have access to the providers available in Pixano-Inference, make sure to install the relevant dependencies. For example to use the vLLM provider run:</p> <pre><code>pip install pixano-inference[vllm]\n</code></pre>"},{"location":"getting_started/#usage","title":"Usage","text":""},{"location":"getting_started/#run-the-application","title":"Run the application","text":"<p>Pixano-Inference can invoke a server that will serve the API. To do so, simply execute the following command:</p> <pre><code>pixano-inference --port 8000\n</code></pre> <p>The default port is <code>8000</code>. You can change it by passing the <code>--port</code> argument.</p>"},{"location":"getting_started/#instantiate-the-client","title":"Instantiate the client","text":"<p>The easiest way to interact with Pixano-Inference is through the Python client.</p> <pre><code>from pixano_inference.client import PixanoInferenceClient\n\n\nclient = PixanoInferenceClient(url=\"http://localhost:8000\")\n</code></pre>"},{"location":"getting_started/#instantiate-a-model","title":"Instantiate a model","text":"<p>To instantiate a model, you need to provide the path of the model file and the task that the model will perform. For example to run a Llava model from the vLLM provider:</p> <pre><code>from pixano_inference.pydantic import ModelConfig\nfrom pixano_inference.tasks import MultimodalImageNLPTask\n\n\nclient.instantiate_model(\n    provider=\"vllm\",\n    config=ModelConfig(\n        name=\"llava\",\n        task=MultimodalImageNLPTask.CONDITIONAL_GENERATION.value,\n        path=\"llava-hf/llava-v1.6-mistral-7b-hf\",\n        config={\n            \"dtype\": \"bfloat16\",\n        },\n    ),\n)\n</code></pre>"},{"location":"getting_started/#run-an-inference","title":"Run an inference","text":"<p>The client provide methods to run the different models on various tasks. For image-text conditional generation using Llava here is the relevant method:</p> <pre><code>from pixano_inference.pydantic import TextImageConditionalGenerationRequest, TextImageConditionalGenerationResponse\n\n\nfrom pixano_inference.pydantic import TextImageConditionalGenerationRequest\n\n\nrequest = TextImageConditionalGenerationRequest(\n    model=\"llava\",\n    prompt=[\n        {\n            \"content\": [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/9/9e/Ours_brun_parcanimalierpyrenees_1.jpg\"\n                    },\n                },\n                {\"type\": \"text\", \"text\": \"What is displayed in this image ? Answer concisely. \"},\n            ],\n            \"role\": \"user\",\n        }\n    ],\n    image_path=\"/path/to/image.jpg\",\n    max_new_tokens=100,\n)\n\n\nresponse: TextImageConditionalGenerationResponse = client.text_image_conditional_generation(request)\nprint(response.data.generated_text)\n</code></pre>"},{"location":"home/","title":"Index","text":""},{"location":"home/#pixano-inference","title":"Pixano-Inference","text":""},{"location":"home/#overview","title":"Overview","text":"<p>Pixano-Inference is a Python library that provides an API for performing inference tasks on multi-modal data such as images, videos, and text. It includes model and API providers for various tasks, including mask generation, text-image conditional generation.</p> <p>Pixano-Inference aims to provide a simple and easy-to-use interface for developers to interact with models solving different tasks and from various providers from a common interface via its RESTful API.</p>"},{"location":"home/#how-to-use-pixano-inference","title":"How to use Pixano-Inference","text":"<p>Please refer to the Getting Started guide for instructions on how to install, configure, and use Pixano-Inference in your project.</p> <p>For going in-depth on how to use the APIs, please refer to the API Reference.</p>"},{"location":"home/#contributing","title":"Contributing","text":"<p>We welcome contributions from the community! Please open issues and pull requests for any bugs or feature requests you may have.</p>"},{"location":"home/#license","title":"License","text":"<p>Pixano-Inference is released under the terms of the CeCILL-C license.</p>"}]}