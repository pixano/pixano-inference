{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#documentation-of-pixano-inference","title":"Documentation of Pixano-Inference","text":""},{"location":"#mkdocs","title":"Mkdocs","text":"<p>Pixano-Inference documentation relies on MkDocs which offers a great set of features to generate documentations using Markdown.</p>"},{"location":"#locally-serve","title":"Locally serve","text":"<p>To serve locally the doc you need to install the dependencies:</p> <pre><code>cd pixano-inference\n\npip install \".[docs]\"\n</code></pre> <p>Then you can serve it using MkDocs:</p> <pre><code>mkdocs serve -a localhost:8000\n</code></pre> <p>It will listen to modifications in the <code>docs/</code> folder and the <code>mkdocs.yml</code> configuration file.</p>"},{"location":"api_reference/","title":"index","text":""},{"location":"api_reference/#pixano-inference-api-reference","title":"Pixano Inference API reference","text":""},{"location":"api_reference/#client-module","title":"Client module","text":"<p>The client module contains the class for the API client. It is responsible for making requests to the API endpoints.</p>"},{"location":"api_reference/#data-module","title":"Data module","text":"<p>The data module contains the functions to read (and later write) data from/to a database or file.</p>"},{"location":"api_reference/#model-registry-module","title":"Model registry module","text":"<p>The model registry module contains the functions to register a model to the application.</p>"},{"location":"api_reference/#models-module","title":"Models module","text":"<p>The models module contains the inference models to perform the tasks Pixano Inference API is designed for.</p> <p>The models include:</p> <ul> <li><code>BaseInferenceModel</code>: Base class for all Pixano Inference API models.</li> <li><code>Sam2Model</code>: Model used to detect and segment objects in images and videos.</li> <li><code>TransformerModel</code>: Model instantiated from Transformers.</li> <li><code>VLLMModel</code>: Model instantiated from VLLM.</li> </ul>"},{"location":"api_reference/#providers-module","title":"Providers module","text":"<p>The providers module contains the functions to load a model and to perform inference either from a model provider or from an API provider.</p> <p>The providers include:</p> <ul> <li><code>BaseProvider</code>: Base class for all Pixano Inference API providers.</li> <li><code>Sam2Provider</code>: Provider used to instantiate a <code>Sam2Model</code> and call its methods.</li> <li><code>TransformersProvider</code>: Provider used to instantiate a <code>TransformerModel</code> and call its methods.</li> <li><code>VLLMProvider</code>: Provider used to instantiate a <code>VLLMModel</code> and call its methods.</li> </ul>"},{"location":"api_reference/#pydantic-module","title":"Pydantic module","text":"<p>The pydantic module contains the classes for data validation. It is used by the models, providers, and the application itself to validate the input/output of the API.</p>"},{"location":"api_reference/#routers-module","title":"Routers module","text":"<p>The routers module contains the routers for the API. Each router has a path prefix that defines the endpoint where it will be mounted in the API.</p> <p>The routers swagger is accessible at at the <code>/docs</code> endpoint.</p>"},{"location":"api_reference/#settings-module","title":"Settings module","text":"<p>The settings module contains the configuration of the application.</p>"},{"location":"api_reference/#tasks-module","title":"Tasks module","text":"<p>The tasks module contains the enums used to define the task that a model can perform.</p>"},{"location":"api_reference/#utils-module","title":"Utils module","text":"<p>The utils module contains the functions and classes used by the other modules.</p>"},{"location":"api_reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>index</li> <li>celery</li> <li>client</li> <li>data<ul> <li>read_vector_databases</li> </ul> </li> <li>main</li> <li>models<ul> <li>base</li> <li>sam2</li> <li>transformers</li> <li>vllm</li> </ul> </li> <li>providers<ul> <li>base</li> <li>registry</li> <li>sam2</li> <li>transformers</li> <li>utils</li> <li>vllm</li> </ul> </li> <li>pydantic<ul> <li>base</li> <li>data<ul> <li>vector_database</li> </ul> </li> <li>models</li> <li>nd_array</li> <li>tasks<ul> <li>image<ul> <li>mask_generation</li> <li>utils</li> <li>zero_shot_detection</li> </ul> </li> <li>multimodal<ul> <li>conditional_generation</li> </ul> </li> <li>video<ul> <li>mask_generation</li> </ul> </li> </ul> </li> </ul> </li> <li>routers<ul> <li>app</li> <li>providers</li> <li>tasks<ul> <li>image</li> <li>multimodal</li> <li>nlp</li> <li>tasks</li> <li>video</li> </ul> </li> <li>utils</li> </ul> </li> <li>settings</li> <li>tasks<ul> <li>image</li> <li>multimodal</li> <li>nlp</li> <li>task</li> <li>utils</li> <li>video</li> </ul> </li> <li>utils<ul> <li>media</li> <li>package</li> <li>url</li> <li>vector</li> </ul> </li> </ul>"},{"location":"api_reference/celery/","title":"celery","text":""},{"location":"api_reference/celery/#pixano_inference.celery","title":"<code>pixano_inference.celery</code>","text":"<p>Celery configuration file.</p>"},{"location":"api_reference/celery/#pixano_inference.celery.add_celery_worker_and_queue","title":"<code>add_celery_worker_and_queue(provider, model_config, gpu)</code>","text":"<p>Add a new worker and a queue to the celery app to handle model.</p> Source code in <code>pixano_inference/celery.py</code> <pre><code>def add_celery_worker_and_queue(provider: str, model_config: ModelConfig, gpu: int | None) -&gt; CeleryTask:\n    \"\"\"Add a new worker and a queue to the celery app to handle model.\"\"\"\n    queue = model_queue_name(model_config.name)\n    celery_app.control.add_consumer(queue=model_queue_name(model_config.name), reply=True)\n\n    command = [\n        sys.executable,\n        \"-m\",\n        \"celery\",\n        \"-A\",\n        \"pixano_inference.celery.celery_app\",\n        \"worker\",\n        \"--loglevel=INFO\",\n        \"-Q\",\n        queue,\n        \"--pool=solo\",\n    ]\n    worker = Popen(\n        command,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        start_new_session=True,\n    )\n\n    uvicorn_logger.info(f\"Spawned Celery worker {worker.pid} handling model {model_config.name}.\")\n    queues_to_workers[queue] = worker\n\n    task: AsyncResult = instantiate_model.apply_async(\n        (provider, jsonable_encoder(model_config.model_dump()), gpu), queue=queue\n    )\n    task_result, result = list(task.collect())[0]\n\n    return CeleryTask(id=task.id, status=task_result.status)\n</code></pre>"},{"location":"api_reference/celery/#pixano_inference.celery.create_celery_app","title":"<code>create_celery_app()</code>","text":"<p>Create a new celery app.</p> Source code in <code>pixano_inference/celery.py</code> <pre><code>def create_celery_app() -&gt; Celery:\n    \"\"\"Create a new celery app.\"\"\"\n    redis_url: str = os.environ.get(\"REDIS_URL\", \"localhost\")\n    redis_port: int = int(os.environ.get(\"REDIS_PORT\", 6379))\n    redis_db_number: int = int(os.environ.get(\"REDIS_DB_NUMBER\", 0))\n\n    redis_url = f\"redis://{redis_url}:{redis_port}/{redis_db_number}\"\n\n    app = Celery(__name__, broker=redis_url, backend=redis_url)\n\n    return app\n</code></pre>"},{"location":"api_reference/celery/#pixano_inference.celery.delete_celery_worker_and_queue","title":"<code>delete_celery_worker_and_queue(model_name)</code>","text":"<p>Delete a worker and a queue of the celery app that handled a model.</p> Source code in <code>pixano_inference/celery.py</code> <pre><code>def delete_celery_worker_and_queue(model_name: str):\n    \"\"\"Delete a worker and a queue of the celery app that handled a model.\"\"\"\n    queue = model_queue_name(model_name)\n\n    command = [\n        sys.executable,\n        \"-m\",\n        \"celery\",\n        \"-Q\",\n        queue,\n        \"purge\",\n    ]\n    purge_process = Popen(\n        command,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        start_new_session=True,\n    )\n    purge_process.wait()\n    purge_process.kill()\n    try:\n        worker = queues_to_workers.pop(queue)\n    except KeyError:  # Instantiation failed before storing the worker\n        pass\n    else:\n        delete_model.apply_async(queue=queue).get()\n        os.killpg(os.getpgid(worker.pid), signal.SIGTERM)\n        worker.wait()\n        uvicorn_logger.info(f\"Killed Celery worker {worker.pid} handling model {model_name}.\")\n    celery_app.control.cancel_consumer(queue=queue)\n</code></pre>"},{"location":"api_reference/celery/#pixano_inference.celery.delete_model","title":"<code>delete_model()</code>","text":"<p>Delete model.</p> Source code in <code>pixano_inference/celery.py</code> <pre><code>@celery_app.task\ndef delete_model() -&gt; None:\n    \"\"\"Delete model.\"\"\"\n    global worker_model\n\n    try:\n        worker_model.delete()\n    except NameError:\n        pass\n</code></pre>"},{"location":"api_reference/celery/#pixano_inference.celery.instantiate_model","title":"<code>instantiate_model(provider, model_config, gpu)</code>","text":"<p>Instantiate a model.</p> Source code in <code>pixano_inference/celery.py</code> <pre><code>@celery_app.task\ndef instantiate_model(provider: str, model_config: dict[str, Any], gpu: int | None) -&gt; None:\n    \"\"\"Instantiate a model.\"\"\"\n    global worker_provider, worker_model, worker_task, is_model_initialized\n\n    if is_model_initialized:\n        raise ValueError(\"Do not initialize twice a model.\")\n    is_model_initialized = True\n\n    assert_torch_installed()\n    device = torch.device(f\"cuda:{gpu}\") if gpu is not None else torch.device(\"cpu\")\n    worker_provider = instantiate_provider(provider)\n    worker_provider = cast(ModelProvider, worker_provider)\n    worker_model = worker_provider.load_model(**model_config, device=device)\n    worker_task = str_to_task(model_config[\"task\"])\n</code></pre>"},{"location":"api_reference/celery/#pixano_inference.celery.model_queue_name","title":"<code>model_queue_name(model_name)</code>","text":"<p>Get the name of the queue for a given model.</p> Source code in <code>pixano_inference/celery.py</code> <pre><code>def model_queue_name(model_name: str) -&gt; str:\n    \"\"\"Get the name of the queue for a given model.\"\"\"\n    return f\"{model_name}_queue\"\n</code></pre>"},{"location":"api_reference/celery/#pixano_inference.celery.predict","title":"<code>predict(request)</code>","text":"<p>Run a model inference from the request.</p> Source code in <code>pixano_inference/celery.py</code> <pre><code>@celery_app.task\ndef predict(request: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Run a model inference from the request.\"\"\"\n    global worker_provider, worker_model, worker_task\n\n    start_time = time.time()\n    match worker_task:\n        case ImageTask.MASK_GENERATION:\n            output = worker_provider.image_mask_generation(\n                request=ImageMaskGenerationRequest.model_construct(request), model=worker_model\n            )\n        case MultimodalImageNLPTask.CONDITIONAL_GENERATION:\n            output = worker_provider.text_image_conditional_generation(\n                request=TextImageConditionalGenerationRequest.model_construct(**request), model=worker_model\n            )\n        case VideoTask.MASK_GENERATION:\n            output = worker_provider.video_mask_generation(\n                request=VideoMaskGenerationRequest.model_construct(**request), model=worker_model\n            )\n        case ImageTask.ZERO_SHOT_DETECTION:\n            output = worker_provider.image_zero_shot_detection(\n                request=ImageZeroShotDetectionRequest.model_construct(**request), model=worker_model\n            )\n        case _:\n            raise ValueError(f\"Unknown task: {worker_task}\")\n    response = {\n        \"timestamp\": datetime.now(),\n        \"processing_time\": time.time() - start_time,\n        \"metadata\": worker_model.metadata,\n        \"data\": output.model_dump(),\n    }\n    return response\n</code></pre>"},{"location":"api_reference/client/","title":"client","text":""},{"location":"api_reference/client/#pixano_inference.client","title":"<code>pixano_inference.client</code>","text":"<p>Pixano inference client.</p>"},{"location":"api_reference/client/#pixano_inference.client.InferenceTooLongError","title":"<code>InferenceTooLongError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exeption when inference took too long.</p>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient","title":"<code>PixanoInferenceClient(**data)</code>","text":"<p>               Bases: <code>Settings</code></p> <p>Pixano Inference Client.</p> Source code in <code>pixano_inference/settings.py</code> <pre><code>def __init__(self, **data: Any):\n    \"\"\"Initialize the settings.\"\"\"\n    if \"num_cpus\" not in data:\n        data[\"num_cpus\"] = os.cpu_count()\n    if \"num_gpus\" not in data:\n        if is_torch_installed():\n            if torch.cuda.is_available():\n                data[\"num_gpus\"] = torch.cuda.device_count()\n            else:\n                data[\"num_gpus\"] = 0\n        else:\n            data[\"num_gpus\"] = 0\n\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient._rest_call","title":"<code>_rest_call(path, method, timeout=60, **kwargs)</code>  <code>async</code>","text":"<p>Perform a REST call to the pixano inference server.</p> Source code in <code>pixano_inference/client.py</code> <pre><code>async def _rest_call(\n    self,\n    path: str,\n    method: Literal[\"GET\", \"POST\", \"PUT\", \"DELETE\"],\n    timeout: int = 60,\n    **kwargs,\n) -&gt; Response:\n    \"\"\"Perform a REST call to the pixano inference server.\"\"\"\n    async with httpx.AsyncClient(timeout=timeout) as client:\n        match method:\n            case \"GET\":\n                request_fn = client.get\n            case \"POST\":\n                request_fn = client.post\n            case \"PUT\":\n                request_fn = client.put\n            case \"DELETE\":\n                request_fn = client.delete\n            case _:\n                raise ValueError(\n                    f\"Invalid REST call method. Expected one of ['GET', 'POST', 'PUT', 'DELETE'], but got \"\n                    f\"'{method}'.\"\n                )\n\n        if path.startswith(\"/\"):\n            path = path[1:]\n\n        url = f\"{self.url}/{path}\"\n        response = await request_fn(url, **kwargs)\n        raise_if_error(response)\n\n        return response\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.connect","title":"<code>connect(url)</code>  <code>staticmethod</code>","text":"<p>Connect to pixano inference.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the pixano inference server.</p> required Source code in <code>pixano_inference/client.py</code> <pre><code>@staticmethod\ndef connect(url: str) -&gt; \"PixanoInferenceClient\":\n    \"\"\"Connect to pixano inference.\n\n    Args:\n        url: The URL of the pixano inference server.\n    \"\"\"\n    settings = Settings.model_validate(requests.get(f\"{url}/app/settings/\").json())\n    client = PixanoInferenceClient(url=url, **settings.model_dump())\n    return client\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.delete","title":"<code>delete(path, **kwargs)</code>  <code>async</code>","text":"<p>Perform a DELETE request to the pixano inference server.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the request.</p> required <code>kwargs</code> <code>Any</code> <p>The keyword arguments to pass to the request or httpx client.</p> <code>{}</code> Source code in <code>pixano_inference/client.py</code> <pre><code>async def delete(self, path: str, **kwargs: Any) -&gt; Response:\n    \"\"\"Perform a DELETE request to the pixano inference server.\n\n    Args:\n        path: The path of the request.\n        kwargs: The keyword arguments to pass to the request or httpx client.\n    \"\"\"\n    return await self._rest_call(path=path, method=\"DELETE\", **kwargs)\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.delete_model","title":"<code>delete_model(model_name)</code>  <code>async</code>","text":"<p>Delete a model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model.</p> required Source code in <code>pixano_inference/client.py</code> <pre><code>async def delete_model(self, model_name: str) -&gt; None:\n    \"\"\"Delete a model.\n\n    Args:\n        model_name: The name of the model.\n    \"\"\"\n    await self.delete(f\"providers/model/{model_name}\")\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.get","title":"<code>get(path, **kwargs)</code>  <code>async</code>","text":"<p>Perform a GET request to the pixano inference server.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the request.</p> required <code>kwargs</code> <code>Any</code> <p>The keyword arguments to pass to the request or httpx client.</p> <code>{}</code> Source code in <code>pixano_inference/client.py</code> <pre><code>async def get(self, path: str, **kwargs: Any) -&gt; Response:\n    \"\"\"Perform a GET request to the pixano inference server.\n\n    Args:\n        path: The path of the request.\n        kwargs: The keyword arguments to pass to the request or httpx client.\n    \"\"\"\n    return await self._rest_call(path=path, method=\"GET\", **kwargs)\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.get_settings","title":"<code>get_settings()</code>  <code>async</code>","text":"<p>Get the settings for the pixano inference server.</p> Source code in <code>pixano_inference/client.py</code> <pre><code>async def get_settings(self) -&gt; Settings:\n    \"\"\"Get the settings for the pixano inference server.\"\"\"\n    response = await self.get(\"app/settings/\")\n    raise_if_error(response)\n    return Settings(**response.json())\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.image_mask_generation","title":"<code>image_mask_generation(request=None, poll_interval=0.1, timeout=60, task_id=None, asynchronous=False)</code>  <code>async</code>","text":"<pre><code>image_mask_generation(request: ImageMaskGenerationRequest | None, poll_interval: float, timeout: float, task_id: str, asynchronous: Literal[True]) -&gt; ImageMaskGenerationResponse | CeleryTask\n</code></pre><pre><code>image_mask_generation(request: ImageMaskGenerationRequest | None, poll_interval: float, timeout: float, task_id: None, asynchronous: Literal[True]) -&gt; CeleryTask\n</code></pre><pre><code>image_mask_generation(request: ImageMaskGenerationRequest, poll_interval: float, timeout: float, task_id: str | None, asynchronous: Literal[False]) -&gt; ImageMaskGenerationResponse\n</code></pre><pre><code>image_mask_generation(request: ImageMaskGenerationRequest | None, poll_interval: float, timeout: float, task_id: str | None, asynchronous: bool) -&gt; ImageMaskGenerationResponse | CeleryTask\n</code></pre> <p>Perform an inference to perform image mask generation.</p> Source code in <code>pixano_inference/client.py</code> <pre><code>async def image_mask_generation(\n    self,\n    request: ImageMaskGenerationRequest | None = None,\n    poll_interval: float = 0.1,\n    timeout: float = 60,\n    task_id: str | None = None,\n    asynchronous: bool = False,\n) -&gt; ImageMaskGenerationResponse | CeleryTask:\n    \"\"\"Perform an inference to perform image mask generation.\"\"\"\n    return await self.inference(\n        route=\"tasks/image/mask_generation/\",\n        request=request,\n        response_type=ImageMaskGenerationResponse,\n        poll_interval=poll_interval,\n        timeout=timeout,\n        task_id=task_id,\n        asynchronous=asynchronous,\n    )\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.image_zero_shot_detection","title":"<code>image_zero_shot_detection(request=None, poll_interval=0.1, timeout=60, task_id=None, asynchronous=False)</code>  <code>async</code>","text":"<pre><code>image_zero_shot_detection(request: ImageZeroShotDetectionRequest | None, poll_interval: float, timeout: float, task_id: str, asynchronous: Literal[True]) -&gt; ImageZeroShotDetectionResponse | CeleryTask\n</code></pre><pre><code>image_zero_shot_detection(request: ImageZeroShotDetectionRequest | None, poll_interval: float, timeout: float, task_id: None, asynchronous: Literal[True]) -&gt; CeleryTask\n</code></pre><pre><code>image_zero_shot_detection(request: ImageZeroShotDetectionRequest, poll_interval: float, timeout: float, task_id: str | None, asynchronous: Literal[False]) -&gt; ImageZeroShotDetectionResponse\n</code></pre><pre><code>image_zero_shot_detection(request: ImageZeroShotDetectionRequest | None, poll_interval: float, timeout: float, task_id: str | None, asynchronous: bool) -&gt; ImageZeroShotDetectionResponse | CeleryTask\n</code></pre> <p>Perform an inference to perform video mask generation.</p> Source code in <code>pixano_inference/client.py</code> <pre><code>async def image_zero_shot_detection(\n    self,\n    request: ImageZeroShotDetectionRequest | None = None,\n    poll_interval: float = 0.1,\n    timeout: float = 60,\n    task_id: str | None = None,\n    asynchronous: bool = False,\n) -&gt; ImageZeroShotDetectionResponse | CeleryTask:\n    \"\"\"Perform an inference to perform video mask generation.\"\"\"\n    return await self.inference(\n        route=\"tasks/image/zero_shot_detection/\",\n        request=request,\n        response_type=ImageZeroShotDetectionResponse,\n        poll_interval=poll_interval,\n        timeout=timeout,\n        task_id=task_id,\n        asynchronous=asynchronous,\n    )\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.inference","title":"<code>inference(route, request=None, response_type=None, poll_interval=0.1, timeout=60.0, task_id=None, asynchronous=False)</code>  <code>async</code>","text":"<pre><code>inference(route: str, request: BaseRequest | None, response_type: type[BaseResponse] | None, poll_interval: float, timeout: float, task_id: str, asynchronous: Literal[True]) -&gt; BaseResponse | CeleryTask\n</code></pre><pre><code>inference(route: str, request: BaseRequest | None, response_type: type[BaseResponse] | None, poll_interval: float, timeout: float, task_id: None, asynchronous: Literal[True]) -&gt; CeleryTask\n</code></pre><pre><code>inference(route: str, request: BaseRequest, response_type: type[BaseResponse], poll_interval: float, timeout: float, task_id: str | None, asynchronous: Literal[False]) -&gt; BaseResponse\n</code></pre><pre><code>inference(route: str, request: BaseRequest | None, response_type: type[BaseResponse] | None, poll_interval: float, timeout: float, task_id: str | None, asynchronous: bool) -&gt; BaseResponse | CeleryTask\n</code></pre> <p>Perform a POST request to the pixano inference server.</p> <p>Parameters:</p> Name Type Description Default <code>route</code> <code>str</code> <p>The root to the request.</p> required <code>request</code> <code>BaseRequest | None</code> <p>The request of the model.</p> <code>None</code> <code>response_type</code> <code>type[BaseResponse] | None</code> <p>The type of the response.</p> <code>None</code> <code>poll_interval</code> <code>float</code> <p>waiting time between subsequent requests to server to retrieve task results for synchronous requests.</p> <code>0.1</code> <code>timeout</code> <code>float</code> <p>Time to wait for response for synchronous requests. If reached, the request will be aborted.</p> <code>60.0</code> <code>task_id</code> <code>str | None</code> <p>The id of the task to poll for results.</p> <code>None</code> <code>asynchronous</code> <code>bool</code> <p>If True then the function will be called asynchronously and returns a CeleryTask object or poll results when task id is provided.</p> <code>False</code> <p>Returns:</p> Type Description <code>BaseResponse | CeleryTask</code> <p>A response from the pixano inference server.</p> Source code in <code>pixano_inference/client.py</code> <pre><code>async def inference(\n    self,\n    route: str,\n    request: BaseRequest | None = None,\n    response_type: type[BaseResponse] | None = None,\n    poll_interval: float = 0.1,\n    timeout: float = 60.0,\n    task_id: str | None = None,\n    asynchronous: bool = False,\n) -&gt; BaseResponse | CeleryTask:\n    \"\"\"Perform a POST request to the pixano inference server.\n\n    Args:\n        route: The root to the request.\n        request: The request of the model.\n        response_type: The type of the response.\n        poll_interval: waiting time between subsequent requests to server to retrieve task results for synchronous\n            requests.\n        timeout: Time to wait for response for synchronous requests. If reached, the request will be aborted.\n        task_id: The id of the task to poll for results.\n        asynchronous: If True then the function will be called asynchronously and returns a CeleryTask object or\n            poll results when task id is provided.\n\n    Returns:\n        A response from the pixano inference server.\n    \"\"\"\n    _validate_task_id_asynchronous_request_response_type(\n        task_id=task_id, asynchronous=asynchronous, request=request, response_type=response_type\n    )\n    _validate_poll_interval_timeout(poll_interval=poll_interval, timeout=timeout)\n\n    if not asynchronous or task_id is None:\n        request = cast(BaseRequest, request)\n        celery_response: Response = await self.post(route, json=request.model_dump())\n        celery_task: CeleryTask = CeleryTask.model_construct(**celery_response.json())\n\n    # Asynchronous calls\n    if asynchronous and task_id is None:\n        return celery_task\n    elif asynchronous and task_id is not None:\n        response_type = cast(type[BaseResponse], response_type)\n        has_slash = route.endswith(\"/\")\n        task_route = route + f\"{'' if has_slash else '/'}{task_id}\"\n        response: dict[str, Any] = (await self.get(task_route)).json()\n        if response[\"status\"] == states.SUCCESS:\n            return response_type.model_validate(response)\n        return CeleryTask.model_construct(**response)\n\n    # Synchronous calls with polling for result retrieval and deletion of celery tasks after timeout.\n    response_type = cast(type[BaseResponse], response_type)\n\n    time = 0.0\n    has_slash = route.endswith(\"/\")\n    task_route = route + f\"{'' if has_slash else '/'}{celery_task.id}\"\n    while time &lt; timeout:\n        response = (await self.get(task_route)).json()\n        if response[\"status\"] == states.SUCCESS:\n            return response_type.model_validate(response)\n        elif response[\"status\"] == states.FAILURE:\n            raise ValueError(\"The inference failed. Please check your inputs.\")\n        time += poll_interval\n        await asyncio.sleep(poll_interval)\n    await self.delete(task_route)\n    raise InferenceTooLongError(\"The model is either busy or the task takes too long to perform.\")\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.instantiate_model","title":"<code>instantiate_model(provider, config, timeout=60)</code>  <code>async</code>","text":"<p>Instantiate a model.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>The model provider.</p> required <code>config</code> <code>ModelConfig</code> <p>The configuration of the model.</p> required <code>timeout</code> <code>int</code> <p>The timeout to wait for a response. Please note that even if the timeout is reached, the request will not be aborted.</p> <code>60</code> Source code in <code>pixano_inference/client.py</code> <pre><code>async def instantiate_model(self, provider: str, config: ModelConfig, timeout: int = 60) -&gt; None:\n    \"\"\"Instantiate a model.\n\n    Args:\n        provider: The model provider.\n        config: The configuration of the model.\n        timeout: The timeout to wait for a response. Please note that even if the timeout is reached, the request\n            will not be aborted.\n    \"\"\"\n    json_content = jsonable_encoder({\"provider\": provider, \"config\": config})\n    await self.post(\"providers/instantiate\", json=json_content, timeout=timeout)\n    return\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.list_models","title":"<code>list_models()</code>  <code>async</code>","text":"<p>List all models.</p> Source code in <code>pixano_inference/client.py</code> <pre><code>async def list_models(self) -&gt; list[ModelInfo]:\n    \"\"\"List all models.\"\"\"\n    response = await self.get(\"app/models/\")\n    return [ModelInfo.model_construct(**model) for model in response.json()]\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.post","title":"<code>post(path, **kwargs)</code>  <code>async</code>","text":"<p>Perform a POST request to the pixano inference server.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the request.</p> required <code>kwargs</code> <code>Any</code> <p>The keyword arguments to pass to the request or httpx client.</p> <code>{}</code> Source code in <code>pixano_inference/client.py</code> <pre><code>async def post(self, path: str, **kwargs: Any) -&gt; Response:\n    \"\"\"Perform a POST request to the pixano inference server.\n\n    Args:\n        path: The path of the request.\n        kwargs: The keyword arguments to pass to the request or httpx client.\n    \"\"\"\n    return await self._rest_call(path=path, method=\"POST\", **kwargs)\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.put","title":"<code>put(path, **kwargs)</code>  <code>async</code>","text":"<p>Perform a PUT request to the pixano inference server.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the request.</p> required <code>kwargs</code> <code>Any</code> <p>The keyword arguments to pass to the request or httpx client.</p> <code>{}</code> Source code in <code>pixano_inference/client.py</code> <pre><code>async def put(self, path: str, **kwargs: Any) -&gt; Response:\n    \"\"\"Perform a PUT request to the pixano inference server.\n\n    Args:\n        path: The path of the request.\n        kwargs: The keyword arguments to pass to the request or httpx client.\n    \"\"\"\n    return await self._rest_call(path=path, method=\"PUT\", **kwargs)\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.text_image_conditional_generation","title":"<code>text_image_conditional_generation(request=None, poll_interval=0.1, timeout=60, task_id=None, asynchronous=False)</code>  <code>async</code>","text":"<pre><code>text_image_conditional_generation(request: TextImageConditionalGenerationRequest | None, poll_interval: float, timeout: float, task_id: str, asynchronous: Literal[True]) -&gt; TextImageConditionalGenerationResponse | CeleryTask\n</code></pre><pre><code>text_image_conditional_generation(request: TextImageConditionalGenerationRequest | None, poll_interval: float, timeout: float, task_id: None, asynchronous: Literal[True]) -&gt; CeleryTask\n</code></pre><pre><code>text_image_conditional_generation(request: TextImageConditionalGenerationRequest, poll_interval: float, timeout: float, task_id: str | None, asynchronous: Literal[False]) -&gt; TextImageConditionalGenerationResponse | CeleryTask\n</code></pre><pre><code>text_image_conditional_generation(request: TextImageConditionalGenerationRequest | None, poll_interval: float, timeout: float, task_id: str | None, asynchronous: bool) -&gt; TextImageConditionalGenerationResponse | CeleryTask\n</code></pre> <p>Perform an inference to perform text-image conditional generation.</p> Source code in <code>pixano_inference/client.py</code> <pre><code>async def text_image_conditional_generation(\n    self,\n    request: TextImageConditionalGenerationRequest | None = None,\n    poll_interval: float = 0.1,\n    timeout: float = 60,\n    task_id: str | None = None,\n    asynchronous: bool = False,\n) -&gt; TextImageConditionalGenerationResponse | CeleryTask:\n    \"\"\"Perform an inference to perform text-image conditional generation.\"\"\"\n    return await self.inference(\n        route=\"tasks/multimodal/text-image/conditional_generation/\",\n        request=request,\n        response_type=TextImageConditionalGenerationResponse,\n        poll_interval=poll_interval,\n        timeout=timeout,\n        task_id=task_id,\n        asynchronous=asynchronous,\n    )\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.PixanoInferenceClient.video_mask_generation","title":"<code>video_mask_generation(request=None, poll_interval=0.1, timeout=60, task_id=None, asynchronous=False)</code>  <code>async</code>","text":"<pre><code>video_mask_generation(request: VideoMaskGenerationRequest | None, poll_interval: float, timeout: float, task_id: str, asynchronous: Literal[True]) -&gt; VideoMaskGenerationResponse | CeleryTask\n</code></pre><pre><code>video_mask_generation(request: VideoMaskGenerationRequest | None, poll_interval: float, timeout: float, task_id: None, asynchronous: Literal[True]) -&gt; CeleryTask\n</code></pre><pre><code>video_mask_generation(request: VideoMaskGenerationRequest, poll_interval: float, timeout: float, task_id: str | None, asynchronous: Literal[False]) -&gt; VideoMaskGenerationResponse\n</code></pre><pre><code>video_mask_generation(request: VideoMaskGenerationRequest | None, poll_interval: float, timeout: float, task_id: str | None, asynchronous: bool) -&gt; VideoMaskGenerationResponse | CeleryTask\n</code></pre> <p>Perform an inference to perform video mask generation.</p> Source code in <code>pixano_inference/client.py</code> <pre><code>async def video_mask_generation(\n    self,\n    request: VideoMaskGenerationRequest | None = None,\n    poll_interval: float = 0.1,\n    timeout: float = 60,\n    task_id: str | None = None,\n    asynchronous: bool = False,\n) -&gt; VideoMaskGenerationResponse | CeleryTask:\n    \"\"\"Perform an inference to perform video mask generation.\"\"\"\n    return await self.inference(\n        route=\"tasks/video/mask_generation/\",\n        request=request,\n        response_type=VideoMaskGenerationResponse,\n        poll_interval=poll_interval,\n        timeout=timeout,\n        task_id=task_id,\n        asynchronous=asynchronous,\n    )\n</code></pre>"},{"location":"api_reference/client/#pixano_inference.client.raise_if_error","title":"<code>raise_if_error(response)</code>","text":"<p>Raise an error from a response.</p> Source code in <code>pixano_inference/client.py</code> <pre><code>def raise_if_error(response: Response) -&gt; None:\n    \"\"\"Raise an error from a response.\"\"\"\n    if response.is_success:\n        return\n    error_out = f\"HTTP {response.status_code}: {response.reason_phrase}\"\n    try:\n        json_detail = response.json()\n    except Exception:\n        json_detail = {}\n\n    detail = json_detail.get(\"detail\", None)\n    if detail is not None:\n        error_out += f\" - {detail}\"\n    error = json_detail.get(\"error\", None)\n    if error is not None:\n        error_out += f\" - {error}\"\n    raise HTTPException(response.status_code, detail=error_out)\n</code></pre>"},{"location":"api_reference/main/","title":"main","text":""},{"location":"api_reference/main/#pixano_inference.main","title":"<code>pixano_inference.main</code>","text":"<p>Main application entry point.</p>"},{"location":"api_reference/main/#pixano_inference.main.create_app","title":"<code>create_app()</code>","text":"<p>Create the FastAPI application.</p> Source code in <code>pixano_inference/main.py</code> <pre><code>def create_app() -&gt; FastAPI:\n    \"\"\"Create the FastAPI application.\"\"\"\n    app = FastAPI()\n\n    app.include_router(routers.tasks.router)\n    app.include_router(routers.providers.router)\n    app.include_router(routers.app.router)\n\n    app.include_router(router)\n    return app\n</code></pre>"},{"location":"api_reference/main/#pixano_inference.main.docs_redirect","title":"<code>docs_redirect()</code>  <code>async</code>","text":"<p>Redirect homepage to docs.</p> Source code in <code>pixano_inference/main.py</code> <pre><code>@router.get(\"/\", include_in_schema=False)\nasync def docs_redirect() -&gt; RedirectResponse:\n    \"\"\"Redirect homepage to docs.\"\"\"\n    return RedirectResponse(url=\"/docs\")\n</code></pre>"},{"location":"api_reference/main/#pixano_inference.main.serve","title":"<code>serve(host, port)</code>","text":"<p>Main application entry point.</p> Source code in <code>pixano_inference/main.py</code> <pre><code>@click.command(context_settings={\"auto_envvar_prefix\": \"UVICORN\"})\n@click.option(\n    \"--host\",\n    type=str,\n    default=\"127.0.0.1\",\n    help=\"Pixano Inference app URL host\",\n    show_default=True,\n)\n@click.option(\n    \"--port\",\n    type=int,\n    default=80,\n    help=\"Pixano Inference app URL port\",\n    show_default=True,\n)\ndef serve(host: str, port: int):\n    \"\"\"Main application entry point.\"\"\"\n    config = uvicorn.Config(fast_api_app, host=host, port=port)\n    server = uvicorn.Server(config)\n    server.run()\n</code></pre>"},{"location":"api_reference/settings/","title":"settings","text":""},{"location":"api_reference/settings/#pixano_inference.settings","title":"<code>pixano_inference.settings</code>","text":"<p>Settings for the Pixano Inference API.</p>"},{"location":"api_reference/settings/#pixano_inference.settings.Settings","title":"<code>Settings(**data)</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Application settings.</p> <p>Attributes:</p> Name Type Description <code>app_name</code> <code>str</code> <p>The name of the application.</p> <code>app_version</code> <code>str</code> <p>The version of the application.</p> <code>app_description</code> <code>str</code> <p>A description of the application.</p> <code>num_cpus</code> <code>int</code> <p>The number of CPUs accessible to the application.</p> <code>num_gpus</code> <code>int</code> <p>The number of GPUs available for inference.</p> <code>num_nodes</code> <code>int</code> <p>The number of nodes available for inference.</p> <code>gpus_used</code> <code>list[int]</code> <p>The list of GPUs used by the application.</p> Source code in <code>pixano_inference/settings.py</code> <pre><code>def __init__(self, **data: Any):\n    \"\"\"Initialize the settings.\"\"\"\n    if \"num_cpus\" not in data:\n        data[\"num_cpus\"] = os.cpu_count()\n    if \"num_gpus\" not in data:\n        if is_torch_installed():\n            if torch.cuda.is_available():\n                data[\"num_gpus\"] = torch.cuda.device_count()\n            else:\n                data[\"num_gpus\"] = 0\n        else:\n            data[\"num_gpus\"] = 0\n\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/settings/#pixano_inference.settings.Settings.gpus_available","title":"<code>gpus_available</code>  <code>property</code>","text":"<p>Return the available GPUs.</p>"},{"location":"api_reference/settings/#pixano_inference.settings.Settings.add_model","title":"<code>add_model(model, task)</code>","text":"<p>Add a model.</p> Source code in <code>pixano_inference/settings.py</code> <pre><code>def add_model(self, model: str, task: str) -&gt; int | None:\n    \"\"\"Add a model.\"\"\"\n    if model in self.models:\n        raise ValueError(f\"Model {model} already registered.\")\n    gpu = self.assign_model_gpu(model)\n    self.models_to_task[model] = task\n    self.models.append(model)\n    return gpu\n</code></pre>"},{"location":"api_reference/settings/#pixano_inference.settings.Settings.assign_model_gpu","title":"<code>assign_model_gpu(model)</code>","text":"<p>Assign a model to a gpu.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The model name.</p> required <p>Returns:</p> Type Description <code>int | None</code> <p>The gpu index. If no gpu available, returns None.</p> Source code in <code>pixano_inference/settings.py</code> <pre><code>def assign_model_gpu(self, model: str) -&gt; int | None:\n    \"\"\"Assign a model to a gpu.\n\n    Args:\n        model: The model name.\n\n    Returns:\n        The gpu index. If no gpu available, returns None.\n    \"\"\"\n    gpu = self.reserve_gpu()\n    if gpu is not None:\n        self.gpu_to_model[gpu] = model\n    return gpu\n</code></pre>"},{"location":"api_reference/settings/#pixano_inference.settings.Settings.free_gpu","title":"<code>free_gpu(gpu)</code>","text":"<p>Free a GPU if used.</p> Source code in <code>pixano_inference/settings.py</code> <pre><code>def free_gpu(self, gpu: int) -&gt; None:\n    \"\"\"Free a GPU if used.\"\"\"\n    try:\n        self.gpus_used.remove(gpu)\n    except ValueError:\n        pass\n    self.gpu_to_model.pop(gpu)\n    return\n</code></pre>"},{"location":"api_reference/settings/#pixano_inference.settings.Settings.remove_model","title":"<code>remove_model(model)</code>","text":"<p>Remove a model.</p> Source code in <code>pixano_inference/settings.py</code> <pre><code>def remove_model(self, model: str) -&gt; None:\n    \"\"\"Remove a model.\"\"\"\n    self.models.remove(model)\n    self.models_to_task.pop(model, None)\n    for gpu, model_stored in self.gpu_to_model.items():\n        if model_stored == model:\n            self.free_gpu(gpu)\n            break\n    return\n</code></pre>"},{"location":"api_reference/settings/#pixano_inference.settings.Settings.reserve_gpu","title":"<code>reserve_gpu()</code>","text":"<p>Reserve a gpu if any available.</p> Source code in <code>pixano_inference/settings.py</code> <pre><code>def reserve_gpu(self) -&gt; int | None:\n    \"\"\"Reserve a gpu if any available.\"\"\"\n    gpus = self.gpus_available\n    if len(gpus) == 0:\n        return None\n    selected_gpu = gpus[0]\n    self.gpus_used.append(selected_gpu)\n    return selected_gpu\n</code></pre>"},{"location":"api_reference/settings/#pixano_inference.settings.get_pixano_inference_settings","title":"<code>get_pixano_inference_settings()</code>","text":"<p>Return the settings.</p> Source code in <code>pixano_inference/settings.py</code> <pre><code>def get_pixano_inference_settings() -&gt; Settings:\n    \"\"\"Return the settings.\"\"\"\n    return PIXANO_INFERENCE_SETTINGS\n</code></pre>"},{"location":"api_reference/data/read_vector_databases/","title":"read_vector_databases","text":""},{"location":"api_reference/data/read_vector_databases/#pixano_inference.data.read_vector_databases","title":"<code>pixano_inference.data.read_vector_databases</code>","text":"<p>This module contains functions for reading data from Vector databases.</p>"},{"location":"api_reference/data/read_vector_databases/#pixano_inference.data.read_vector_databases.read_lance_vector","title":"<code>read_lance_vector(path, column, indice=None, where=None, shape=None, return_type='tensor')</code>","text":"<p>Reads a vector from a Lance dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the Lance dataset.</p> required <code>column</code> <code>str</code> <p>The vector column to read.</p> required <code>indice</code> <code>int | None</code> <p>The index of the row to read.</p> <code>None</code> <code>where</code> <code>str | None</code> <p>The filter to apply. If specified, only one row should be returned.</p> <code>None</code> <code>shape</code> <code>list[int] | None</code> <p>The shape of the vector.</p> <code>None</code> <code>return_type</code> <code>Literal['tensor', 'numpy']</code> <p>The type of the return value. Either 'tensor' or 'numpy'.</p> <code>'tensor'</code> <p>Returns:</p> Type Description <code>'Tensor' | ndarray</code> <p>The value of the column.</p> Source code in <code>pixano_inference/data/read_vector_databases.py</code> <pre><code>def read_lance_vector(\n    path: Path | str,\n    column: str,\n    indice: int | None = None,\n    where: str | None = None,\n    shape: list[int] | None = None,\n    return_type: Literal[\"tensor\", \"numpy\"] = \"tensor\",\n) -&gt; \"Tensor\" | np.ndarray:\n    \"\"\"Reads a vector from a Lance dataset.\n\n    Args:\n        path: The path to the Lance dataset.\n        column: The vector column to read.\n        indice: The index of the row to read.\n        where: The filter to apply. If specified, only one row should be returned.\n        shape: The shape of the vector.\n        return_type: The type of the return value. Either 'tensor' or 'numpy'.\n\n    Returns:\n        The value of the column.\n    \"\"\"\n    assert_lance_installed()\n    if return_type not in [\"tensor\", \"numpy\"]:\n        raise ValueError(\"return_type must be either 'tensor' or 'numpy'.\")\n    elif return_type == \"tensor\":\n        assert_torch_installed()\n    if indice is not None and where is not None:\n        raise ValueError(\"Only one of 'index' and 'where' can be specified.\")\n    elif indice is None and where is None:\n        raise ValueError(\"One of 'index' and 'where' must be specified.\")\n    elif indice is not None and not isinstance(indice, int):\n        raise ValueError(\"index must be an integer.\")\n    elif where is not None and not isinstance(where, str):\n        raise ValueError(\"where must be a string.\")\n    elif not isinstance(path, (Path, str)):\n        raise ValueError(\"lance_path must be a Path or a string.\")\n    elif shape is not None and not isinstance(shape, list) and not all(isinstance(i, int) for i in shape):\n        raise ValueError(\"shape must be a list of integers.\")\n\n    if isinstance(path, str):\n        path = Path(path)\n\n    lance_dataset = LanceDataset(path)\n    if indice is not None:\n        pa_table = lance_dataset.take(indices=[indice], columns=[column])\n    else:\n        pa_table = lance_dataset.scanner(columns=[column], filter=where).to_table()\n    polar_df = pl.DataFrame(pa_table)\n    if polar_df.shape[0] != 1:\n        raise ValueError(f\"Expected one row to be returned but found {polar_df.shape[0]} rows.\")\n    vector: np.ndarray = polar_df[column].to_numpy()\n    vector = vector.reshape(shape)\n    if return_type == \"tensor\":\n        return torch.from_numpy(vector)\n    return vector\n</code></pre>"},{"location":"api_reference/models/base/","title":"base","text":""},{"location":"api_reference/models/base/#pixano_inference.models.base","title":"<code>pixano_inference.models.base</code>","text":"<p>Base class for inference models.</p>"},{"location":"api_reference/models/base/#pixano_inference.models.base.BaseInferenceModel","title":"<code>BaseInferenceModel(name, provider)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for inference models.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>provider</code> <code>str</code> <p>Provider of the model.</p> required Source code in <code>pixano_inference/models/base.py</code> <pre><code>def __init__(self, name: str, provider: str):\n    \"\"\"Initialize the model.\n\n    Args:\n        name: Name of the model.\n        provider: Provider of the model.\n    \"\"\"\n    self.name = name\n    self.provider = provider\n    self._status = ModelStatus.IDLE\n</code></pre>"},{"location":"api_reference/models/base/#pixano_inference.models.base.BaseInferenceModel.metadata","title":"<code>metadata</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return the metadata of the model.</p>"},{"location":"api_reference/models/base/#pixano_inference.models.base.BaseInferenceModel.status","title":"<code>status</code>  <code>property</code> <code>writable</code>","text":"<p>Get the status of the model.</p>"},{"location":"api_reference/models/base/#pixano_inference.models.base.BaseInferenceModel.delete","title":"<code>delete()</code>  <code>abstractmethod</code>","text":"<p>Delete the model.</p> Source code in <code>pixano_inference/models/base.py</code> <pre><code>@abstractmethod\ndef delete(self):\n    \"\"\"Delete the model.\"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/base/#pixano_inference.models.base.BaseInferenceModel.image_mask_generation","title":"<code>image_mask_generation(*args, **kwargs)</code>","text":"<p>Generate a mask from the image.</p> Source code in <code>pixano_inference/models/base.py</code> <pre><code>def image_mask_generation(self, *args: Any, **kwargs) -&gt; ImageMaskGenerationOutput:\n    \"\"\"Generate a mask from the image.\"\"\"\n    raise NotImplementedError(\"This model does not support image mask generation.\")\n</code></pre>"},{"location":"api_reference/models/base/#pixano_inference.models.base.BaseInferenceModel.image_zero_shot_detection","title":"<code>image_zero_shot_detection(*args, **kwargs)</code>","text":"<p>Perform zero shot detection on an image.</p> Source code in <code>pixano_inference/models/base.py</code> <pre><code>def image_zero_shot_detection(self, *args: Any, **kwargs) -&gt; ImageZeroShotDetectionOutput:\n    \"\"\"Perform zero shot detection on an image.\"\"\"\n    raise NotImplementedError(\"This model does not support image zero shot detection.\")\n</code></pre>"},{"location":"api_reference/models/base/#pixano_inference.models.base.BaseInferenceModel.text_image_conditional_generation","title":"<code>text_image_conditional_generation(*args, **kwargs)</code>","text":"<p>Generate text from an image and a prompt.</p> Source code in <code>pixano_inference/models/base.py</code> <pre><code>def text_image_conditional_generation(self, *args: Any, **kwargs) -&gt; TextImageConditionalGenerationOutput:\n    \"\"\"Generate text from an image and a prompt.\"\"\"\n    raise NotImplementedError(\"This model does not support text-image conditional generation.\")\n</code></pre>"},{"location":"api_reference/models/base/#pixano_inference.models.base.BaseInferenceModel.video_mask_generation","title":"<code>video_mask_generation(*args, **kwargs)</code>","text":"<p>Generate a mask from the video.</p> Source code in <code>pixano_inference/models/base.py</code> <pre><code>def video_mask_generation(self, *args: Any, **kwargs) -&gt; VideoMaskGenerationOutput:\n    \"\"\"Generate a mask from the video.\"\"\"\n    raise NotImplementedError(\"This model does not support video mask generation.\")\n</code></pre>"},{"location":"api_reference/models/base/#pixano_inference.models.base.ModelStatus","title":"<code>ModelStatus</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Current status of the model.</p> <p>Attributes: - IDLE: waiting for an input. - RUNNING: computing.</p>"},{"location":"api_reference/models/sam2/","title":"sam2","text":""},{"location":"api_reference/models/sam2/#pixano_inference.models.sam2","title":"<code>pixano_inference.models.sam2</code>","text":"<p>Inference model for the SAM2 model.</p>"},{"location":"api_reference/models/sam2/#pixano_inference.models.sam2.Sam2Model","title":"<code>Sam2Model(name, provider, predictor, torch_dtype='bfloat16', config={})</code>","text":"<p>               Bases: <code>BaseInferenceModel</code></p> <p>Inference model for the SAM2 model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>provider</code> <code>str</code> <p>Provider of the model.</p> required <code>predictor</code> <code>Any</code> <p>The SAM2 image predictor.</p> required <code>torch_dtype</code> <code>Literal['float32', 'float16', 'bfloat16']</code> <p>The torch data type to use for inference.</p> <code>'bfloat16'</code> <code>config</code> <code>dict[str, Any]</code> <p>Configuration for the model.</p> <code>{}</code> Source code in <code>pixano_inference/models/sam2.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    provider: str,\n    predictor: Any,\n    torch_dtype: Literal[\"float32\", \"float16\", \"bfloat16\"] = \"bfloat16\",\n    config: dict[str, Any] = {},\n):\n    \"\"\"Initialize the model.\n\n    Args:\n        name: Name of the model.\n        provider: Provider of the model.\n        predictor: The SAM2 image predictor.\n        torch_dtype: The torch data type to use for inference.\n        config: Configuration for the model.\n    \"\"\"\n    assert_sam2_installed()\n\n    super().__init__(name, provider)\n    match torch_dtype:\n        case \"float32\":\n            self.torch_dtype = torch.float32\n        case \"float16\":\n            self.torch_dtype = torch.float16\n        case \"bfloat16\":\n            self.torch_dtype = torch.bfloat16\n        case _:\n            raise ValueError(f\"Invalid torch_dtype: {torch_dtype}\")\n    self.predictor: SAM2ImagePredictor | SAM2VideoPredictor = predictor\n    self.config = config\n</code></pre>"},{"location":"api_reference/models/sam2/#pixano_inference.models.sam2.Sam2Model.metadata","title":"<code>metadata</code>  <code>property</code>","text":"<p>Return the metadata of the model.</p>"},{"location":"api_reference/models/sam2/#pixano_inference.models.sam2.Sam2Model.delete","title":"<code>delete()</code>","text":"<p>Delete the model.</p> Source code in <code>pixano_inference/models/sam2.py</code> <pre><code>def delete(self) -&gt; None:\n    \"\"\"Delete the model.\"\"\"\n    del self.predictor\n    gc.collect()\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"api_reference/models/sam2/#pixano_inference.models.sam2.Sam2Model.image_mask_generation","title":"<code>image_mask_generation(image, points, labels, boxes, multimask_output=True, num_multimask_outputs=3, return_image_embedding=False, **kwargs)</code>","text":"<p>Generate masks from the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray | Image</code> <p>Image for the generation.</p> required <code>points</code> <code>list[list[list[int]]] | None</code> <p>Points for the mask generation. The first dimension is the number of prompts, the second the number of points per mask and the third the coordinates of the points.</p> required <code>labels</code> <code>list[list[int]] | None</code> <p>Labels for the mask generation. The first dimension is the number of prompts, the second the number of labels per mask.</p> required <code>boxes</code> <code>list[list[int]] | None</code> <p>Boxes for the mask generation. The first dimension is the number of prompts, the second the coordinates of the boxes.</p> required <code>multimask_output</code> <code>bool</code> <p>Whether to generate multiple masks per prediction.</p> <code>True</code> <code>num_multimask_outputs</code> <code>int</code> <p>Number of masks to generate per prediction.</p> <code>3</code> <code>return_image_embedding</code> <code>bool</code> <p>Whether to return the image embedding and high-resolution features.</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>pixano_inference/models/sam2.py</code> <pre><code>def image_mask_generation(\n    self,\n    image: np.ndarray | Image,\n    points: list[list[list[int]]] | None,\n    labels: list[list[int]] | None,\n    boxes: list[list[int]] | None,\n    multimask_output: bool = True,\n    num_multimask_outputs: int = 3,\n    return_image_embedding: bool = False,\n    **kwargs: Any,\n) -&gt; ImageMaskGenerationOutput:\n    \"\"\"Generate masks from the image.\n\n    Args:\n        image: Image for the generation.\n        points: Points for the mask generation. The first dimension is the number of prompts, the\n            second the number of points per mask and the third the coordinates of the points.\n        labels: Labels for the mask generation. The first dimension is the number of prompts, the second\n            the number of labels per mask.\n        boxes: Boxes for the mask generation. The first dimension is the number of prompts, the second\n            the coordinates of the boxes.\n        multimask_output: Whether to generate multiple masks per prediction.\n        num_multimask_outputs: Number of masks to generate per prediction.\n        return_image_embedding: Whether to return the image embedding and high-resolution features.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    # Check the input list types\n    with torch.inference_mode():\n        if not isinstance(image, (np.ndarray, Image)):\n            raise ValueError(\"The image should be an numpy array or a PIL image.\")\n        if (\n            points is not None\n            and not isinstance(points, list)\n            and not all(isinstance(point, list) for point in points)\n        ):\n            raise ValueError(\"The points should be a list of lists.\")\n        if (\n            labels is not None\n            and not isinstance(labels, list)\n            and not all(isinstance(label, list) for label in labels)\n        ):\n            raise ValueError(\"The labels should be a list of lists.\")\n        if boxes is not None and not isinstance(boxes, list):\n            if not all(isinstance(box, list) for box in boxes):\n                raise ValueError(\"The boxes should be a list of lists.\")\n\n        if multimask_output and not (num_multimask_outputs == 3):\n            raise ValueError(\"The number of multimask outputs is not configurable for Sam2 and must be 3.\")\n\n        # Check the input batch size\n        if points is None and labels is not None:\n            raise ValueError(\"Labels are not supported without points.\")\n        if points is not None and labels is not None:\n            if len(points) != len(labels):\n                raise ValueError(\"The number of points and labels should match.\")\n        if points is not None and boxes is not None:\n            if len(points) != len(boxes):\n                raise ValueError(\"The number of points and boxes should match.\")\n\n        # Check the input shapes and value types\n        if points is not None:\n            for prompt_point in points:\n                for points_in_mask in prompt_point:\n                    if len(points_in_mask) != 2:\n                        raise ValueError(\"Each point should have 2 coordinates.\")\n                    if not all(isinstance(point, int) for point in points_in_mask):\n                        raise ValueError(\"Each point should be an integer.\")\n        if labels is not None:\n            for i, prompt_label in enumerate(labels):\n                if len(prompt_label) != len(points[i]):  # type: ignore[index]\n                    raise ValueError(\"The number of labels should match the number of points.\")\n\n                if not all(isinstance(label, int) for label in prompt_label):\n                    raise ValueError(\"Each label should be an integer.\")\n        if boxes is not None:\n            for prompt_box in boxes:\n                if len(prompt_box) != 4:\n                    raise ValueError(\"Each box should have 4 coordinates.\")\n                if not all(isinstance(box, int) for box in prompt_box):\n                    raise ValueError(\"Each box should be an integer.\")\n\n        # Convert inputs to numpy arrays\n        if points is not None:\n            np_points = [np.array(prompt_points, dtype=np.int32) for prompt_points in points]\n        if labels is not None:\n            np_labels = [np.array(prompt_labels, dtype=np.int32) for prompt_labels in labels]\n        if boxes is not None:\n            boxes = np.array(boxes, dtype=np.int32)\n\n        if points is not None:\n            # Pad the points and labels to the same length\n            # =============================================================================\n            # From Hugging Face's implementation (Apache-2.0 License):\n            # https://github.com/huggingface/transformers/blob/v4.45.2/src/transformers/models/sam/processing_sam.py#L36\n            expected_nb_points = max([point.shape[0] for point in np_points])\n            processed_points = []\n            for i, point in enumerate(np_points):\n                if point.shape[0] != expected_nb_points:\n                    point = np.concatenate(\n                        [point, np.zeros((expected_nb_points - point.shape[0], 2)) + -10], axis=0\n                    )\n                    if labels is not None:\n                        np_labels[i] = np.append(np_labels[i], [-10])\n                processed_points.append(point)\n            # =============================================================================\n            np_points = np.array(processed_points)\n\n        input_points = np_points if points is not None else None\n        input_labels = np.array(np_labels) if labels is not None else None\n\n        with torch.autocast(self.predictor.device.type, dtype=self.torch_dtype):\n            if not self.predictor._is_image_set:\n                self.predictor.set_image(image)\n\n            masks, scores, _ = self.predictor.predict(\n                point_coords=input_points,\n                point_labels=input_labels,\n                box=boxes,\n                mask_input=None,\n                multimask_output=multimask_output,\n                return_logits=False,\n            )\n\n            if len(masks.shape) == 3:\n                masks = np.expand_dims(masks, 0)\n                scores = np.expand_dims(scores, 0)\n\n            if return_image_embedding:\n                image_embedding: Tensor = self.predictor._features[\"image_embed\"]\n                image_embedding_list = image_embedding.to(torch.float32).flatten().tolist()\n                high_resolution_features: list[Tensor] = self.predictor._features[\"high_res_feats\"]\n                high_resolution_features_list = [\n                    features.to(torch.float32).flatten().tolist() for features in high_resolution_features\n                ]\n                image_embedding_ndarray = NDArrayFloat(\n                    values=image_embedding_list, shape=image_embedding.shape[1:]\n                )\n                high_resolution_features_ndarray = [\n                    NDArrayFloat(values=features_list, shape=features.shape[1:])\n                    for features, features_list in zip(high_resolution_features, high_resolution_features_list)\n                ]\n            else:\n                image_embedding_ndarray = None\n                high_resolution_features_ndarray = None\n\n            masks = ImageMaskGenerationOutput(\n                masks=[\n                    [CompressedRLE.from_mask(mask.astype(np.uint8)) for mask in prediction_masks]\n                    for prediction_masks in masks\n                ],\n                scores=NDArrayFloat.from_numpy(scores),\n                image_embedding=image_embedding_ndarray,\n                high_resolution_features=high_resolution_features_ndarray,\n            )\n            return masks\n</code></pre>"},{"location":"api_reference/models/sam2/#pixano_inference.models.sam2.Sam2Model.init_video_state","title":"<code>init_video_state(video, offload_video_to_cpu=False, offload_state_to_cpu=False)</code>","text":"<p>Initialize an inference state.</p> Source code in <code>pixano_inference/models/sam2.py</code> <pre><code>def init_video_state(\n    self,\n    video: bytes | Path | list[str] | list[Path],\n    offload_video_to_cpu=False,\n    offload_state_to_cpu=False,\n):\n    \"\"\"Initialize an inference state.\"\"\"\n    with torch.inference_mode():\n        #############################################################\n        ## Adapted from Sam2 Repository (Apache License, Version 2.0)\n        #############################################################\n\n        compute_device = self.predictor.device  # device of the model\n\n        if isinstance(video, bytes) or isinstance(video, Path) and video.is_file():\n            images, video_height, video_width = load_video_frames(\n                video_path=video,\n                image_size=self.predictor.image_size,\n                offload_video_to_cpu=offload_video_to_cpu,\n                async_loading_frames=False,\n                compute_device=compute_device,\n            )\n        elif isinstance(video, list) or isinstance(video, Path) and video.is_dir():\n            if isinstance(video, Path):\n                frames: list[str] | list[Path] = sorted([f for f in video.glob(\"**/*.jpg\") if f.is_file()])\n            else:\n                frames = video\n            images, video_height, video_width = self.load_video_frames_from_images(\n                frames=frames,\n                image_size=self.predictor.image_size,\n                offload_video_to_cpu=offload_video_to_cpu,\n                compute_device=compute_device,\n            )\n        else:\n            raise ValueError(\"Unknown video type.\")\n\n        inference_state = {}\n        inference_state[\"images\"] = images\n        inference_state[\"num_frames\"] = len(images)\n        # whether to offload the video frames to CPU memory\n        # turning on this option saves the GPU memory with only a very small overhead\n        inference_state[\"offload_video_to_cpu\"] = offload_video_to_cpu\n        # whether to offload the inference state to CPU memory\n        # turning on this option saves the GPU memory at the cost of a lower tracking fps\n        # (e.g. in a test case of 768x768 model, fps dropped from 27 to 24 when tracking one object\n        # and from 24 to 21 when tracking two objects)\n        inference_state[\"offload_state_to_cpu\"] = offload_state_to_cpu\n        # the original video height and width, used for resizing final output scores\n        inference_state[\"video_height\"] = video_height\n        inference_state[\"video_width\"] = video_width\n        inference_state[\"device\"] = compute_device\n        if offload_state_to_cpu:\n            inference_state[\"storage_device\"] = torch.device(\"cpu\")\n        else:\n            inference_state[\"storage_device\"] = compute_device\n        # inputs on each frame\n        inference_state[\"point_inputs_per_obj\"] = {}\n        inference_state[\"mask_inputs_per_obj\"] = {}\n        # visual features on a small number of recently visited frames for quick interactions\n        inference_state[\"cached_features\"] = {}\n        # values that don't change across frames (so we only need to hold one copy of them)\n        inference_state[\"constants\"] = {}\n        # mapping between client-side object id and model-side object index\n        inference_state[\"obj_id_to_idx\"] = OrderedDict()\n        inference_state[\"obj_idx_to_id\"] = OrderedDict()\n        inference_state[\"obj_ids\"] = []\n        # Slice (view) of each object tracking results, sharing the same memory with \"output_dict\"\n        inference_state[\"output_dict_per_obj\"] = {}\n        # A temporary storage to hold new outputs when user interact with a frame\n        # to add clicks or mask (it's merged into \"output_dict\" before propagation starts)\n        inference_state[\"temp_output_dict_per_obj\"] = {}\n        # Frames that already holds consolidated outputs from click or mask inputs\n        # (we directly use their consolidated outputs during tracking)\n        # metadata for each tracking frame (e.g. which direction it's tracked)\n        inference_state[\"frames_tracked_per_obj\"] = {}\n        # Warm up the visual backbone and cache the image feature on frame 0\n        self.predictor._get_image_feature(inference_state, frame_idx=0, batch_size=1)\n        return inference_state\n</code></pre>"},{"location":"api_reference/models/sam2/#pixano_inference.models.sam2.Sam2Model.load_video_frames_from_images","title":"<code>load_video_frames_from_images(frames, image_size, offload_video_to_cpu, compute_device, images_mean=(0.485, 0.456, 0.406), images_std=(0.229, 0.224, 0.225))</code>","text":"<p>Load the video frames from a directory of JPEG files (\".jpg\" format). <p>The frames are resized to image_size x image_size and are loaded to GPU if <code>offload_video_to_cpu</code> is <code>False</code> and to CPU if <code>offload_video_to_cpu</code> is <code>True</code>.</p> Source code in <code>pixano_inference/models/sam2.py</code> <pre><code>def load_video_frames_from_images(\n    self,\n    frames: list[str] | list[Path],\n    image_size: int,\n    offload_video_to_cpu: bool,\n    compute_device: \"torch.device\",\n    images_mean: tuple[float, float, float] = (0.485, 0.456, 0.406),\n    images_std: tuple[float, float, float] = (0.229, 0.224, 0.225),\n) -&gt; tuple[Tensor, int, int]:\n    \"\"\"Load the video frames from a directory of JPEG files (\"&lt;frame_index&gt;.jpg\" format).\n\n    The frames are resized to image_size x image_size and are loaded to GPU if\n    `offload_video_to_cpu` is `False` and to CPU if `offload_video_to_cpu` is `True`.\n    \"\"\"\n    #############################################################\n    ## Adapted from Sam2 Repository (Apache License, Version 2.0)\n    #############################################################\n    num_frames = len(frames)\n    if num_frames == 0:\n        raise RuntimeError(f\"no images found in {frames}\")\n    device = torch.device(\"cpu\") if offload_video_to_cpu else compute_device\n\n    images_mean = torch.tensor(images_mean, dtype=torch.float32, device=device)[:, None, None]\n    images_std = torch.tensor(images_std, dtype=torch.float32, device=device)[:, None, None]\n\n    torch_images = torch.zeros(num_frames, 3, image_size, image_size, dtype=torch.float32, device=device)\n    for n, image in enumerate(tqdm(frames, desc=\"frame loading\")):\n        pil_image = convert_string_to_image(image)\n        video_height = pil_image.height\n        video_width = pil_image.width\n        torch_images[n] = convert_image_pil_to_tensor(image=pil_image, size=image_size, device=device)\n\n    torch_images -= images_mean\n    torch_images /= images_std\n    return torch_images, video_height, video_width\n</code></pre>"},{"location":"api_reference/models/sam2/#pixano_inference.models.sam2.Sam2Model.set_image_embeddings","title":"<code>set_image_embeddings(image, image_embedding, high_resolution_features)</code>","text":"<p>Calculates the image embeddings for the provided image.</p> <p>Adapted from https://github.com/facebookresearch/sam2/blob/main/sam2/sam2_image_predictor.py (Apache-2.0 License).</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray | 'Tensor' | Image</code> <p>The input image to embed in RGB format. The image should be in HWC format if np.ndarray, or WHC format if PIL Image with or CHW format if torch.Tensor.</p> required <code>image_embedding</code> <code>Tensor</code> <p>The image embedding tensor.</p> required <code>high_resolution_features</code> <code>Tensor</code> <p>The high-resolution features tensor.</p> required Source code in <code>pixano_inference/models/sam2.py</code> <pre><code>def set_image_embeddings(\n    self,\n    image: np.ndarray | \"Tensor\" | Image,\n    image_embedding: Tensor,\n    high_resolution_features: Tensor,\n) -&gt; None:\n    \"\"\"Calculates the image embeddings for the provided image.\n\n    Adapted from https://github.com/facebookresearch/sam2/blob/main/sam2/sam2_image_predictor.py\n    (Apache-2.0 License).\n\n    Args:\n        image: The input image to embed in RGB format. The image should be in HWC format if np.ndarray, or WHC\n            format if PIL Image with or CHW format if torch.Tensor.\n        image_embedding: The image embedding tensor.\n        high_resolution_features: The high-resolution features tensor.\n    \"\"\"\n    with torch.inference_mode():\n        self.predictor.reset_predictor()\n        # Transform the image to the form expected by the model\n        if isinstance(image, np.ndarray):\n            self.predictor._orig_hw = [image.shape[:2]]\n        elif isinstance(image, Tensor):\n            self.predictor._orig_hw = [image.shape[-2:]]\n        elif isinstance(image, Image):\n            w, h = image.size\n            self.predictor._orig_hw = [(h, w)]\n        else:\n            raise NotImplementedError(\"Image format not supported\")\n\n        self.predictor._features = {\n            \"image_embed\": image_embedding.unsqueeze(0).to(device=self.predictor.model.device),\n            \"high_res_feats\": [\n                features.unsqueeze(0).to(device=self.predictor.model.device)\n                for features in high_resolution_features\n            ],\n        }\n        self.predictor._is_image_set = True\n</code></pre>"},{"location":"api_reference/models/sam2/#pixano_inference.models.sam2.Sam2Model.video_mask_generation","title":"<code>video_mask_generation(video, objects_ids, frame_indexes, points=None, labels=None, boxes=None, propagate=False, **kwargs)</code>","text":"<p>Generate masks from the video.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>bytes | Path | list[str] | list[Path]</code> <p>Video data as a video file or a list of frames files.</p> required <code>objects_ids</code> <code>list[int]</code> <p>IDs of the objects to generate masks for.</p> required <code>frame_indexes</code> <code>list[int]</code> <p>Indexes of the frames where the objects are located.</p> required <code>points</code> <code>list[list[list[int]]] | None</code> <p>Points for the mask generation. The first fimension is the number of objects, the second the number of points for each object and the third the coordinates of the points.</p> <code>None</code> <code>labels</code> <code>list[list[int]] | None</code> <p>Labels for the mask generation. The first fimension is the number of objects, the second the number of labels for each object.</p> <code>None</code> <code>boxes</code> <code>list[list[int]] | None</code> <p>Boxes for the mask generation. The first fimension is the number of objects, the second the coordinates of the boxes.</p> <code>None</code> <code>propagate</code> <code>bool</code> <p>Whether to propagate the masks in the video.</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>VideoMaskGenerationOutput</code> <p>Output of the generation.</p> Source code in <code>pixano_inference/models/sam2.py</code> <pre><code>def video_mask_generation(\n    self,\n    video: bytes | Path | list[str] | list[Path],\n    objects_ids: list[int],\n    frame_indexes: list[int],\n    points: list[list[list[int]]] | None = None,\n    labels: list[list[int]] | None = None,\n    boxes: list[list[int]] | None = None,\n    propagate: bool = False,\n    **kwargs: Any,\n) -&gt; VideoMaskGenerationOutput:\n    \"\"\"Generate masks from the video.\n\n    Args:\n        video: Video data as a video file or a list of frames files.\n        objects_ids: IDs of the objects to generate masks for.\n        frame_indexes: Indexes of the frames where the objects are located.\n        points: Points for the mask generation. The first fimension is the number of objects, the\n            second the number of points for each object and the third the coordinates of the points.\n        labels: Labels for the mask generation. The first fimension is the number of objects, the second\n            the number of labels for each object.\n        boxes: Boxes for the mask generation. The first fimension is the number of objects, the second\n            the coordinates of the boxes.\n        propagate: Whether to propagate the masks in the video.\n        kwargs: Additional keyword arguments.\n\n    Returns:\n        Output of the generation.\n    \"\"\"\n    # Check the input list types\n    with torch.inference_mode():\n        if (\n            points is not None\n            and not isinstance(points, list)\n            and not all(isinstance(point, list) for point in points)\n        ):\n            raise ValueError(\"The points should be a list of lists.\")\n        if (\n            labels is not None\n            and not isinstance(labels, list)\n            and not all(isinstance(label, list) for label in labels)\n        ):\n            raise ValueError(\"The labels should be a list of lists.\")\n        if boxes is not None and not isinstance(boxes, list):\n            if not all(isinstance(box, list) for box in boxes):\n                raise ValueError(\"The boxes should be a list of lists.\")\n\n        # Check the input batch size\n        if points is None and labels is not None:\n            raise ValueError(\"Labels are not supported without points.\")\n        if points is not None and labels is not None:\n            if len(points) != len(labels):\n                raise ValueError(\"The number of points and labels should match.\")\n        if points is not None and boxes is not None:\n            if len(points) != len(boxes):\n                raise ValueError(\"The number of points and boxes should match.\")\n\n        # Check the input shapes and value types\n        if points is not None:\n            for prompt_point in points:\n                for points_in_mask in prompt_point:\n                    if len(points_in_mask) != 2:\n                        raise ValueError(\"Each point should have 2 coordinates.\")\n                    if not all(isinstance(point, int) for point in points_in_mask):\n                        raise ValueError(\"Each point should be an integer.\")\n        if labels is not None:\n            for i, prompt_label in enumerate(labels):\n                if len(prompt_label) != len(points[i]):  # type: ignore[index]\n                    raise ValueError(\"The number of labels should match the number of points.\")\n\n                if not all(isinstance(label, int) for label in prompt_label):\n                    raise ValueError(\"Each label should be an integer.\")\n        if boxes is not None:\n            for prompt_box in boxes:\n                if len(prompt_box) != 4:\n                    raise ValueError(\"Each box should have 4 coordinates.\")\n                if not all(isinstance(box, int) for box in prompt_box):\n                    raise ValueError(\"Each box should be an integer.\")\n\n        # Convert inputs to numpy arrays\n        if points is not None:\n            input_points = [np.array(prompt_points, dtype=np.int32) for prompt_points in points]\n        else:\n            input_points = [None for _ in objects_ids]\n        if labels is not None:\n            input_labels = [np.array(prompt_labels, dtype=np.int32) for prompt_labels in labels]\n        else:\n            input_labels = [None for _ in objects_ids]\n        if boxes is not None:\n            input_boxes = [np.array(box, dtype=np.int32) for box in boxes]\n        else:\n            input_boxes = [None for _ in objects_ids]\n\n        video_segments: dict[int, dict[int, np.ndarray]] = {}\n        with torch.autocast(self.predictor.device.type, dtype=self.torch_dtype):\n            inference_state = self.init_video_state(\n                video=video,\n                offload_state_to_cpu=False,\n                offload_video_to_cpu=False,\n            )\n            for object_id, object_frame_index, object_points, object_labels, object_boxes in zip(\n                objects_ids, frame_indexes, input_points, input_labels, input_boxes\n            ):\n                _, out_obj_ids, out_mask_logits = self.predictor.add_new_points_or_box(\n                    inference_state=inference_state,\n                    frame_idx=object_frame_index,\n                    obj_id=object_id,\n                    points=object_points,\n                    labels=object_labels,\n                    box=object_boxes,\n                )\n\n                if not propagate:\n                    video_segments[object_frame_index] = {\n                        out_obj_id: (out_mask_logits[i] &gt; 0.0).cpu().numpy()\n                        for i, out_obj_id in enumerate(out_obj_ids)\n                    }\n\n            if propagate:\n                video_segments = {}  # video_segments contains the per-frame segmentation results\n                for out_frame_idx, out_obj_ids, out_mask_logits in self.predictor.propagate_in_video(\n                    inference_state\n                ):\n                    video_segments[out_frame_idx] = {\n                        out_obj_id: (out_mask_logits[i] &gt; 0.0).cpu().numpy()\n                        for i, out_obj_id in enumerate(out_obj_ids)\n                    }\n\n        out_objects_ids = []\n        out_frame_indexes = []\n        out_masks = []\n\n        for frame_index, object_masks in video_segments.items():\n            for object_id, mask in object_masks.items():\n                out_objects_ids.append(object_id)\n                out_frame_indexes.append(frame_index)\n                out_masks.append(mask)\n\n        return VideoMaskGenerationOutput(\n            objects_ids=out_objects_ids,\n            frame_indexes=out_frame_indexes,\n            masks=[CompressedRLE.from_mask(mask[0].astype(np.uint8)) for mask in out_masks],\n        )\n</code></pre>"},{"location":"api_reference/models/transformers/","title":"transformers","text":""},{"location":"api_reference/models/transformers/#pixano_inference.models.transformers","title":"<code>pixano_inference.models.transformers</code>","text":"<p>Inference models for Transformers.</p>"},{"location":"api_reference/models/transformers/#pixano_inference.models.transformers.TransformerModel","title":"<code>TransformerModel(name, path, processor, model)</code>","text":"<p>               Bases: <code>BaseInferenceModel</code></p> <p>Inference model for transformers.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>path</code> <code>Path | str</code> <p>Path to the model or its Hugging Face hub's identifier.</p> required <code>processor</code> <code>'ProcessorMixin'</code> <p>Processor for the model.</p> required <code>model</code> <code>'PreTrainedModel'</code> <p>Model for the inference.</p> required Source code in <code>pixano_inference/models/transformers.py</code> <pre><code>def __init__(self, name: str, path: Path | str, processor: \"ProcessorMixin\", model: \"PreTrainedModel\"):\n    \"\"\"Initialize the model.\n\n    Args:\n        name: Name of the model.\n        path: Path to the model or its Hugging Face hub's identifier.\n        processor: Processor for the model.\n        model: Model for the inference.\n    \"\"\"\n    assert_transformers_installed()\n\n    super().__init__(name, provider=\"transformers\")\n    self.processor = processor\n    self.path = path\n    self.model = model.eval()\n</code></pre>"},{"location":"api_reference/models/transformers/#pixano_inference.models.transformers.TransformerModel.metadata","title":"<code>metadata</code>  <code>property</code>","text":"<p>Return the metadata of the model.</p>"},{"location":"api_reference/models/transformers/#pixano_inference.models.transformers.TransformerModel.delete","title":"<code>delete()</code>","text":"<p>Delete the model.</p> Source code in <code>pixano_inference/models/transformers.py</code> <pre><code>def delete(self):\n    \"\"\"Delete the model.\"\"\"\n    del self.model\n    del self.processor\n    gc.collect()\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"api_reference/models/transformers/#pixano_inference.models.transformers.TransformerModel.image_mask_generation","title":"<code>image_mask_generation(image, image_embedding=None, points=None, labels=None, boxes=None, num_multimask_outputs=3, multimask_output=True, return_image_embedding=False, **kwargs)</code>","text":"<p>Generate a mask from the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>'Tensor' | Image</code> <p>Image for the generation.</p> required <code>image_embedding</code> <code>'Tensor' | None</code> <p>Image embeddings for the generation.</p> <code>None</code> <code>points</code> <code>list[list[list[int]]] | None</code> <p>Points for the mask generation. The first fimension is the number of prompts, the second the number of points per mask and the third the coordinates of the points.</p> <code>None</code> <code>labels</code> <code>list[list[int]] | None</code> <p>Labels for the mask generation. The first fimension is the number of prompts, the second the number of labels per mask.</p> <code>None</code> <code>boxes</code> <code>list[list[int]] | None</code> <p>Boxes for the mask generation. The first fimension is the number of prompts, the second the coordinates of the boxes.</p> <code>None</code> <code>num_multimask_outputs</code> <code>int</code> <p>Number of masks to generate per prediction.</p> <code>3</code> <code>multimask_output</code> <code>bool</code> <p>Whether to generate multiple masks per prediction.</p> <code>True</code> <code>return_image_embedding</code> <code>bool</code> <p>Whether to return the image embedding.</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>pixano_inference/models/transformers.py</code> <pre><code>def image_mask_generation(\n    self,\n    image: \"Tensor\" | Image,\n    image_embedding: \"Tensor\" | None = None,\n    points: list[list[list[int]]] | None = None,\n    labels: list[list[int]] | None = None,\n    boxes: list[list[int]] | None = None,\n    num_multimask_outputs: int = 3,\n    multimask_output: bool = True,\n    return_image_embedding: bool = False,\n    **kwargs: Any,\n) -&gt; ImageMaskGenerationOutput:\n    \"\"\"Generate a mask from the image.\n\n    Args:\n        image: Image for the generation.\n        image_embedding: Image embeddings for the generation.\n        points: Points for the mask generation. The first fimension is the number of prompts, the\n            second the number of points per mask and the third the coordinates of the points.\n        labels: Labels for the mask generation. The first fimension is the number of prompts, the second\n            the number of labels per mask.\n        boxes: Boxes for the mask generation. The first fimension is the number of prompts, the second\n            the coordinates of the boxes.\n        num_multimask_outputs: Number of masks to generate per prediction.\n        multimask_output: Whether to generate multiple masks per prediction.\n        return_image_embedding: Whether to return the image embedding.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    with torch.inference_mode():\n        inputs = self.processor(\n            image,\n            input_points=[points] if points is not None else None,\n            input_boxes=[boxes] if boxes is not None else None,\n            input_labels=[labels] if labels is not None else None,\n            return_tensors=\"pt\",\n        ).to(self.model.device, dtype=self.model.dtype)\n\n        if return_image_embedding:\n            if image_embedding is None:  # Compute image embeddings if not provided\n                image_embedding = self.model.get_image_embeddings(inputs[\"pixel_values\"])\n\n        if image_embedding is not None:\n            if image_embedding.ndim == 3:\n                image_embedding = image_embedding.unsqueeze(0)\n            inputs.pop(\"pixel_values\", None)\n            inputs.update({\"image_embeddings\": image_embedding.to(self.model.device, dtype=self.model.dtype)})\n\n        outputs = self.model(\n            **inputs, num_multimask_outputs=num_multimask_outputs, multimask_output=multimask_output, **kwargs\n        )\n\n        masks = (\n            self.processor.image_processor.post_process_masks(\n                outputs.pred_masks, inputs[\"original_sizes\"], inputs[\"reshaped_input_sizes\"]\n            )\n        )[0].cpu()\n        return ImageMaskGenerationOutput(\n            masks=[\n                [CompressedRLE(**encode_mask_to_rle(mask)) for mask in prediction_masks]\n                for prediction_masks in masks\n            ],\n            scores=NDArrayFloat.from_torch(outputs.iou_scores[0].cpu()),\n            image_embedding=(\n                NDArrayFloat.from_torch(image_embedding[0].cpu()) if return_image_embedding else None\n            ),\n        )\n</code></pre>"},{"location":"api_reference/models/transformers/#pixano_inference.models.transformers.TransformerModel.image_zero_shot_detection","title":"<code>image_zero_shot_detection(image, classes, box_threshold, text_threshold, **kwargs)</code>","text":"<p>Perform zero shot detection on an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>'Tensor' | Image</code> <p>The image.</p> required <code>classes</code> <code>str</code> <p>The list of classes to detect in the format 'class1. class2'.</p> required <code>box_threshold</code> <code>float</code> <p>The threshold for bounding boxes detection.</p> required <code>text_threshold</code> <code>float</code> <p>The threshold for the classes identification during zero shot learning phase.</p> required <code>kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ImageZeroShotDetectionOutput</code> <p>The output of image zero-shot detection task.</p> Source code in <code>pixano_inference/models/transformers.py</code> <pre><code>def image_zero_shot_detection(\n    self,\n    image: \"Tensor\" | Image,\n    classes: str,\n    box_threshold: float,\n    text_threshold: float,\n    **kwargs: Any,\n) -&gt; ImageZeroShotDetectionOutput:\n    \"\"\"Perform zero shot detection on an image.\n\n    Args:\n        image: The image.\n        classes: The list of classes to detect in the format 'class1. class2'.\n        box_threshold: The threshold for bounding boxes detection.\n        text_threshold: The threshold for the classes identification during zero shot learning phase.\n        kwargs: Additional arguments.\n\n    Returns:\n        The output of image zero-shot detection task.\n    \"\"\"\n    with torch.inference_mode():\n        inputs = self.processor(images=image, text=classes, return_tensors=\"pt\").to(self.model.device)\n\n        outputs = self.model(**inputs)\n\n        target_size = (\n            (image.shape[-2], image.shape[-1]) if isinstance(image, torch.Tensor) else (image.height, image.width)\n        )\n\n        result = self.processor.post_process_grounded_object_detection(\n            outputs,\n            inputs.input_ids,\n            box_threshold=box_threshold,\n            text_threshold=text_threshold,\n            target_sizes=[target_size],\n        )[0]\n\n        return ImageZeroShotDetectionOutput(\n            boxes=[[int(round(x, 0)) for x in box.tolist()] for box in result[\"boxes\"]],\n            scores=result[\"scores\"],\n            classes=result[\"labels\"],\n        )\n</code></pre>"},{"location":"api_reference/models/transformers/#pixano_inference.models.transformers.TransformerModel.text_image_conditional_generation","title":"<code>text_image_conditional_generation(prompt, images, generation_config=None, **kwargs)</code>","text":"<p>Generate text from an image and a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str | list[dict[str, Any]]</code> <p>Prompt for the generation.</p> required <code>images</code> <code>list['Tensor']</code> <p>Images for the generation.</p> required <code>generation_config</code> <code>'GenerationConfig' | None</code> <p>Configuration for the generation as Hugging Face's GenerationConfig.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>pixano_inference/models/transformers.py</code> <pre><code>def text_image_conditional_generation(\n    self,\n    prompt: str | list[dict[str, Any]],\n    images: list[\"Tensor\"],\n    generation_config: \"GenerationConfig\" | None = None,\n    **kwargs: Any,\n) -&gt; TextImageConditionalGenerationOutput:\n    \"\"\"Generate text from an image and a prompt.\n\n    Args:\n        prompt: Prompt for the generation.\n        images: Images for the generation.\n        generation_config: Configuration for the generation as Hugging Face's GenerationConfig.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    with torch.inference_mode():\n        if generation_config is None:\n            generation_config = GenerationConfig()\n\n        generation_config = self._fill_generation_config(generation_config, **kwargs)\n\n        if isinstance(prompt, list):\n            prompt = self.processor.apply_chat_template(prompt, add_generation_prompt=True)\n\n        inputs = self.processor(prompt, images, return_tensors=\"pt\").to(self.model.device)\n        generate_ids = self.model.generate(**inputs, generation_config=generation_config)\n\n        total_tokens: int = generate_ids.shape[1]\n        prompt_tokens: int = inputs[\"input_ids\"].shape[1]\n        completion_tokens: int = total_tokens - prompt_tokens\n\n        output = self.processor.decode(\n            generate_ids[0, prompt_tokens:], skip_special_tokens=True, clean_up_tokenization_spaces=False\n        )\n\n        return TextImageConditionalGenerationOutput(\n            generated_text=output,\n            usage=UsageConditionalGeneration(\n                prompt_tokens=prompt_tokens,\n                completion_tokens=completion_tokens,\n                total_tokens=total_tokens,\n            ),\n            generation_config=generation_config.to_diff_dict(),\n        )\n</code></pre>"},{"location":"api_reference/models/vllm/","title":"vllm","text":""},{"location":"api_reference/models/vllm/#pixano_inference.models.vllm","title":"<code>pixano_inference.models.vllm</code>","text":"<p>Inference models for vLLM.</p>"},{"location":"api_reference/models/vllm/#pixano_inference.models.vllm.VLLMModel","title":"<code>VLLMModel(name, vllm_model, model_config, processor_config, device=None)</code>","text":"<p>               Bases: <code>BaseInferenceModel</code></p> <p>Inference model for vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>vllm_model</code> <code>str</code> <p>The model Hugging Face hub's identifier.</p> required <code>model_config</code> <code>dict[str, Any]</code> <p>Configuration for the model.</p> required <code>processor_config</code> <code>dict[str, Any]</code> <p>Configuration for the processor of the model.</p> required <code>device</code> <code>device | str | None</code> <p>The device to use for inference.</p> <code>None</code> Source code in <code>pixano_inference/models/vllm.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    vllm_model: str,\n    model_config: dict[str, Any],\n    processor_config: dict[str, Any],\n    device: torch.device | str | None = None,\n):\n    \"\"\"Initialize the model.\n\n    Args:\n        name: Name of the model.\n        vllm_model: The model Hugging Face hub's identifier.\n        model_config: Configuration for the model.\n        processor_config: Configuration for the processor of the model.\n        device: The device to use for inference.\n    \"\"\"\n    assert_vllm_installed()\n\n    super().__init__(name, provider=\"vllm\")\n    self.vllm_model = vllm_model\n    self.model_config = model_config\n    self.processor_config = processor_config\n    self.model = LLM(model=vllm_model, **model_config, **processor_config, device=device)\n</code></pre>"},{"location":"api_reference/models/vllm/#pixano_inference.models.vllm.VLLMModel.metadata","title":"<code>metadata</code>  <code>property</code>","text":"<p>Return the metadata of the model.</p>"},{"location":"api_reference/models/vllm/#pixano_inference.models.vllm.VLLMModel.delete","title":"<code>delete()</code>","text":"<p>Delete the model.</p> Source code in <code>pixano_inference/models/vllm.py</code> <pre><code>def delete(self):\n    \"\"\"Delete the model.\"\"\"\n    del self.model\n    gc.collect()\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"api_reference/models/vllm/#pixano_inference.models.vllm.VLLMModel.text_image_conditional_generation","title":"<code>text_image_conditional_generation(prompt, temperature=1.0, max_new_tokens=16, **kwargs)</code>","text":"<p>Generate text from an image and a prompt from the vLLM's <code>LLM.chat</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>list[dict[str, Any]]</code> <p>Prompt for the generation.</p> required <code>temperature</code> <code>float</code> <p>Temperature for the generation.</p> <code>1.0</code> <code>max_new_tokens</code> <code>int</code> <p>Maximum number of tokens to generate.</p> <code>16</code> <code>kwargs</code> <code>Any</code> <p>Additional generation arguments.</p> <code>{}</code> Source code in <code>pixano_inference/models/vllm.py</code> <pre><code>def text_image_conditional_generation(\n    self,\n    prompt: list[dict[str, Any]],\n    temperature: float = 1.0,\n    max_new_tokens: int = 16,\n    **kwargs: Any,\n) -&gt; TextImageConditionalGenerationOutput:\n    \"\"\"Generate text from an image and a prompt from the vLLM's `LLM.chat` method.\n\n    Args:\n        prompt: Prompt for the generation.\n        temperature: Temperature for the generation.\n        max_new_tokens: Maximum number of tokens to generate.\n        kwargs: Additional generation arguments.\n    \"\"\"\n    sampling_params = SamplingParams(temperature=temperature, max_tokens=max_new_tokens, **kwargs)\n    with torch.inference_mode():\n        request_output: RequestOutput = self.model.chat(\n            messages=prompt, use_tqdm=False, sampling_params=sampling_params\n        )[0]\n        prompt = request_output.prompt\n        output: CompletionOutput = request_output.outputs[0]\n\n        prompt_tokens = len(request_output.prompt_token_ids)\n        completion_tokens = len(output.token_ids)\n        total_tokens = prompt_tokens + completion_tokens\n\n        return TextImageConditionalGenerationOutput(\n            generated_text=output.text,\n            usage=UsageConditionalGeneration(\n                prompt_tokens=prompt_tokens,\n                completion_tokens=completion_tokens,\n                total_tokens=total_tokens,\n            ),\n            generation_config=msgspec.to_builtins(sampling_params),\n        )\n</code></pre>"},{"location":"api_reference/providers/base/","title":"base","text":""},{"location":"api_reference/providers/base/#pixano_inference.providers.base","title":"<code>pixano_inference.providers.base</code>","text":"<p>Base classes for providers.</p>"},{"location":"api_reference/providers/base/#pixano_inference.providers.base.APIProvider","title":"<code>APIProvider(**kwargs)</code>","text":"<p>               Bases: <code>BaseProvider</code></p> <p>Base class for API providers.</p> Source code in <code>pixano_inference/providers/base.py</code> <pre><code>def __init__(self, **kwargs: Any):\n    \"\"\"Initialize the provider.\"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/providers/base/#pixano_inference.providers.base.APIProvider.api_url","title":"<code>api_url</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>URL of the API.</p>"},{"location":"api_reference/providers/base/#pixano_inference.providers.base.APIProvider.send_request","title":"<code>send_request(request)</code>  <code>abstractmethod</code>","text":"<p>Send a request to the API and return the response.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>APIRequest</code> <p>Request to send.</p> required <p>Returns:</p> Type Description <code>BaseResponse</code> <p>Response from the API.</p> Source code in <code>pixano_inference/providers/base.py</code> <pre><code>@abstractmethod\ndef send_request(self, request: APIRequest) -&gt; BaseResponse:\n    \"\"\"Send a request to the API and return the response.\n\n    Args:\n        request: Request to send.\n\n    Returns:\n        Response from the API.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/providers/base/#pixano_inference.providers.base.BaseProvider","title":"<code>BaseProvider(**kwargs)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for providers.</p> Source code in <code>pixano_inference/providers/base.py</code> <pre><code>def __init__(self, **kwargs: Any):\n    \"\"\"Initialize the provider.\"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/providers/base/#pixano_inference.providers.base.BaseProvider.image_mask_generation","title":"<code>image_mask_generation(request, model, *args, **kwargs)</code>","text":"<p>Generate a mask from the image.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ImageMaskGenerationRequest</code> <p>Request for the generation.</p> required <code>model</code> <code>BaseInferenceModel</code> <p>Model to use for the generation.</p> required <code>args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>pixano_inference/providers/base.py</code> <pre><code>def image_mask_generation(\n    self, request: ImageMaskGenerationRequest, model: BaseInferenceModel, *args: Any, **kwargs: Any\n) -&gt; ImageMaskGenerationOutput:\n    \"\"\"Generate a mask from the image.\n\n    Args:\n        request: Request for the generation.\n        model: Model to use for the generation.\n        args: Additional arguments.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    raise NotImplementedError(\"This provider does not support image mask generation.\")\n</code></pre>"},{"location":"api_reference/providers/base/#pixano_inference.providers.base.BaseProvider.image_zero_shot_detection","title":"<code>image_zero_shot_detection(request, model, *args, **kwargs)</code>","text":"<p>Perform zero-shot image detection.</p> Source code in <code>pixano_inference/providers/base.py</code> <pre><code>def image_zero_shot_detection(\n    self, request: ImageZeroShotDetectionRequest, model: BaseInferenceModel, *args: Any, **kwargs: Any\n) -&gt; ImageZeroShotDetectionOutput:\n    \"\"\"Perform zero-shot image detection.\"\"\"\n    raise NotImplementedError(\"This provider does not support image zero-shot detection.\")\n</code></pre>"},{"location":"api_reference/providers/base/#pixano_inference.providers.base.BaseProvider.text_image_conditional_generation","title":"<code>text_image_conditional_generation(request, model, *args, **kwargs)</code>","text":"<p>Generate an image from the text and image.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>TextImageConditionalGenerationRequest</code> <p>Request for the generation.</p> required <code>model</code> <code>BaseInferenceModel</code> <p>Model to use for the generation.</p> required <code>args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextImageConditionalGenerationOutput</code> <p>Output of the generation</p> Source code in <code>pixano_inference/providers/base.py</code> <pre><code>def text_image_conditional_generation(\n    self, request: TextImageConditionalGenerationRequest, model: BaseInferenceModel, *args: Any, **kwargs: Any\n) -&gt; TextImageConditionalGenerationOutput:\n    \"\"\"Generate an image from the text and image.\n\n    Args:\n        request: Request for the generation.\n        model: Model to use for the generation.\n        args: Additional arguments.\n        kwargs: Additional keyword arguments.\n\n    Returns:\n        Output of the generation\n    \"\"\"\n    raise NotImplementedError(\"This provider does not support text-image conditional generation.\")\n</code></pre>"},{"location":"api_reference/providers/base/#pixano_inference.providers.base.BaseProvider.video_mask_generation","title":"<code>video_mask_generation(request, model, *args, **kwargs)</code>","text":"<p>Generate a mask from the video.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>VideoMaskGenerationRequest</code> <p>Request for the generation.</p> required <code>model</code> <code>BaseInferenceModel</code> <p>Model to use for the generation.</p> required <code>args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>VideoMaskGenerationOutput</code> <p>Output of the generation</p> Source code in <code>pixano_inference/providers/base.py</code> <pre><code>def video_mask_generation(\n    self, request: VideoMaskGenerationRequest, model: BaseInferenceModel, *args: Any, **kwargs: Any\n) -&gt; VideoMaskGenerationOutput:\n    \"\"\"Generate a mask from the video.\n\n    Args:\n        request: Request for the generation.\n        model: Model to use for the generation.\n        args: Additional arguments.\n        kwargs: Additional keyword arguments.\n\n    Returns:\n        Output of the generation\n    \"\"\"\n    raise NotImplementedError(\"This provider does not support video mask generation.\")\n</code></pre>"},{"location":"api_reference/providers/base/#pixano_inference.providers.base.ModelProvider","title":"<code>ModelProvider(**kwargs)</code>","text":"<p>               Bases: <code>BaseProvider</code></p> <p>Base class for model providers.</p> Source code in <code>pixano_inference/providers/base.py</code> <pre><code>def __init__(self, **kwargs: Any):\n    \"\"\"Initialize the provider.\"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/providers/base/#pixano_inference.providers.base.ModelProvider.load_model","title":"<code>load_model(name, task, device, path=None, processor_config={}, config={})</code>  <code>abstractmethod</code>","text":"<p>Load the model from the provider.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>task</code> <code>Task | str</code> <p>Task of the model.</p> required <code>device</code> <code>device</code> <p>Device to use for the model.</p> required <code>path</code> <code>Path | str | None</code> <p>Path to the model.</p> <code>None</code> <code>processor_config</code> <code>dict</code> <p>Processor configuration.</p> <code>{}</code> <code>config</code> <code>dict</code> <p>Configuration for the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseInferenceModel</code> <p>The loaded model.</p> Source code in <code>pixano_inference/providers/base.py</code> <pre><code>@abstractmethod\ndef load_model(\n    self,\n    name: str,\n    task: Task | str,\n    device: \"torch.device\",\n    path: Path | str | None = None,\n    processor_config: dict = {},\n    config: dict = {},\n) -&gt; \"BaseInferenceModel\":\n    \"\"\"Load the model from the provider.\n\n    Args:\n        name: Name of the model.\n        task: Task of the model.\n        device: Device to use for the model.\n        path: Path to the model.\n        processor_config: Processor configuration.\n        config: Configuration for the model.\n\n    Returns:\n        The loaded model.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/providers/registry/","title":"registry","text":""},{"location":"api_reference/providers/registry/#pixano_inference.providers.registry","title":"<code>pixano_inference.providers.registry</code>","text":"<p>Registry for providers.</p>"},{"location":"api_reference/providers/registry/#pixano_inference.providers.registry.register_provider","title":"<code>register_provider(provider)</code>","text":"<p>Return a decorator to register a provider in the registry.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Name of the provider.</p> required <p>Returns:</p> Type Description <p>The decorator.</p> Source code in <code>pixano_inference/providers/registry.py</code> <pre><code>def register_provider(provider: str):\n    \"\"\"Return a decorator to register a provider in the registry.\n\n    Args:\n        provider: Name of the provider.\n\n    Returns:\n        The decorator.\n    \"\"\"\n\n    def decorator(cls):\n        \"\"\"Register the provider in the registry.\n\n        Args:\n            cls: Class to register.\n\n        Returns:\n            The class.\n        \"\"\"\n        if provider in PROVIDERS_REGISTRY:\n            raise ValueError(f\"Provider {provider} already registered.\")\n        PROVIDERS_REGISTRY[provider] = cls\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api_reference/providers/sam2/","title":"sam2","text":""},{"location":"api_reference/providers/sam2/#pixano_inference.providers.sam2","title":"<code>pixano_inference.providers.sam2</code>","text":"<p>Provider for the SAM2 model.</p>"},{"location":"api_reference/providers/sam2/#pixano_inference.providers.sam2.Sam2Provider","title":"<code>Sam2Provider(**kwargs)</code>","text":"<p>               Bases: <code>ModelProvider</code></p> <p>Provider for the SAM2 model.</p> Source code in <code>pixano_inference/providers/sam2.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the SAM2 provider.\"\"\"\n    assert_sam2_installed()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api_reference/providers/sam2/#pixano_inference.providers.sam2.Sam2Provider.image_mask_generation","title":"<code>image_mask_generation(request, model, *args, **kwargs)</code>","text":"<p>Generate a mask from the image.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ImageMaskGenerationRequest</code> <p>Request for the generation.</p> required <code>model</code> <code>Sam2Model</code> <p>Model to use for the generation.</p> required <code>args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ImageMaskGenerationOutput</code> <p>Output of the generation</p> Source code in <code>pixano_inference/providers/sam2.py</code> <pre><code>def image_mask_generation(\n    self,\n    request: ImageMaskGenerationRequest,\n    model: Sam2Model,  # type: ignore[override]\n    *args: Any,\n    **kwargs: Any,\n) -&gt; ImageMaskGenerationOutput:\n    \"\"\"Generate a mask from the image.\n\n    Args:\n        request: Request for the generation.\n        model: Model to use for the generation.\n        args: Additional arguments.\n        kwargs: Additional keyword arguments.\n\n    Returns:\n        Output of the generation\n    \"\"\"\n    request_input = request.to_input()\n    image = convert_string_to_image(request_input.image)\n\n    if request_input.image_embedding is not None and request_input.high_resolution_features is not None:\n        image_embedding = vector_to_tensor(request_input.image_embedding)\n        high_resolution_features = [vector_to_tensor(v) for v in request_input.high_resolution_features]\n        model.set_image_embeddings(image, image_embedding, high_resolution_features)\n    elif request_input.image_embedding is not None or request_input.high_resolution_features is not None:\n        raise ValueError(\"Both image_embedding and high_resolution_features must be provided.\")\n\n    model_input = request_input.model_dump(exclude=[\"image\", \"image_embedding\", \"high_resolution_features\"])\n    model_input[\"image\"] = image\n    output = model.image_mask_generation(**model_input)\n    model.predictor.reset_predictor()\n    return output\n</code></pre>"},{"location":"api_reference/providers/sam2/#pixano_inference.providers.sam2.Sam2Provider.load_model","title":"<code>load_model(name, task, device, path=None, processor_config={}, config={})</code>","text":"<p>Load the model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>task</code> <code>Task | str</code> <p>Task of the model.</p> required <code>device</code> <code>device</code> <p>Device to use for the model.</p> required <code>path</code> <code>Path | str | None</code> <p>Path to the model.</p> <code>None</code> <code>processor_config</code> <code>dict</code> <p>Processor configuration.</p> <code>{}</code> <code>config</code> <code>dict</code> <p>Configuration for the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Sam2Model</code> <p>The loaded model.</p> Source code in <code>pixano_inference/providers/sam2.py</code> <pre><code>def load_model(\n    self,\n    name: str,\n    task: Task | str,\n    device: \"torch.device\",\n    path: Path | str | None = None,\n    processor_config: dict = {},\n    config: dict = {},\n) -&gt; Sam2Model:\n    \"\"\"Load the model.\n\n    Args:\n        name: Name of the model.\n        task: Task of the model.\n        device: Device to use for the model.\n        path: Path to the model.\n        processor_config: Processor configuration.\n        config: Configuration for the model.\n\n    Returns:\n        The loaded model.\n    \"\"\"\n    task = str_to_task(task) if isinstance(task, str) else task\n    if task == ImageTask.MASK_GENERATION:\n        if path is not None and Path(path).exists():\n            model = build_sam2(ckpt_path=path, mode=\"eval\", device=device, **config)\n        else:\n            model = build_sam2_hf(model_id=path, mode=\"eval\", device=device, **config)\n        model = torch.compile(model)\n        predictor = SAM2ImagePredictor(model)\n    elif task == VideoTask.MASK_GENERATION:\n        if path is not None and Path(path).exists():\n            predictor = build_sam2_video_predictor(\n                ckpt_path=path, mode=\"eval\", device=device, vos_optimized=True, **config\n            )\n        else:\n            predictor = build_sam2_video_predictor_hf(\n                model_id=path, mode=\"eval\", device=device, vos_optimized=True, **config\n            )\n        predictor = torch.compile(predictor)\n    else:\n        raise ValueError(f\"Invalid task '{task}' for the SAM2 provider.\")\n\n    our_model = Sam2Model(\n        name=name,\n        provider=\"sam2\",\n        predictor=predictor,\n        torch_dtype=config.get(\"torch_dtype\", \"bfloat16\"),\n        config=config,\n    )\n\n    return our_model\n</code></pre>"},{"location":"api_reference/providers/sam2/#pixano_inference.providers.sam2.Sam2Provider.video_mask_generation","title":"<code>video_mask_generation(request, model, *args, **kwargs)</code>","text":"<p>Generate masks from the video.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>VideoMaskGenerationRequest</code> <p>Request for the generation.</p> required <code>model</code> <code>Sam2Model</code> <p>Model to use for the generation.</p> required <code>args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>VideoMaskGenerationResponse</code> <p>Response of the generation.</p> Source code in <code>pixano_inference/providers/sam2.py</code> <pre><code>def video_mask_generation(\n    self,\n    request: VideoMaskGenerationRequest,\n    model: Sam2Model,  # type: ignore[override]\n    *args: Any,\n    **kwargs: Any,\n) -&gt; VideoMaskGenerationResponse:\n    \"\"\"Generate masks from the video.\n\n    Args:\n        request: Request for the generation.\n        model: Model to use for the generation.\n        args: Additional arguments.\n        kwargs: Additional keyword arguments.\n\n    Returns:\n        Response of the generation.\n    \"\"\"\n    request_input = request.to_input().model_dump()\n    request_input[\"video\"] = convert_string_video_to_bytes_or_path(request_input[\"video\"])\n    output = model.video_mask_generation(**request_input)\n    return output\n</code></pre>"},{"location":"api_reference/providers/transformers/","title":"transformers","text":""},{"location":"api_reference/providers/transformers/#pixano_inference.providers.transformers","title":"<code>pixano_inference.providers.transformers</code>","text":"<p>Provider for Hugging Face Transformers models.</p>"},{"location":"api_reference/providers/transformers/#pixano_inference.providers.transformers.TransformersProvider","title":"<code>TransformersProvider(*args, **kwargs)</code>","text":"<p>               Bases: <code>ModelProvider</code></p> <p>Provider for Hugging Face Transformers models.</p> Source code in <code>pixano_inference/providers/transformers.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the transformer provider.\"\"\"\n    assert_transformers_installed()\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api_reference/providers/transformers/#pixano_inference.providers.transformers.TransformersProvider.image_mask_generation","title":"<code>image_mask_generation(request, model, *args, **kwargs)</code>","text":"<p>Generate a mask from the image.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ImageMaskGenerationRequest</code> <p>Request for the generation.</p> required <code>model</code> <code>TransformerModel</code> <p>Model to use for the generation.</p> required <code>args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ImageMaskGenerationOutput</code> <p>Output of the generation</p> Source code in <code>pixano_inference/providers/transformers.py</code> <pre><code>def image_mask_generation(\n    self,\n    request: ImageMaskGenerationRequest,\n    model: TransformerModel,  # type: ignore[override]\n    *args: Any,\n    **kwargs: Any,\n) -&gt; ImageMaskGenerationOutput:\n    \"\"\"Generate a mask from the image.\n\n    Args:\n        request: Request for the generation.\n        model: Model to use for the generation.\n        args: Additional arguments.\n        kwargs: Additional keyword arguments.\n\n    Returns:\n        Output of the generation\n    \"\"\"\n    request_input = request.to_input()\n    image = convert_string_to_image(request_input.image)\n\n    if request_input.image_embedding is not None:\n        image_embedding = vector_to_tensor(request_input.image_embedding)\n\n    model_input = request_input.model_dump(exclude=[\"image\", \"image_embedding\"])\n    model_input[\"image\"] = image\n    model_input[\"image_embedding\"] = image_embedding if request_input.image_embedding is not None else None\n    output = model.image_mask_generation(**model_input)\n    return output\n</code></pre>"},{"location":"api_reference/providers/transformers/#pixano_inference.providers.transformers.TransformersProvider.image_zero_shot_detection","title":"<code>image_zero_shot_detection(request, model, *args, **kwargs)</code>","text":"<p>Perform zero-shot image detection.</p> Source code in <code>pixano_inference/providers/transformers.py</code> <pre><code>def image_zero_shot_detection(\n    self,\n    request: ImageZeroShotDetectionRequest,\n    model: TransformerModel,  # type: ignore[override]\n    *args: Any,\n    **kwargs: Any,\n) -&gt; ImageZeroShotDetectionOutput:\n    \"\"\"Perform zero-shot image detection.\"\"\"\n    request_input = request.to_input()\n\n    image = convert_string_to_image(request_input.image)\n    classes = request.classes\n    if isinstance(classes, list):\n        classes = \". \".join(classes)\n\n    model_input = request_input.model_dump(exclude=[\"image\", \"classes\"])\n    model_input[\"image\"] = image\n    model_input[\"classes\"] = classes\n\n    output = model.image_zero_shot_detection(**model_input)\n    return output\n</code></pre>"},{"location":"api_reference/providers/transformers/#pixano_inference.providers.transformers.TransformersProvider.load_model","title":"<code>load_model(name, task, device, path=None, processor_config={}, config={})</code>","text":"<p>Load a model from transformers.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>task</code> <code>Task | str</code> <p>Task of the model.</p> required <code>device</code> <code>device</code> <p>Device to use for the model.</p> required <code>path</code> <code>Path | str | None</code> <p>Path to the model or its Hugging Face hub's identifier.</p> <code>None</code> <code>processor_config</code> <code>dict</code> <p>Configuration for the processor.</p> <code>{}</code> <code>config</code> <code>dict</code> <p>Configuration for the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>TransformerModel</code> <p>Loaded model.</p> Source code in <code>pixano_inference/providers/transformers.py</code> <pre><code>def load_model(\n    self,\n    name: str,\n    task: Task | str,\n    device: \"torch.device\",\n    path: Path | str | None = None,\n    processor_config: dict = {},\n    config: dict = {},\n) -&gt; TransformerModel:\n    \"\"\"Load a model from transformers.\n\n    Args:\n        name: Name of the model.\n        task: Task of the model.\n        device: Device to use for the model.\n        path: Path to the model or its Hugging Face hub's identifier.\n        processor_config: Configuration for the processor.\n        config: Configuration for the model.\n\n    Returns:\n        Loaded model.\n    \"\"\"\n    if path is None:\n        raise ValueError(\"Path is required to load a model from transformers.\")\n    if isinstance(task, str):\n        task = str_to_task(task)\n    processor = AutoProcessor.from_pretrained(path, **processor_config)\n\n    if (quantization_config := config.pop(\"quantization_config\", None)) is not None:\n        quantization_config = BitsAndBytesConfig(**quantization_config)\n        config[\"quantization_config\"] = quantization_config\n\n    model = get_transformer_automodel_from_pretrained(path, task, device_map=device, **config)\n    if model is None:\n        if task in [NLPTask.CONDITONAL_GENERATION, MultimodalImageNLPTask.CONDITIONAL_GENERATION]:\n            model = get_conditional_generation_transformer_from_pretrained(name, path, device_map=device, **config)\n\n    model = model.eval()\n    model = torch.compile(model)\n\n    our_model = TransformerModel(name, path, processor, model)\n    return our_model\n</code></pre>"},{"location":"api_reference/providers/transformers/#pixano_inference.providers.transformers.TransformersProvider.text_image_conditional_generation","title":"<code>text_image_conditional_generation(request, model, *args, **kwargs)</code>","text":"<p>Generate text from an image and a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>TextImageConditionalGenerationRequest</code> <p>Request for text-image conditional generation.</p> required <code>model</code> <code>TransformerModel</code> <p>Model for text-image conditional generation</p> required <code>args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextImageConditionalGenerationOutput</code> <p>Output of text-image conditional generation.</p> Source code in <code>pixano_inference/providers/transformers.py</code> <pre><code>def text_image_conditional_generation(\n    self,\n    request: TextImageConditionalGenerationRequest,\n    model: TransformerModel,  # type: ignore[override]\n    *args: Any,\n    **kwargs: Any,\n) -&gt; TextImageConditionalGenerationOutput:\n    \"\"\"Generate text from an image and a prompt.\n\n    Args:\n        request: Request for text-image conditional generation.\n        model: Model for text-image conditional generation\n        args: Additional arguments.\n        kwargs: Additional keyword arguments.\n\n    Returns:\n        Output of text-image conditional generation.\n    \"\"\"\n    model_input = request.to_input()\n\n    images: list[Image] | None\n    if model_input.images is None:\n        if isinstance(model_input.prompt, str):\n            raise ValueError(\"Images must be provided if the prompt is a string.\")\n        images = []\n        for message in model_input.prompt:\n            new_content = []\n            for content in message[\"content\"]:\n                if content[\"type\"] == \"image_url\":\n                    images.append(convert_string_to_image(content[\"image_url\"][\"url\"]))\n                    new_content.append({\"type\": \"image\"})\n                else:\n                    new_content.append(content)\n            message[\"content\"] = new_content\n\n    else:\n        images = (\n            [convert_string_to_image(image) for image in model_input[\"images\"]]\n            if len(model_input[\"images\"]) &gt; 0\n            else None\n        )\n\n    model_input_dump = model_input.model_dump()\n    model_input_dump[\"images\"] = images\n    output = model.text_image_conditional_generation(**model_input_dump)\n    return output\n</code></pre>"},{"location":"api_reference/providers/transformers/#pixano_inference.providers.transformers.get_conditional_generation_transformer_from_pretrained","title":"<code>get_conditional_generation_transformer_from_pretrained(name, path, **model_kwargs)</code>","text":"<p>Get a transformer model from transformers using automodel.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>path</code> <code>Path | str | None</code> <p>Path to the model or its Hugging Face hub's identifier.</p> required <code>model_kwargs</code> <code>Any</code> <p>Additional keyword arguments for the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>PreTrainedModel</code> <p>Model from Transformers.</p> Source code in <code>pixano_inference/providers/transformers.py</code> <pre><code>def get_conditional_generation_transformer_from_pretrained(\n    name: str, path: Path | str | None, **model_kwargs: Any\n) -&gt; PreTrainedModel:\n    \"\"\"Get a transformer model from transformers using automodel.\n\n    Args:\n        name: Name of the model.\n        path: Path to the model or its Hugging Face hub's identifier.\n        model_kwargs: Additional keyword arguments for the model.\n\n    Returns:\n        Model from Transformers.\n    \"\"\"\n    name = name.lower()\n    if \"llava\" in name:\n        if \"next\" in name:\n            if \"video\" in name:\n                from transformers import LlavaNextVideoForConditionalGeneration\n\n                model = LlavaNextVideoForConditionalGeneration.from_pretrained(path, **model_kwargs)\n            else:\n                from transformers import LlavaNextForConditionalGeneration\n\n                model = LlavaNextForConditionalGeneration.from_pretrained(path, **model_kwargs)\n        else:\n            from transformers import LlavaForConditionalGeneration\n\n            model = LlavaForConditionalGeneration.from_pretrained(path, **model_kwargs)\n    else:\n        raise ValueError(f\"Model {name} not supported.\")\n    return model\n</code></pre>"},{"location":"api_reference/providers/transformers/#pixano_inference.providers.transformers.get_transformer_automodel_from_pretrained","title":"<code>get_transformer_automodel_from_pretrained(pretrained_model_name_or_path, task, **model_kwargs)</code>","text":"<p>Get a transformer model from transformers using automodel.</p> <p>Parameters:</p> Name Type Description Default <code>pretrained_model_name_or_path</code> <code>str | Path</code> <p>Name or path of the pretrained model.</p> required <code>task</code> <code>Task</code> <p>Task of the model.</p> required <code>model_kwargs</code> <code>Any</code> <p>Additional keyword arguments for the model.</p> <code>{}</code> Source code in <code>pixano_inference/providers/transformers.py</code> <pre><code>def get_transformer_automodel_from_pretrained(\n    pretrained_model_name_or_path: str | Path, task: Task, **model_kwargs: Any\n):\n    \"\"\"Get a transformer model from transformers using automodel.\n\n    Args:\n        pretrained_model_name_or_path: Name or path of the pretrained model.\n        task: Task of the model.\n        model_kwargs: Additional keyword arguments for the model.\n    \"\"\"\n    assert_transformers_installed()\n    pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n    if isinstance(task, ImageTask):\n        match task:\n            case ImageTask.CLASSIFICATION:\n                from transformers import AutoModelForImageClassification\n\n                return AutoModelForImageClassification.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case ImageTask.DEPTH_ESTIMATION:\n                from transformers import AutoModelForDepthEstimation\n\n                return AutoModelForDepthEstimation.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case ImageTask.FEATURE_EXTRACTION:\n                from transformers import AutoModel\n\n                return AutoModel.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case ImageTask.KEYPOINT_DETECTION:\n                from transformers import AutoModelForKeypointDetection\n\n                return AutoModelForKeypointDetection.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case ImageTask.MASK_GENERATION:\n                from transformers import AutoModelForMaskGeneration\n\n                return AutoModelForMaskGeneration.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case ImageTask.OBJECT_DETECTION:\n                from transformers import AutoModelForObjectDetection\n\n                return AutoModelForObjectDetection.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case ImageTask.SEMANTIC_SEGMENTATION:\n                from transformers import AutoModelForSemanticSegmentation\n\n                return AutoModelForSemanticSegmentation.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case ImageTask.INSTANCE_SEGMENTATION:\n                from transformers import AutoModelForInstanceSegmentation\n\n                return AutoModelForInstanceSegmentation.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case ImageTask.UNIVERSAL_SEGMENTATION:\n                from transformers import AutoModelForUniversalSegmentation\n\n                return AutoModelForUniversalSegmentation.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case ImageTask.ZERO_SHOT_CLASSIFICATION:\n                from transformers import AutoModelForZeroShotImageClassification\n\n                return AutoModelForZeroShotImageClassification.from_pretrained(\n                    pretrained_model_name_or_path, **model_kwargs\n                )\n            case ImageTask.ZERO_SHOT_DETECTION:\n                from transformers import AutoModelForZeroShotObjectDetection\n\n                return AutoModelForZeroShotObjectDetection.from_pretrained(\n                    pretrained_model_name_or_path, **model_kwargs\n                )\n    elif isinstance(task, NLPTask):\n        match task:\n            case NLPTask.CAUSAL_LM:\n                from transformers import AutoModelForCausalLM\n\n                return AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case NLPTask.MASKED_LM:\n                from transformers import AutoModelForMaskedLM\n\n                return AutoModelForMaskedLM.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case NLPTask.MASK_GENERATION:\n                from transformers import AutoModelForMaskGeneration\n\n                return AutoModelForMaskGeneration.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case NLPTask.SEQUENCE_CLASSIFICATION:\n                from transformers import AutoModelForSequenceClassification\n\n                return AutoModelForSequenceClassification.from_pretrained(\n                    pretrained_model_name_or_path, **model_kwargs\n                )\n            case NLPTask.MULTIPLE_CHOICE:\n                from transformers import AutoModelForMultipleChoice\n\n                return AutoModelForMultipleChoice.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case NLPTask.NEXT_SENTENCE_PREDICTION:\n                from transformers import AutoModelForNextSentencePrediction\n\n                return AutoModelForNextSentencePrediction.from_pretrained(\n                    pretrained_model_name_or_path, **model_kwargs\n                )\n            case NLPTask.TOKEN_CLASSIFICATION:\n                from transformers import AutoModelForTokenClassification\n\n                return AutoModelForTokenClassification.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case NLPTask.QUESTION_ANSWERING:\n                from transformers import AutoModelForQuestionAnswering\n\n                return AutoModelForQuestionAnswering.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case NLPTask.TEXT_ENCODING:\n                from transformers import AutoModelForTextEncoding\n\n                return AutoModelForTextEncoding.from_pretrained(pretrained_model_name_or_path, **model_kwargs)\n            case _:\n                raise ValueError(f\"Task {task} not supported.\")\n</code></pre>"},{"location":"api_reference/providers/utils/","title":"utils","text":""},{"location":"api_reference/providers/utils/#pixano_inference.providers.utils","title":"<code>pixano_inference.providers.utils</code>","text":"<p>Provider for the SAM2 model.</p>"},{"location":"api_reference/providers/utils/#pixano_inference.providers.utils.get_provider","title":"<code>get_provider(provider)</code>","text":"<p>Return the provider from the registry.</p> Source code in <code>pixano_inference/providers/utils.py</code> <pre><code>def get_provider(provider: str) -&gt; type[BaseProvider]:\n    \"\"\"Return the provider from the registry.\"\"\"\n    if (actual_provider := PROVIDERS_REGISTRY.get(provider)) is not None:\n        return actual_provider\n    raise ValueError(f\"Provider {provider} not found.\")\n</code></pre>"},{"location":"api_reference/providers/utils/#pixano_inference.providers.utils.get_provider_name","title":"<code>get_provider_name(provider)</code>","text":"<p>Return the name of the provider.</p> Source code in <code>pixano_inference/providers/utils.py</code> <pre><code>def get_provider_name(provider: BaseProvider) -&gt; str:\n    \"\"\"Return the name of the provider.\"\"\"\n    for name, provider_cls in PROVIDERS_REGISTRY.items():\n        if isinstance(provider, provider_cls):\n            return name\n    raise ValueError(f\"Provider {provider} not found.\")\n</code></pre>"},{"location":"api_reference/providers/utils/#pixano_inference.providers.utils.get_providers","title":"<code>get_providers()</code>","text":"<p>Return the list of providers in the registry.</p> Source code in <code>pixano_inference/providers/utils.py</code> <pre><code>def get_providers() -&gt; list[str]:\n    \"\"\"Return the list of providers in the registry.\"\"\"\n    return list(PROVIDERS_REGISTRY.keys())\n</code></pre>"},{"location":"api_reference/providers/utils/#pixano_inference.providers.utils.instantiate_provider","title":"<code>instantiate_provider(provider)</code>","text":"<p>Instantiate a provider.</p> Source code in <code>pixano_inference/providers/utils.py</code> <pre><code>def instantiate_provider(provider: str) -&gt; BaseProvider:\n    \"\"\"Instantiate a provider.\"\"\"\n    return get_provider(provider)()\n</code></pre>"},{"location":"api_reference/providers/utils/#pixano_inference.providers.utils.is_provider","title":"<code>is_provider(provider)</code>","text":"<p>Return True if the provider is in the registry.</p> Source code in <code>pixano_inference/providers/utils.py</code> <pre><code>def is_provider(provider: str) -&gt; bool:\n    \"\"\"Return True if the provider is in the registry.\"\"\"\n    return provider in PROVIDERS_REGISTRY\n</code></pre>"},{"location":"api_reference/providers/vllm/","title":"vllm","text":""},{"location":"api_reference/providers/vllm/#pixano_inference.providers.vllm","title":"<code>pixano_inference.providers.vllm</code>","text":"<p>Provider for vLLM models.</p>"},{"location":"api_reference/providers/vllm/#pixano_inference.providers.vllm.VLLMProvider","title":"<code>VLLMProvider(*args, **kwargs)</code>","text":"<p>               Bases: <code>ModelProvider</code></p> <p>Provider for vLLM models.</p> Source code in <code>pixano_inference/providers/vllm.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the vLLM provider.\"\"\"\n    assert_vllm_installed()\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api_reference/providers/vllm/#pixano_inference.providers.vllm.VLLMProvider.load_model","title":"<code>load_model(name, task, device, path=None, processor_config={}, config={})</code>","text":"<p>Load a model from vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>task</code> <code>Task | str</code> <p>Task of the model.</p> required <code>device</code> <code>device</code> <p>Device to use for the model.</p> required <code>path</code> <code>str | None</code> <p>Path to the model or its Hugging Face hub's identifier.</p> <code>None</code> <code>processor_config</code> <code>dict</code> <p>Configuration for the processor.</p> <code>{}</code> <code>config</code> <code>dict</code> <p>Configuration for the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>VLLMModel</code> <p>Loaded model.</p> Source code in <code>pixano_inference/providers/vllm.py</code> <pre><code>def load_model(\n    self,\n    name: str,\n    task: Task | str,\n    device: \"torch.device\",\n    path: str | None = None,  # type: ignore[override]\n    processor_config: dict = {},\n    config: dict = {},\n) -&gt; VLLMModel:\n    \"\"\"Load a model from vLLM.\n\n    Args:\n        name: Name of the model.\n        task: Task of the model.\n        device: Device to use for the model.\n        path: Path to the model or its Hugging Face hub's identifier.\n        processor_config: Configuration for the processor.\n        config: Configuration for the model.\n\n    Returns:\n        Loaded model.\n    \"\"\"\n    if path is None:\n        raise ValueError(\"Path is required to load a model from vLLm.\")\n    if isinstance(task, str):\n        task = str_to_task(task)\n\n    our_model = VLLMModel(\n        name=name, vllm_model=path, model_config=config, processor_config=processor_config, device=device\n    )\n\n    return our_model\n</code></pre>"},{"location":"api_reference/providers/vllm/#pixano_inference.providers.vllm.VLLMProvider.text_image_conditional_generation","title":"<code>text_image_conditional_generation(request, model, *args, **kwargs)</code>","text":"<p>Generate text from an image and a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>TextImageConditionalGenerationRequest</code> <p>Request for text-image conditional generation.</p> required <code>model</code> <code>VLLMModel</code> <p>Model for text-image conditional generation</p> required <code>args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextImageConditionalGenerationOutput</code> <p>Output of text-image conditional generation.</p> Source code in <code>pixano_inference/providers/vllm.py</code> <pre><code>def text_image_conditional_generation(\n    self,\n    request: TextImageConditionalGenerationRequest,\n    model: VLLMModel,  # type: ignore[override]\n    *args: Any,\n    **kwargs: Any,\n) -&gt; TextImageConditionalGenerationOutput:\n    \"\"\"Generate text from an image and a prompt.\n\n    Args:\n        request: Request for text-image conditional generation.\n        model: Model for text-image conditional generation\n        args: Additional arguments.\n        kwargs: Additional keyword arguments.\n\n    Returns:\n        Output of text-image conditional generation.\n    \"\"\"\n    model_input = request.to_input()\n    if model_input.images is not None:\n        raise ValueError(\"images should be passed in the prompt for vLLM.\")\n    if isinstance(model_input.prompt, str):\n        raise ValueError(\"Pixano-inference only support a chat template for vLLM.\")\n\n    output = model.text_image_conditional_generation(**model_input.model_dump(exclude=\"images\"))\n    return output\n</code></pre>"},{"location":"api_reference/pydantic/base/","title":"base","text":""},{"location":"api_reference/pydantic/base/#pixano_inference.pydantic.base","title":"<code>pixano_inference.pydantic.base</code>","text":"<p>Pydantic base models for request and response.</p>"},{"location":"api_reference/pydantic/base/#pixano_inference.pydantic.base.APIRequest","title":"<code>APIRequest</code>","text":"<p>               Bases: <code>BaseRequest</code>, <code>ABC</code></p> <p>API request model.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>API key.</p> <code>secret_key</code> <code>str</code> <p>Secret key.</p>"},{"location":"api_reference/pydantic/base/#pixano_inference.pydantic.base.BaseRequest","title":"<code>BaseRequest</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base request model.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>Name of the model.</p>"},{"location":"api_reference/pydantic/base/#pixano_inference.pydantic.base.BaseRequest.to_base_model","title":"<code>to_base_model(base_model)</code>","text":"<p>Convert request to input type.</p> Source code in <code>pixano_inference/pydantic/base.py</code> <pre><code>def to_base_model(self, base_model: type[T]) -&gt; T:\n    \"\"\"Convert request to input type.\"\"\"\n    if not issubclass(base_model, BaseModel):\n        raise ValueError(f\"base_model must be a subclass of pydantic's BaseModel, got {base_model.__name__}.\")\n    return base_model.model_validate(self.model_dump(include=list(base_model.model_fields.keys())))\n</code></pre>"},{"location":"api_reference/pydantic/base/#pixano_inference.pydantic.base.BaseResponse","title":"<code>BaseResponse</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base response model.</p> <p>Attributes:</p> Name Type Description <code>task_id</code> <p>ID of the celery task.</p> <code>status</code> <code>str</code> <p>Status of the celery task.</p> <code>timestamp</code> <code>datetime</code> <p>Timestamp of the response.</p> <code>processing_time</code> <code>float</code> <p>Processing time of the response.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Metadata of the response.</p> <code>data</code> <code>Any</code> <p>Data of the response.</p>"},{"location":"api_reference/pydantic/base/#pixano_inference.pydantic.base.CeleryTask","title":"<code>CeleryTask</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Celery task model.</p> <p>Attributes:</p> Name Type Description <code>task_id</code> <p>ID of the celery task.</p> <code>status</code> <code>str</code> <p>Status of the celery task.</p>"},{"location":"api_reference/pydantic/models/","title":"models","text":""},{"location":"api_reference/pydantic/models/#pixano_inference.pydantic.models","title":"<code>pixano_inference.pydantic.models</code>","text":"<p>Pydantic models for model configuration.</p>"},{"location":"api_reference/pydantic/models/#pixano_inference.pydantic.models.ModelConfig","title":"<code>ModelConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model configuration for instantiation.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the model.</p> <code>task</code> <code>str</code> <p>Task of the model.</p> <code>path</code> <code>Path | str | None</code> <p>Path to the model dump.</p> <code>config</code> <code>dict[str, Any]</code> <p>Configuration of the model.</p> <code>processor_config</code> <code>dict[str, Any]</code> <p>Configuration of the processor.</p>"},{"location":"api_reference/pydantic/models/#pixano_inference.pydantic.models.ModelInfo","title":"<code>ModelInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model Information.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the model.</p> <code>task</code> <code>str</code> <p>Task of the model.</p>"},{"location":"api_reference/pydantic/nd_array/","title":"nd_array","text":""},{"location":"api_reference/pydantic/nd_array/#pixano_inference.pydantic.nd_array","title":"<code>pixano_inference.pydantic.nd_array</code>","text":"<p>Pydantic models for N-dimensional arrays.</p>"},{"location":"api_reference/pydantic/nd_array/#pixano_inference.pydantic.nd_array.NDArray","title":"<code>NDArray</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[T]</code>, <code>ABC</code></p> <p>Represents an N-dimensional array.</p> <p>Attributes:</p> Name Type Description <code>values</code> <code>list[T]</code> <p>The list of values.</p> <code>shape</code> <code>list[int]</code> <p>The shape of the array, represented as a list of integers.</p> <code>np_dtype</code> <code>dtype</code> <p>The NumPy data type of the array.</p>"},{"location":"api_reference/pydantic/nd_array/#pixano_inference.pydantic.nd_array.NDArray.from_numpy","title":"<code>from_numpy(arr)</code>  <code>classmethod</code>","text":"<p>Create an instance of the class from a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>The NumPy array to convert.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>An instance of the class with values and shape derived from the input array.</p> Source code in <code>pixano_inference/pydantic/nd_array.py</code> <pre><code>@classmethod\ndef from_numpy(cls, arr: np.ndarray) -&gt; Self:\n    \"\"\"Create an instance of the class from a NumPy array.\n\n    Args:\n        arr: The NumPy array to convert.\n\n    Returns:\n        An instance of the class with values and shape derived from\n            the input array.\n    \"\"\"\n    shape = list(arr.shape)\n    arr = arr.astype(dtype=cls.np_dtype)\n    return cls(\n        values=arr.reshape(-1).tolist(),\n        shape=shape,\n    )\n</code></pre>"},{"location":"api_reference/pydantic/nd_array/#pixano_inference.pydantic.nd_array.NDArray.from_torch","title":"<code>from_torch(tensor)</code>  <code>classmethod</code>","text":"<p>Create an instance of the class from a PyTorch tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The PyTorch tensor to convert.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>An instance of the class with values and shape derived from the input tensor.</p> Source code in <code>pixano_inference/pydantic/nd_array.py</code> <pre><code>@classmethod\ndef from_torch(cls, tensor: \"Tensor\") -&gt; Self:\n    \"\"\"Create an instance of the class from a PyTorch tensor.\n\n    Args:\n        tensor: The PyTorch tensor to convert.\n\n    Returns:\n        An instance of the class with values and shape derived from\n            the input tensor.\n    \"\"\"\n    assert_torch_installed()\n    return cls.from_numpy(tensor.cpu().numpy())\n</code></pre>"},{"location":"api_reference/pydantic/nd_array/#pixano_inference.pydantic.nd_array.NDArray.to_numpy","title":"<code>to_numpy()</code>","text":"<p>Convert the instance to a NumPy array.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array with values and shape derived from the instance.</p> Source code in <code>pixano_inference/pydantic/nd_array.py</code> <pre><code>def to_numpy(self) -&gt; np.ndarray:\n    \"\"\"Convert the instance to a NumPy array.\n\n    Returns:\n        A NumPy array with values and shape derived from the instance.\n    \"\"\"\n    array = np.array(self.values, dtype=self.np_dtype).reshape(self.shape)\n    return array\n</code></pre>"},{"location":"api_reference/pydantic/nd_array/#pixano_inference.pydantic.nd_array.NDArray.to_torch","title":"<code>to_torch()</code>","text":"<p>Convert the instance to a PyTorch tensor.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A PyTorch tensor with values and shape derived from the instance.</p> Source code in <code>pixano_inference/pydantic/nd_array.py</code> <pre><code>def to_torch(self) -&gt; \"Tensor\":\n    \"\"\"Convert the instance to a PyTorch tensor.\n\n    Returns:\n        A PyTorch tensor with values and shape derived from the instance.\n    \"\"\"\n    assert_torch_installed()\n    return torch.from_numpy(self.to_numpy())\n</code></pre>"},{"location":"api_reference/pydantic/nd_array/#pixano_inference.pydantic.nd_array.NDArrayFloat","title":"<code>NDArrayFloat</code>","text":"<p>               Bases: <code>NDArray[float]</code></p> <p>Represents an N-dimensional array of 32-bit floating-point values.</p> <p>Attributes:</p> Name Type Description <code>values</code> <code>list[T]</code> <p>The list of 32-bit floating-point values in the array.</p> <code>shape</code> <code>list[int]</code> <p>The shape of the array, represented as a list of integers.</p> <code>np_dtype</code> <code>dtype</code> <p>The NumPy data type of the array.</p>"},{"location":"api_reference/pydantic/data/vector_database/","title":"vector_database","text":""},{"location":"api_reference/pydantic/data/vector_database/#pixano_inference.pydantic.data.vector_database","title":"<code>pixano_inference.pydantic.data.vector_database</code>","text":"<p>Pydantic model for the Vector databases.</p>"},{"location":"api_reference/pydantic/data/vector_database/#pixano_inference.pydantic.data.vector_database.LanceVector","title":"<code>LanceVector</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Lance vector model.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> <p>The path to the LANCE dataset.</p> <code>column</code> <code>str</code> <p>The column to read.</p> <code>indice</code> <code>int | None</code> <p>The index of the row to read.</p> <code>where</code> <code>str | None</code> <p>The filter to apply. If specified, only one row should be returned.</p> <code>shape</code> <code>list[int] | None</code> <p>The shape of the vector.</p>"},{"location":"api_reference/pydantic/data/vector_database/#pixano_inference.pydantic.data.vector_database.LanceVector.read_vector","title":"<code>read_vector(return_type='tensor')</code>","text":"<p>Reads a vector from a Lance dataset.</p> <p>Parameters:</p> Name Type Description Default <code>return_type</code> <code>Literal['tensor', 'numpy']</code> <p>The type of the return value. Either 'tensor' or 'numpy'.</p> <code>'tensor'</code> <p>Returns:</p> Type Description <code>'Tensor' | ndarray</code> <p>The vector.</p> Source code in <code>pixano_inference/pydantic/data/vector_database.py</code> <pre><code>def read_vector(self, return_type: Literal[\"tensor\", \"numpy\"] = \"tensor\") -&gt; \"Tensor\" | np.ndarray:\n    \"\"\"Reads a vector from a Lance dataset.\n\n    Args:\n        return_type: The type of the return value. Either 'tensor' or 'numpy'.\n\n    Returns:\n        The vector.\n    \"\"\"\n    return read_lance_vector(self.path, self.column, self.indice, self.where, self.shape, return_type)\n</code></pre>"},{"location":"api_reference/pydantic/data/vector_database/#pixano_inference.pydantic.data.vector_database.LanceVector.validate_indice_or_where","title":"<code>validate_indice_or_where()</code>","text":"<p>Validates that only one of 'indice' and 'where' is specified.</p> Source code in <code>pixano_inference/pydantic/data/vector_database.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_indice_or_where(self) -&gt; Self:\n    \"\"\"Validates that only one of 'indice' and 'where' is specified.\"\"\"\n    if self.indice is not None and self.where is not None:\n        raise ValueError(\"Only one of 'indice' and 'where' can be specified.\")\n    elif self.indice is None and self.where is None:\n        raise ValueError(\"One of 'indice' and 'where' must be specified.\")\n    return self\n</code></pre>"},{"location":"api_reference/pydantic/tasks/image/mask_generation/","title":"mask_generation","text":""},{"location":"api_reference/pydantic/tasks/image/mask_generation/#pixano_inference.pydantic.tasks.image.mask_generation","title":"<code>pixano_inference.pydantic.tasks.image.mask_generation</code>","text":"<p>Pydantic models for image mask generation task.</p>"},{"location":"api_reference/pydantic/tasks/image/mask_generation/#pixano_inference.pydantic.tasks.image.mask_generation.ImageMaskGenerationInput","title":"<code>ImageMaskGenerationInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input for image mask generation.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>str | Path</code> <p>Image for image mask generation.</p> <code>image_embedding</code> <code>NDArrayFloat | LanceVector | None</code> <p>Image embedding for the image mask generation.</p> <code>high_resolution_features</code> <code>list[NDArrayFloat] | list[LanceVector] | None</code> <p>High resolution features for the image mask generation.</p> <code>points</code> <code>list[list[list[int]]] | None</code> <p>Points for the image mask generation. The first fimension is the number of prompts the second the number of points per mask and the third the coordinates of the points.</p> <code>labels</code> <code>list[list[int]] | None</code> <p>Labels for the image mask generation. The first fimension is the number of prompts, the second the number of labels per mask.</p> <code>boxes</code> <code>list[list[int]] | None</code> <p>Boxes for the image mask generation. The first fimension is the number of prompts, the second the coordinates of the boxes.</p> <code>num_multimask_outputs</code> <code>int</code> <p>Number of masks to generate per prediction.</p> <code>multimask_output</code> <code>bool</code> <p>Whether to generate multiple masks per prediction.</p> <code>return_image_embedding</code> <code>bool</code> <p>Whether to return the image embeddings.</p>"},{"location":"api_reference/pydantic/tasks/image/mask_generation/#pixano_inference.pydantic.tasks.image.mask_generation.ImageMaskGenerationOutput","title":"<code>ImageMaskGenerationOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output for image mask generation.</p> <p>Attributes:</p> Name Type Description <code>masks</code> <code>list[list[CompressedRLE]]</code> <p>Generated masks. The first dimension is the number of predictions, the second the number of masks per prediction.</p> <code>scores</code> <code>NDArrayFloat</code> <p>Scores of the masks. The first dimension is the number of predictions, the second the number of masks per prediction.</p> <code>image_embedding</code> <code>NDArrayFloat | None</code> <p>Image embeddings.</p> <code>high_resolution_features</code> <code>list[NDArrayFloat] | None</code> <p>High resolution features.</p>"},{"location":"api_reference/pydantic/tasks/image/mask_generation/#pixano_inference.pydantic.tasks.image.mask_generation.ImageMaskGenerationRequest","title":"<code>ImageMaskGenerationRequest</code>","text":"<p>               Bases: <code>BaseRequest</code>, <code>ImageMaskGenerationInput</code></p> <p>Request for image mask generation.</p>"},{"location":"api_reference/pydantic/tasks/image/mask_generation/#pixano_inference.pydantic.tasks.image.mask_generation.ImageMaskGenerationRequest.to_input","title":"<code>to_input()</code>","text":"<p>Convert the request to the input.</p> Source code in <code>pixano_inference/pydantic/tasks/image/mask_generation.py</code> <pre><code>def to_input(self) -&gt; ImageMaskGenerationInput:\n    \"\"\"Convert the request to the input.\"\"\"\n    return self.to_base_model(ImageMaskGenerationInput)\n</code></pre>"},{"location":"api_reference/pydantic/tasks/image/mask_generation/#pixano_inference.pydantic.tasks.image.mask_generation.ImageMaskGenerationResponse","title":"<code>ImageMaskGenerationResponse</code>","text":"<p>               Bases: <code>BaseResponse</code></p> <p>Response for image mask generation.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>ImageMaskGenerationOutput</code> <p>Output of the generation.</p>"},{"location":"api_reference/pydantic/tasks/image/utils/","title":"utils","text":""},{"location":"api_reference/pydantic/tasks/image/utils/#pixano_inference.pydantic.tasks.image.utils","title":"<code>pixano_inference.pydantic.tasks.image.utils</code>","text":"<p>Pydantic models for image tasks.</p>"},{"location":"api_reference/pydantic/tasks/image/utils/#pixano_inference.pydantic.tasks.image.utils.CompressedRLE","title":"<code>CompressedRLE</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Compressed RLE mask type.</p> <p>Attributes:</p> Name Type Description <code>size</code> <code>list[int]</code> <p>Mask size.</p> <code>counts</code> <code>bytes</code> <p>Mask RLE encoding.</p>"},{"location":"api_reference/pydantic/tasks/image/utils/#pixano_inference.pydantic.tasks.image.utils.CompressedRLE.from_mask","title":"<code>from_mask(mask, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a compressed RLE mask from a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Image | ndarray</code> <p>The mask as a NumPy array.</p> required <code>kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CompressedRLE</code> <p>The compressed RLE mask.</p> Source code in <code>pixano_inference/pydantic/tasks/image/utils.py</code> <pre><code>@staticmethod\ndef from_mask(mask: Image | np.ndarray, **kwargs: Any) -&gt; \"CompressedRLE\":\n    \"\"\"Create a compressed RLE mask from a NumPy array.\n\n    Args:\n        mask: The mask as a NumPy array.\n        kwargs: Additional arguments.\n\n    Returns:\n        The compressed RLE mask.\n    \"\"\"\n    rle = mask_to_rle(mask)\n    return CompressedRLE(size=rle[\"size\"], counts=rle[\"counts\"], **kwargs)\n</code></pre>"},{"location":"api_reference/pydantic/tasks/image/utils/#pixano_inference.pydantic.tasks.image.utils.CompressedRLE.to_mask","title":"<code>to_mask()</code>","text":"<p>Convert the compressed RLE mask to a NumPy array.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>The mask as a NumPy array.</p> Source code in <code>pixano_inference/pydantic/tasks/image/utils.py</code> <pre><code>def to_mask(self) -&gt; np.ndarray:\n    \"\"\"Convert the compressed RLE mask to a NumPy array.\n\n    Returns:\n        The mask as a NumPy array.\n    \"\"\"\n    return rle_to_mask({\"size\": self.size, \"counts\": self.counts})\n</code></pre>"},{"location":"api_reference/pydantic/tasks/image/utils/#pixano_inference.pydantic.tasks.image.utils.mask_to_rle","title":"<code>mask_to_rle(mask)</code>","text":"<p>Encode mask from Pillow or NumPy array to RLE.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Image | ndarray</code> <p>Mask as Pillow or NumPy array.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Mask as RLE.</p> Source code in <code>pixano_inference/pydantic/tasks/image/utils.py</code> <pre><code>def mask_to_rle(mask: Image | np.ndarray) -&gt; dict:\n    \"\"\"Encode mask from Pillow or NumPy array to RLE.\n\n    Args:\n        mask: Mask as Pillow or NumPy array.\n\n    Returns:\n        Mask as RLE.\n    \"\"\"\n    mask_array = np.asfortranarray(mask)\n    return mask_api.encode(mask_array)\n</code></pre>"},{"location":"api_reference/pydantic/tasks/image/utils/#pixano_inference.pydantic.tasks.image.utils.rle_to_mask","title":"<code>rle_to_mask(rle)</code>","text":"<p>Decode mask from RLE to NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>rle</code> <code>dict[str, list[int] | bytes]</code> <p>Mask as RLE.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Mask as NumPy array.</p> Source code in <code>pixano_inference/pydantic/tasks/image/utils.py</code> <pre><code>def rle_to_mask(rle: dict[str, list[int] | bytes]) -&gt; np.ndarray:\n    \"\"\"Decode mask from RLE to NumPy array.\n\n    Args:\n        rle: Mask as RLE.\n\n    Returns:\n        Mask as NumPy array.\n    \"\"\"\n    return mask_api.decode(rle)\n</code></pre>"},{"location":"api_reference/pydantic/tasks/image/zero_shot_detection/","title":"zero_shot_detection","text":""},{"location":"api_reference/pydantic/tasks/image/zero_shot_detection/#pixano_inference.pydantic.tasks.image.zero_shot_detection","title":"<code>pixano_inference.pydantic.tasks.image.zero_shot_detection</code>","text":"<p>Pydantic models for image zero shot detection task.</p>"},{"location":"api_reference/pydantic/tasks/image/zero_shot_detection/#pixano_inference.pydantic.tasks.image.zero_shot_detection.ImageZeroShotDetectionInput","title":"<code>ImageZeroShotDetectionInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input for image zero shot detection.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>str | Path</code> <p>Image for image zero shot detection.</p> <code>classes</code> <code>str | list[str]</code> <p>List of classes to detect.</p>"},{"location":"api_reference/pydantic/tasks/image/zero_shot_detection/#pixano_inference.pydantic.tasks.image.zero_shot_detection.ImageZeroShotDetectionOutput","title":"<code>ImageZeroShotDetectionOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output for image zero shot detection.</p> <p>Attributes:</p> Name Type Description <code>boxes</code> <code>list[list[int]]</code> <p>List of boxes detected by the model.</p> <code>scores</code> <code>list[float]</code> <p>List of scores detected by the model (higher is better).</p> <code>classes</code> <code>list[str]</code> <p>List of class names associated with each box and score.</p>"},{"location":"api_reference/pydantic/tasks/image/zero_shot_detection/#pixano_inference.pydantic.tasks.image.zero_shot_detection.ImageZeroShotDetectionRequest","title":"<code>ImageZeroShotDetectionRequest</code>","text":"<p>               Bases: <code>BaseRequest</code>, <code>ImageZeroShotDetectionInput</code></p> <p>Request for image zero shot detection.</p>"},{"location":"api_reference/pydantic/tasks/image/zero_shot_detection/#pixano_inference.pydantic.tasks.image.zero_shot_detection.ImageZeroShotDetectionRequest.to_input","title":"<code>to_input()</code>","text":"<p>Convert the request to the input.</p> Source code in <code>pixano_inference/pydantic/tasks/image/zero_shot_detection.py</code> <pre><code>def to_input(self) -&gt; ImageZeroShotDetectionInput:\n    \"\"\"Convert the request to the input.\"\"\"\n    return self.to_base_model(ImageZeroShotDetectionInput)\n</code></pre>"},{"location":"api_reference/pydantic/tasks/image/zero_shot_detection/#pixano_inference.pydantic.tasks.image.zero_shot_detection.ImageZeroShotDetectionResponse","title":"<code>ImageZeroShotDetectionResponse</code>","text":"<p>               Bases: <code>BaseResponse</code></p> <p>Response for image zero shot detection.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>ImageZeroShotDetectionOutput</code> <p>Output of the generation.</p>"},{"location":"api_reference/pydantic/tasks/multimodal/conditional_generation/","title":"conditional_generation","text":""},{"location":"api_reference/pydantic/tasks/multimodal/conditional_generation/#pixano_inference.pydantic.tasks.multimodal.conditional_generation","title":"<code>pixano_inference.pydantic.tasks.multimodal.conditional_generation</code>","text":"<p>Pydantic models for text-image conditional generation task.</p>"},{"location":"api_reference/pydantic/tasks/multimodal/conditional_generation/#pixano_inference.pydantic.tasks.multimodal.conditional_generation.TextImageConditionalGenerationInput","title":"<code>TextImageConditionalGenerationInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input for text-image conditional generation.</p> <p>Attributes:</p> Name Type Description <code>prompt</code> <code>str | list[dict[str, Any]]</code> <p>Prompt for the generation. Can be a string or a list of dictionaries to apply a chat template.</p> <code>images</code> <code>list[str | Path] | None</code> <p>Images for the generation. Can be None if images are passed in the prompt.</p> <code>max_new_tokens</code> <code>int</code> <p>Maximum number of new tokens to generate.</p> <code>temperature</code> <code>float</code> <p>Temperature for the generation.</p>"},{"location":"api_reference/pydantic/tasks/multimodal/conditional_generation/#pixano_inference.pydantic.tasks.multimodal.conditional_generation.TextImageConditionalGenerationOutput","title":"<code>TextImageConditionalGenerationOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output for text-image conditional generation.</p> <p>Attributes:</p> Name Type Description <code>generated_text</code> <code>str</code> <p>Generated text.</p> <code>usage</code> <code>UsageConditionalGeneration</code> <p>Usage of the model for the generation.</p> <code>generation_config</code> <code>dict[str, Any]</code> <p>Configuration for the generation.</p>"},{"location":"api_reference/pydantic/tasks/multimodal/conditional_generation/#pixano_inference.pydantic.tasks.multimodal.conditional_generation.TextImageConditionalGenerationRequest","title":"<code>TextImageConditionalGenerationRequest</code>","text":"<p>               Bases: <code>BaseRequest</code>, <code>TextImageConditionalGenerationInput</code></p> <p>Request for text-image conditional generation.</p>"},{"location":"api_reference/pydantic/tasks/multimodal/conditional_generation/#pixano_inference.pydantic.tasks.multimodal.conditional_generation.TextImageConditionalGenerationRequest.to_input","title":"<code>to_input()</code>","text":"<p>Convert the request to the input.</p> Source code in <code>pixano_inference/pydantic/tasks/multimodal/conditional_generation.py</code> <pre><code>def to_input(self) -&gt; TextImageConditionalGenerationInput:\n    \"\"\"Convert the request to the input.\"\"\"\n    return self.to_base_model(TextImageConditionalGenerationInput)\n</code></pre>"},{"location":"api_reference/pydantic/tasks/multimodal/conditional_generation/#pixano_inference.pydantic.tasks.multimodal.conditional_generation.TextImageConditionalGenerationResponse","title":"<code>TextImageConditionalGenerationResponse</code>","text":"<p>               Bases: <code>BaseResponse</code></p> <p>Response for text-image conditional generation.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>TextImageConditionalGenerationOutput</code> <p>Output of the generation.</p>"},{"location":"api_reference/pydantic/tasks/multimodal/conditional_generation/#pixano_inference.pydantic.tasks.multimodal.conditional_generation.UsageConditionalGeneration","title":"<code>UsageConditionalGeneration</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Usage metadata of the model for text-image conditional generation.</p> <p>Attributes:</p> Name Type Description <code>prompt_tokens</code> <code>int</code> <p>Number of tokens in the prompt.</p> <code>completion_tokens</code> <code>int</code> <p>Number of tokens in the completion.</p> <code>total_tokens</code> <code>int</code> <p>Total number of tokens.</p>"},{"location":"api_reference/pydantic/tasks/video/mask_generation/","title":"mask_generation","text":""},{"location":"api_reference/pydantic/tasks/video/mask_generation/#pixano_inference.pydantic.tasks.video.mask_generation","title":"<code>pixano_inference.pydantic.tasks.video.mask_generation</code>","text":"<p>Pydantic models for mask generation task.</p>"},{"location":"api_reference/pydantic/tasks/video/mask_generation/#pixano_inference.pydantic.tasks.video.mask_generation.VideoMaskGenerationInput","title":"<code>VideoMaskGenerationInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input for mask generation.</p> <p>Attributes:</p> Name Type Description <code>video</code> <code>str | Path | list[str] | list[Path]</code> <p>Can be a path to the video or list of paths to the frames or a base64 encoded video or a list of base64 encoded frames.</p> <code>points</code> <code>list[list[list[int]]] | None</code> <p>Points for the mask generation. The first fimension is the number of objects the second the number of points for each object and the third the coordinates of the points.</p> <code>labels</code> <code>list[list[int]] | None</code> <p>Labels for the mask generation. The first fimension is the number of objects, the second the number of labels for each object.</p> <code>boxes</code> <code>list[list[int]] | None</code> <p>Boxes for the mask generation. The first fimension is the number of objects, the second the coordinates of the boxes.</p> <code>objects_ids</code> <code>list[int]</code> <p>IDs of the objects to generate masks for.</p> <code>frame_indexes</code> <code>list[int]</code> <p>Indexes of the frames where the objects are located.</p>"},{"location":"api_reference/pydantic/tasks/video/mask_generation/#pixano_inference.pydantic.tasks.video.mask_generation.VideoMaskGenerationOutput","title":"<code>VideoMaskGenerationOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output for mask generation.</p> <p>Attributes:</p> Name Type Description <code>objects_ids</code> <code>list[int]</code> <p>IDs of the objects.</p> <code>frame_indexes</code> <code>list[int]</code> <p>Indexes of the frames where the objects are located.</p> <code>masks</code> <code>list[CompressedRLE]</code> <p>Masks for the objects.</p>"},{"location":"api_reference/pydantic/tasks/video/mask_generation/#pixano_inference.pydantic.tasks.video.mask_generation.VideoMaskGenerationRequest","title":"<code>VideoMaskGenerationRequest</code>","text":"<p>               Bases: <code>BaseRequest</code>, <code>VideoMaskGenerationInput</code></p> <p>Request for mask generation.</p>"},{"location":"api_reference/pydantic/tasks/video/mask_generation/#pixano_inference.pydantic.tasks.video.mask_generation.VideoMaskGenerationRequest.to_input","title":"<code>to_input()</code>","text":"<p>Convert the request to the input.</p> Source code in <code>pixano_inference/pydantic/tasks/video/mask_generation.py</code> <pre><code>def to_input(self) -&gt; VideoMaskGenerationInput:\n    \"\"\"Convert the request to the input.\"\"\"\n    return self.to_base_model(VideoMaskGenerationInput)\n</code></pre>"},{"location":"api_reference/pydantic/tasks/video/mask_generation/#pixano_inference.pydantic.tasks.video.mask_generation.VideoMaskGenerationResponse","title":"<code>VideoMaskGenerationResponse</code>","text":"<p>               Bases: <code>BaseResponse</code></p> <p>Response for mask generation.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>VideoMaskGenerationOutput</code> <p>Output of the generation.</p>"},{"location":"api_reference/routers/app/","title":"app","text":""},{"location":"api_reference/routers/app/#pixano_inference.routers.app","title":"<code>pixano_inference.routers.app</code>","text":"<p>API routes for the app.</p>"},{"location":"api_reference/routers/app/#pixano_inference.routers.app.get_list_models","title":"<code>get_list_models(settings)</code>  <code>async</code>","text":"<p>List all models available in the app.</p> Source code in <code>pixano_inference/routers/app.py</code> <pre><code>@router.get(\"/models/\", response_model=list[ModelInfo])\nasync def get_list_models(\n    settings: Annotated[Settings, Depends(get_pixano_inference_settings)],\n) -&gt; list[ModelInfo]:\n    \"\"\"List all models available in the app.\"\"\"\n    models = [ModelInfo(name=model_name, task=task) for model_name, task in settings.models_to_task.items()]\n    return models\n</code></pre>"},{"location":"api_reference/routers/app/#pixano_inference.routers.app.get_settings","title":"<code>get_settings(settings)</code>  <code>async</code>","text":"<p>Get the current settings of the app.</p> Source code in <code>pixano_inference/routers/app.py</code> <pre><code>@router.get(\"/settings/\", response_model=Settings)\nasync def get_settings(\n    settings: Annotated[Settings, Depends(get_pixano_inference_settings)],\n) -&gt; Settings:\n    \"\"\"Get the current settings of the app.\"\"\"\n    return settings\n</code></pre>"},{"location":"api_reference/routers/providers/","title":"providers","text":""},{"location":"api_reference/routers/providers/#pixano_inference.routers.providers","title":"<code>pixano_inference.routers.providers</code>","text":"<p>API routes for model providers.</p>"},{"location":"api_reference/routers/providers/#pixano_inference.routers.providers.delete_model","title":"<code>delete_model(model_name, settings)</code>  <code>async</code>","text":"<p>Delete a model from the system.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to be deleted.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_pixano_inference_settings)]</code> <p>Settings of the app.</p> required Source code in <code>pixano_inference/routers/providers.py</code> <pre><code>@router.delete(\"/model/{model_name}\")\nasync def delete_model(\n    model_name: str,\n    settings: Annotated[Settings, Depends(get_pixano_inference_settings)],\n):\n    \"\"\"Delete a model from the system.\n\n    Args:\n        model_name: The name of the model to be deleted.\n        settings: Settings of the app.\n    \"\"\"\n    if model_name not in settings.models:\n        raise HTTPException(status_code=404, detail=f\"Model {model_name} not found\")\n    settings.remove_model(model_name)\n    delete_celery_worker_and_queue(model_name)\n    return\n</code></pre>"},{"location":"api_reference/routers/providers/#pixano_inference.routers.providers.get_instantiate_model_status","title":"<code>get_instantiate_model_status(task_id)</code>  <code>async</code>","text":"<p>Return status of model instantiation.</p> Source code in <code>pixano_inference/routers/providers.py</code> <pre><code>@router.get(\"/instantiate/{task_id}\", response_model=CeleryTask)\nasync def get_instantiate_model_status(\n    task_id: str,\n) -&gt; CeleryTask:\n    \"\"\"Return status of model instantiation.\"\"\"\n    task_result = celery_app.AsyncResult(task_id)\n    return CeleryTask(id=task_result.id, status=task_result.status)\n</code></pre>"},{"location":"api_reference/routers/providers/#pixano_inference.routers.providers.instantiate_model","title":"<code>instantiate_model(config, provider, settings)</code>  <code>async</code>","text":"<p>Instantiate a model from a provider.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ModelConfig</code> <p>Model configuration for instantiation.</p> required <code>provider</code> <code>Annotated[str, Body()]</code> <p>The model provider.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_pixano_inference_settings)]</code> <p>Settings of the app.</p> required Source code in <code>pixano_inference/routers/providers.py</code> <pre><code>@router.post(\"/instantiate\", response_model=CeleryTask)\nasync def instantiate_model(\n    config: ModelConfig,\n    provider: Annotated[str, Body()],\n    settings: Annotated[Settings, Depends(get_pixano_inference_settings)],\n):\n    \"\"\"Instantiate a model from a provider.\n\n    Args:\n        config: Model configuration for instantiation.\n        provider: The model provider.\n        settings: Settings of the app.\n    \"\"\"\n    try:\n        instantiate_provider(provider)\n    except ValueError as e:\n        raise HTTPException(status_code=404, detail=f\"Provider {provider} does not exist.\") from e\n    except ImportError as e:\n        raise HTTPException(status_code=500, detail=f\"Provider {provider} is not installed.\") from e\n\n    if not is_task(config.task):\n        raise HTTPException(status_code=404, detail=f\"Task {config.task} is not a supported task.\")\n\n    gpu = settings.add_model(config.name, config.task)\n    try:\n        task = add_celery_worker_and_queue(model_config=config, provider=provider, gpu=gpu)\n    except Exception as e:\n        settings.remove_model(config.name)\n        delete_celery_worker_and_queue(config.name)\n        raise HTTPException(status_code=400, detail=f\"Error while instantiating {config.task} - {str(e)}\") from e\n    return task\n</code></pre>"},{"location":"api_reference/routers/utils/","title":"utils","text":""},{"location":"api_reference/routers/utils/#pixano_inference.routers.utils","title":"<code>pixano_inference.routers.utils</code>","text":"<p>Utils for the routers.</p>"},{"location":"api_reference/routers/utils/#pixano_inference.routers.utils.delete_task","title":"<code>delete_task(task_id)</code>  <code>async</code>","text":"<p>Delete a task.</p> Source code in <code>pixano_inference/routers/utils.py</code> <pre><code>async def delete_task(task_id: str) -&gt; None:\n    \"\"\"Delete a task.\"\"\"\n    celery_app.control.revoke(task_id, terminate=True)\n    return\n</code></pre>"},{"location":"api_reference/routers/utils/#pixano_inference.routers.utils.execute_task_request","title":"<code>execute_task_request(request, task, settings)</code>  <code>async</code>","text":"<p>Execute a task request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>BaseRequest</code> <p>Request to execute.</p> required <code>task</code> <code>Task</code> <p>Task to execute</p> required <code>settings</code> <code>Settings</code> <p>Settings of the app.</p> required <p>Returns:</p> Type Description <code>CeleryTask</code> <p>Response of the request</p> Source code in <code>pixano_inference/routers/utils.py</code> <pre><code>async def execute_task_request(request: BaseRequest, task: Task, settings: Settings) -&gt; CeleryTask:\n    \"\"\"Execute a task request.\n\n    Args:\n        request: Request to execute.\n        task: Task to execute\n        settings: Settings of the app.\n\n    Returns:\n        Response of the request\n    \"\"\"\n    model_name = request.model\n    if model_name not in settings.models:\n        raise HTTPException(404, detail=f\"Model {model_name} is not registered.\")\n    model_task = settings.models_to_task[model_name]\n    if task.value != model_task:\n        raise HTTPException(400, detail=f\"Model {model_name} does not support the {task.value} task.\")\n\n    queue = model_queue_name(model_name)\n    celery_task: AsyncResult = predict.apply_async((jsonable_encoder(request),), queue=queue)\n    return CeleryTask(id=celery_task.id, status=states.PENDING)\n</code></pre>"},{"location":"api_reference/routers/utils/#pixano_inference.routers.utils.get_task_result","title":"<code>get_task_result(task_id, response_type)</code>  <code>async</code>","text":"<p>Get the result of a task.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>ID of the task to retrieve.</p> required <code>response_type</code> <code>type[BaseResponse]</code> <p>Type of response to return.</p> required <p>Returns:</p> Type Description <code>CeleryTask | BaseResponse</code> <p>Response for the task generation.</p> Source code in <code>pixano_inference/routers/utils.py</code> <pre><code>async def get_task_result(task_id: str, response_type: type[BaseResponse]) -&gt; CeleryTask | BaseResponse:\n    \"\"\"Get the result of a task.\n\n    Args:\n        task_id: ID of the task to retrieve.\n        response_type: Type of response to return.\n\n    Returns:\n        Response for the task generation.\n    \"\"\"\n    task_result = celery_app.AsyncResult(task_id)\n    status, result = task_result.status, task_result.result\n    if status in [states.PENDING, states.REVOKED, states.STARTED, states.FAILURE, states.REVOKED]:\n        return CeleryTask(id=task_id, status=status)\n    elif status != states.SUCCESS:\n        raise HTTPException(status_code=500, detail=f\"Unknown task status {status}\")\n    result[\"id\"] = task_id\n    result[\"status\"] = status\n    response = response_type.model_construct(**result)\n    task_result.forget()\n    return response\n</code></pre>"},{"location":"api_reference/routers/tasks/image/","title":"image","text":""},{"location":"api_reference/routers/tasks/image/#pixano_inference.routers.tasks.image","title":"<code>pixano_inference.routers.tasks.image</code>","text":"<p>API routes for Image tasks.</p>"},{"location":"api_reference/routers/tasks/image/#pixano_inference.routers.tasks.image.delete_mask_generation","title":"<code>delete_mask_generation(task_id)</code>  <code>async</code>","text":"<p>Delete a image mask generation task.</p> Source code in <code>pixano_inference/routers/tasks/image.py</code> <pre><code>@router.delete(\"/mask_generation/{task_id}\", response_model=None)\nasync def delete_mask_generation(task_id: str) -&gt; None:\n    \"\"\"Delete a image mask generation task.\"\"\"\n    return await delete_task(task_id=task_id)\n</code></pre>"},{"location":"api_reference/routers/tasks/image/#pixano_inference.routers.tasks.image.delete_zero_shot_detection","title":"<code>delete_zero_shot_detection(task_id)</code>  <code>async</code>","text":"<p>Delete a image zero shot detection task.</p> Source code in <code>pixano_inference/routers/tasks/image.py</code> <pre><code>@router.delete(\"/zero_shot_detection/{task_id}\", response_model=None)\nasync def delete_zero_shot_detection(task_id: str) -&gt; None:\n    \"\"\"Delete a image zero shot detection task.\"\"\"\n    return await delete_task(task_id=task_id)\n</code></pre>"},{"location":"api_reference/routers/tasks/image/#pixano_inference.routers.tasks.image.get_mask_generation","title":"<code>get_mask_generation(task_id)</code>  <code>async</code>","text":"<p>Get the result of a image mask generation task.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>ID of the task to retrieve.</p> required <p>Returns:</p> Type Description <code>ImageMaskGenerationResponse | CeleryTask</code> <p>Response for image mask generation.</p> Source code in <code>pixano_inference/routers/tasks/image.py</code> <pre><code>@router.get(\"/mask_generation/{task_id}\", response_model=ImageMaskGenerationResponse | CeleryTask)\nasync def get_mask_generation(task_id: str) -&gt; ImageMaskGenerationResponse | CeleryTask:\n    \"\"\"Get the result of a image mask generation task.\n\n    Args:\n        task_id: ID of the task to retrieve.\n\n    Returns:\n        Response for image mask generation.\n    \"\"\"\n    return await get_task_result(task_id, response_type=ImageMaskGenerationResponse)\n</code></pre>"},{"location":"api_reference/routers/tasks/image/#pixano_inference.routers.tasks.image.get_zero_shot_detection","title":"<code>get_zero_shot_detection(task_id)</code>  <code>async</code>","text":"<p>Get the result of a image zero shot detection task.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>ID of the task to retrieve.</p> required <p>Returns:</p> Type Description <code>ImageZeroShotDetectionResponse | CeleryTask</code> <p>Response for image zero shot detection.</p> Source code in <code>pixano_inference/routers/tasks/image.py</code> <pre><code>@router.get(\"/zero_shot_detection/{task_id}\", response_model=ImageZeroShotDetectionResponse | CeleryTask)\nasync def get_zero_shot_detection(task_id: str) -&gt; ImageZeroShotDetectionResponse | CeleryTask:\n    \"\"\"Get the result of a image zero shot detection task.\n\n    Args:\n        task_id: ID of the task to retrieve.\n\n    Returns:\n        Response for image zero shot detection.\n    \"\"\"\n    return await get_task_result(task_id, response_type=ImageZeroShotDetectionResponse)\n</code></pre>"},{"location":"api_reference/routers/tasks/image/#pixano_inference.routers.tasks.image.mask_generation","title":"<code>mask_generation(request, settings)</code>  <code>async</code>","text":"<p>Generate mask from an image and optionnaly points and bboxes.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ImageMaskGenerationRequest</code> <p>Request for image mask generation.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_pixano_inference_settings)]</code> <p>Settings of the app.</p> required <p>Returns:</p> Type Description <code>CeleryTask</code> <p>Id and status of the task.</p> Source code in <code>pixano_inference/routers/tasks/image.py</code> <pre><code>@router.post(\"/mask_generation/\", response_model=CeleryTask)\nasync def mask_generation(\n    request: ImageMaskGenerationRequest,\n    settings: Annotated[Settings, Depends(get_pixano_inference_settings)],\n) -&gt; CeleryTask:\n    \"\"\"Generate mask from an image and optionnaly points and bboxes.\n\n    Args:\n        request: Request for image mask generation.\n        settings: Settings of the app.\n\n    Returns:\n        Id and status of the task.\n    \"\"\"\n    return await execute_task_request(request=request, task=ImageTask.MASK_GENERATION, settings=settings)\n</code></pre>"},{"location":"api_reference/routers/tasks/image/#pixano_inference.routers.tasks.image.zero_shot_detection","title":"<code>zero_shot_detection(request, settings)</code>  <code>async</code>","text":"<p>Perform zero shot detection on an image.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ImageZeroShotDetectionRequest</code> <p>Request for image zero shot detection.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_pixano_inference_settings)]</code> <p>Settings of the app.</p> required <p>Returns:</p> Type Description <code>CeleryTask</code> <p>Id and status of the task.</p> Source code in <code>pixano_inference/routers/tasks/image.py</code> <pre><code>@router.post(\"/zero_shot_detection/\", response_model=CeleryTask)\nasync def zero_shot_detection(\n    request: ImageZeroShotDetectionRequest,\n    settings: Annotated[Settings, Depends(get_pixano_inference_settings)],\n) -&gt; CeleryTask:\n    \"\"\"Perform zero shot detection on an image.\n\n    Args:\n        request: Request for image zero shot detection.\n        settings: Settings of the app.\n\n    Returns:\n        Id and status of the task.\n    \"\"\"\n    return await execute_task_request(request=request, task=ImageTask.ZERO_SHOT_DETECTION, settings=settings)\n</code></pre>"},{"location":"api_reference/routers/tasks/multimodal/","title":"multimodal","text":""},{"location":"api_reference/routers/tasks/multimodal/#pixano_inference.routers.tasks.multimodal","title":"<code>pixano_inference.routers.tasks.multimodal</code>","text":"<p>API routes for NLP tasks.</p>"},{"location":"api_reference/routers/tasks/multimodal/#pixano_inference.routers.tasks.multimodal.delete_text_image_conditional_generation","title":"<code>delete_text_image_conditional_generation(task_id)</code>  <code>async</code>","text":"<p>Delete a text image conditional generation task.</p> Source code in <code>pixano_inference/routers/tasks/multimodal.py</code> <pre><code>@router.delete(\"/text-image/conditional_generation/{task_id}\", response_model=None)\nasync def delete_text_image_conditional_generation(task_id: str) -&gt; None:\n    \"\"\"Delete a text image conditional generation task.\"\"\"\n    return await delete_task(task_id=task_id)\n</code></pre>"},{"location":"api_reference/routers/tasks/multimodal/#pixano_inference.routers.tasks.multimodal.get_text_image_conditional_generation","title":"<code>get_text_image_conditional_generation(task_id)</code>  <code>async</code>","text":"<p>Get the result of a text image conditional generation task.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>ID of the task to retrieve.</p> required <p>Returns:</p> Type Description <code>TextImageConditionalGenerationResponse | CeleryTask</code> <p>Response for text image conditional generation.</p> Source code in <code>pixano_inference/routers/tasks/multimodal.py</code> <pre><code>@router.get(\n    \"/text-image/conditional_generation/{task_id}\", response_model=TextImageConditionalGenerationResponse | CeleryTask\n)\nasync def get_text_image_conditional_generation(task_id: str) -&gt; TextImageConditionalGenerationResponse | CeleryTask:\n    \"\"\"Get the result of a text image conditional generation task.\n\n    Args:\n        task_id: ID of the task to retrieve.\n\n    Returns:\n        Response for text image conditional generation.\n    \"\"\"\n    result = await get_task_result(task_id, response_type=TextImageConditionalGenerationResponse)\n    return result\n</code></pre>"},{"location":"api_reference/routers/tasks/multimodal/#pixano_inference.routers.tasks.multimodal.text_image_conditional_generation","title":"<code>text_image_conditional_generation(request, settings)</code>  <code>async</code>","text":"<p>Generate text from an image and a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>TextImageConditionalGenerationRequest</code> <p>Request for text-image conditional generation.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_pixano_inference_settings)]</code> <p>Settings of the app.</p> required <p>Returns:</p> Type Description <code>CeleryTask</code> <p>Response for text-image conditional generation.</p> Source code in <code>pixano_inference/routers/tasks/multimodal.py</code> <pre><code>@router.post(\"/text-image/conditional_generation/\", response_model=CeleryTask)\nasync def text_image_conditional_generation(\n    request: TextImageConditionalGenerationRequest,\n    settings: Annotated[Settings, Depends(get_pixano_inference_settings)],\n) -&gt; CeleryTask:\n    \"\"\"Generate text from an image and a prompt.\n\n    Args:\n        request: Request for text-image conditional generation.\n        settings: Settings of the app.\n\n    Returns:\n        Response for text-image conditional generation.\n    \"\"\"\n    return await execute_task_request(\n        request=request, task=MultimodalImageNLPTask.CONDITIONAL_GENERATION, settings=settings\n    )\n</code></pre>"},{"location":"api_reference/routers/tasks/nlp/","title":"nlp","text":""},{"location":"api_reference/routers/tasks/nlp/#pixano_inference.routers.tasks.nlp","title":"<code>pixano_inference.routers.tasks.nlp</code>","text":"<p>API routes for NLP tasks.</p>"},{"location":"api_reference/routers/tasks/tasks/","title":"tasks","text":""},{"location":"api_reference/routers/tasks/tasks/#pixano_inference.routers.tasks.tasks","title":"<code>pixano_inference.routers.tasks.tasks</code>","text":"<p>API routes for Image tasks.</p>"},{"location":"api_reference/routers/tasks/video/","title":"video","text":""},{"location":"api_reference/routers/tasks/video/#pixano_inference.routers.tasks.video","title":"<code>pixano_inference.routers.tasks.video</code>","text":"<p>API routes for Image tasks.</p>"},{"location":"api_reference/routers/tasks/video/#pixano_inference.routers.tasks.video.mask_generation","title":"<code>mask_generation(request, settings)</code>  <code>async</code>","text":"<p>Generate mask from a video and optionnaly points and bboxes.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>VideoMaskGenerationRequest</code> <p>Request for mask generation.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_pixano_inference_settings)]</code> <p>Settings of the app.</p> required <p>Returns:</p> Type Description <code>CeleryTask</code> <p>Response for mask generation.</p> Source code in <code>pixano_inference/routers/tasks/video.py</code> <pre><code>@router.post(\"/mask_generation\", response_model=CeleryTask)\nasync def mask_generation(\n    request: VideoMaskGenerationRequest,\n    settings: Annotated[Settings, Depends(get_pixano_inference_settings)],\n) -&gt; CeleryTask:\n    \"\"\"Generate mask from a video and optionnaly points and bboxes.\n\n    Args:\n        request: Request for mask generation.\n        settings: Settings of the app.\n\n    Returns:\n        Response for mask generation.\n    \"\"\"\n    return await execute_task_request(request=request, task=VideoTask.MASK_GENERATION, settings=settings)\n</code></pre>"},{"location":"api_reference/tasks/image/","title":"image","text":""},{"location":"api_reference/tasks/image/#pixano_inference.tasks.image","title":"<code>pixano_inference.tasks.image</code>","text":"<p>Image tasks.</p>"},{"location":"api_reference/tasks/image/#pixano_inference.tasks.image.ImageTask","title":"<code>ImageTask</code>","text":"<p>               Bases: <code>Task</code></p> <p>Image tasks.</p>"},{"location":"api_reference/tasks/multimodal/","title":"multimodal","text":""},{"location":"api_reference/tasks/multimodal/#pixano_inference.tasks.multimodal","title":"<code>pixano_inference.tasks.multimodal</code>","text":"<p>Multimodal tasks.</p>"},{"location":"api_reference/tasks/multimodal/#pixano_inference.tasks.multimodal.MultimodalImageNLPTask","title":"<code>MultimodalImageNLPTask</code>","text":"<p>               Bases: <code>Task</code></p> <p>Multimodal tasks.</p>"},{"location":"api_reference/tasks/nlp/","title":"nlp","text":""},{"location":"api_reference/tasks/nlp/#pixano_inference.tasks.nlp","title":"<code>pixano_inference.tasks.nlp</code>","text":"<p>Natural Language Processing tasks.</p>"},{"location":"api_reference/tasks/nlp/#pixano_inference.tasks.nlp.NLPTask","title":"<code>NLPTask</code>","text":"<p>               Bases: <code>Task</code></p> <p>Natural Language Processing tasks.</p>"},{"location":"api_reference/tasks/task/","title":"task","text":""},{"location":"api_reference/tasks/task/#pixano_inference.tasks.task","title":"<code>pixano_inference.tasks.task</code>","text":"<p>Tasks module.</p>"},{"location":"api_reference/tasks/task/#pixano_inference.tasks.task.Task","title":"<code>Task</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Task base class.</p>"},{"location":"api_reference/tasks/utils/","title":"utils","text":""},{"location":"api_reference/tasks/utils/#pixano_inference.tasks.utils","title":"<code>pixano_inference.tasks.utils</code>","text":"<p>Task utilities.</p>"},{"location":"api_reference/tasks/utils/#pixano_inference.tasks.utils.get_tasks","title":"<code>get_tasks()</code>","text":"<p>Get all tasks.</p> Source code in <code>pixano_inference/tasks/utils.py</code> <pre><code>def get_tasks() -&gt; set[str]:\n    \"\"\"Get all tasks.\"\"\"\n    return {task for set in [STR_IMAGE_TASKS, STR_NLP_TASKS, STR_MULTIMODAL_TASKS, STR_VIDEO_TASKS] for task in set}\n</code></pre>"},{"location":"api_reference/tasks/utils/#pixano_inference.tasks.utils.is_task","title":"<code>is_task(task)</code>","text":"<p>Check if a task is valid.</p> Source code in <code>pixano_inference/tasks/utils.py</code> <pre><code>def is_task(task: str) -&gt; bool:\n    \"\"\"Check if a task is valid.\"\"\"\n    return task in get_tasks()\n</code></pre>"},{"location":"api_reference/tasks/utils/#pixano_inference.tasks.utils.str_to_task","title":"<code>str_to_task(task)</code>","text":"<p>Convert a task string to its task.</p> Source code in <code>pixano_inference/tasks/utils.py</code> <pre><code>def str_to_task(task: str) -&gt; Task:\n    \"\"\"Convert a task string to its task.\"\"\"\n    if task in STR_IMAGE_TASKS:\n        return ImageTask(task)\n    elif task in STR_NLP_TASKS:\n        return NLPTask(task)\n    elif task in STR_MULTIMODAL_TASKS:\n        return MultimodalImageNLPTask(task)\n    elif task in STR_VIDEO_TASKS:\n        return VideoTask(task)\n    raise ValueError(f\"Invalid task '{task}'\")\n</code></pre>"},{"location":"api_reference/tasks/video/","title":"video","text":""},{"location":"api_reference/tasks/video/#pixano_inference.tasks.video","title":"<code>pixano_inference.tasks.video</code>","text":"<p>Video tasks.</p>"},{"location":"api_reference/tasks/video/#pixano_inference.tasks.video.VideoTask","title":"<code>VideoTask</code>","text":"<p>               Bases: <code>Task</code></p> <p>Video tasks.</p>"},{"location":"api_reference/utils/media/","title":"media","text":""},{"location":"api_reference/utils/media/#pixano_inference.utils.media","title":"<code>pixano_inference.utils.media</code>","text":"<p>Image utilities.</p>"},{"location":"api_reference/utils/media/#pixano_inference.utils.media.compress_rle","title":"<code>compress_rle(rle)</code>","text":"<p>Compress an RLE encoded mask.</p> <p>Parameters:</p> Name Type Description Default <code>rle</code> <code>dict[str, Any]</code> <p>RLE encoded mask as a dictionary.</p> required <p>Returns:</p> Type Description <code>dict[str, Any | str]</code> <p>Compressed RLE encoded mask as a string.</p> Source code in <code>pixano_inference/utils/media.py</code> <pre><code>def compress_rle(rle: dict[str, Any]) -&gt; dict[str, Any | str]:\n    \"\"\"Compress an RLE encoded mask.\n\n    Args:\n        rle: RLE encoded mask as a dictionary.\n\n    Returns:\n        Compressed RLE encoded mask as a string.\n    \"\"\"\n    counts = np.array(rle[\"counts\"], dtype=np.uint32).tobytes()\n    rle[\"counts\"] = base64.b64encode(counts).decode(\"utf-8\")\n    return rle\n</code></pre>"},{"location":"api_reference/utils/media/#pixano_inference.utils.media.convert_image_pil_to_tensor","title":"<code>convert_image_pil_to_tensor(image, device, size=None)</code>","text":"<p>Convert an image in PIL format to a PyTorch tensor and optionally resize it.</p> Source code in <code>pixano_inference/utils/media.py</code> <pre><code>def convert_image_pil_to_tensor(image: Image, device: \"torch.device\", size: int | None = None) -&gt; \"Tensor\":\n    \"\"\"Convert an image in PIL format to a PyTorch tensor and optionally resize it.\"\"\"\n    assert_torch_installed()\n    image = image.convert(\"RGB\")\n    if size is not None:\n        image = image.resize((size, size))\n    image_np = np.array(image) / 255.0\n    image = torch.from_numpy(image_np).to(device=device).permute(2, 0, 1)\n    return image\n</code></pre>"},{"location":"api_reference/utils/media/#pixano_inference.utils.media.convert_string_to_image","title":"<code>convert_string_to_image(str_image)</code>","text":"<p>Convert a string or path to an image.</p> <p>Parameters:</p> Name Type Description Default <code>str_image</code> <code>str | Path</code> <p>Image as a string or path.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>Image.</p> Source code in <code>pixano_inference/utils/media.py</code> <pre><code>def convert_string_to_image(str_image: str | Path) -&gt; Image.Image:\n    \"\"\"Convert a string or path to an image.\n\n    Args:\n        str_image: Image as a string or path.\n\n    Returns:\n        Image.\n    \"\"\"\n    if isinstance(str_image, str):\n        if is_url(str_image):\n            image_pil = Image.open(requests.get(str_image, stream=True).raw)\n        else:\n            if is_base64_image(str_image):\n                image_bytes = base64.b64decode(extract_media_from_base64(str_image))\n                image_pil = Image.open(BytesIO(image_bytes))\n            elif Path(str_image).exists():\n                image_pil = Image.open(str_image)\n            else:\n                raise ValueError(\"The image is not a valid path, URL or base64 string.\")\n    elif isinstance(str_image, Path):\n        image_pil = Image.open(str_image)\n    else:\n        raise ValueError(\"The image is not a valid path, URL or base64 string.\")\n    image_converted = image_pil.convert(\"RGB\")\n    return image_converted\n</code></pre>"},{"location":"api_reference/utils/media/#pixano_inference.utils.media.convert_string_video_to_bytes_or_path","title":"<code>convert_string_video_to_bytes_or_path(str_video)</code>","text":"<p>Convert a string to a video or video path.</p> <p>Parameters:</p> Name Type Description Default <code>str_video</code> <code>str | Path</code> <p>Video as a string or path.</p> required <p>Returns:</p> Type Description <code>bytes | Path</code> <p>The video.</p> Source code in <code>pixano_inference/utils/media.py</code> <pre><code>def convert_string_video_to_bytes_or_path(str_video: str | Path) -&gt; bytes | Path:\n    \"\"\"Convert a string to a video or video path.\n\n    Args:\n        str_video: Video as a string or path.\n\n    Returns:\n        The video.\n    \"\"\"\n    if isinstance(str_video, str):\n        if is_url(str_video):\n            video_bytes = requests.get(str_video, stream=True).raw\n        else:\n            if is_base64_video(str_video):\n                video_bytes = base64.b64decode(extract_media_from_base64(str_video))\n            elif Path(str_video).exists():\n                return Path(str_video)\n            else:\n                raise ValueError(\"The image is not a valid path, URL or base64 string.\")\n        return video_bytes\n    elif isinstance(str_video, Path):\n        return str_video\n    else:\n        raise ValueError(\"The image is not a valid path, URL or base64 string.\")\n</code></pre>"},{"location":"api_reference/utils/media/#pixano_inference.utils.media.decode_rle_to_mask","title":"<code>decode_rle_to_mask(rle)</code>","text":"<p>Decode an RLE encoded mask.</p> <p>Parameters:</p> Name Type Description Default <code>rle</code> <code>dict</code> <p>RLE encoded mask as a dictionary.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Decoded binary mask of shape (height, width).</p> Source code in <code>pixano_inference/utils/media.py</code> <pre><code>def decode_rle_to_mask(rle: dict) -&gt; np.ndarray:\n    \"\"\"Decode an RLE encoded mask.\n\n    Args:\n        rle: RLE encoded mask as a dictionary.\n\n    Returns:\n        Decoded binary mask of shape (height, width).\n    \"\"\"\n    height, width = rle[\"size\"]\n    mask = np.empty(height * width, dtype=bool)\n    idx = 0\n    parity = False\n    for count in rle[\"counts\"]:\n        mask[idx : idx + count] = parity\n        idx += count\n        parity = not parity\n    mask = mask.reshape(width, height)\n    return mask.transpose()  # Reshape to original shape\n</code></pre>"},{"location":"api_reference/utils/media/#pixano_inference.utils.media.decompress_rle","title":"<code>decompress_rle(rle)</code>","text":"<p>Decompress a compressed RLE encoded mask.</p> <p>Parameters:</p> Name Type Description Default <code>rle</code> <code>dict[str, Any]</code> <p>Compressed RLE encoded mask as a string.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Decompressed RLE encoded mask as a dictionary.</p> Source code in <code>pixano_inference/utils/media.py</code> <pre><code>def decompress_rle(rle: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Decompress a compressed RLE encoded mask.\n\n    Args:\n        rle: Compressed RLE encoded mask as a string.\n\n    Returns:\n        Decompressed RLE encoded mask as a dictionary.\n    \"\"\"\n    rle[\"counts\"] = np.frombuffer(base64.b64decode(rle[\"counts\"]), dtype=np.uint32).tolist()\n    return rle\n</code></pre>"},{"location":"api_reference/utils/media/#pixano_inference.utils.media.encode_mask_to_rle","title":"<code>encode_mask_to_rle(mask)</code>","text":"<p>Encode a binary mask using RLE.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Tensor</code> <p>A binary mask of shape (height, width).</p> required <p>Returns:</p> Type Description <code>dict[str, list[int]]</code> <p>RLE encoded mask as a dictionary.</p> Source code in <code>pixano_inference/utils/media.py</code> <pre><code>def encode_mask_to_rle(mask: \"Tensor\") -&gt; dict[str, list[int]]:\n    \"\"\"Encode a binary mask using RLE.\n\n    Args:\n        mask: A binary mask of shape (height, width).\n\n    Returns:\n        RLE encoded mask as a dictionary.\n    \"\"\"\n    assert_torch_installed()\n    rle = {\"counts\": [], \"size\": list(mask.shape)}\n    mask = mask.permute(1, 0).flatten()\n    diff_arr = torch.diff(mask)\n    nonzero_indices = torch.where(diff_arr != 0)[0] + 1\n    lengths = torch.diff(torch.concatenate((torch.tensor([0]), nonzero_indices, torch.tensor([len(mask)]))))\n\n    # note that the odd counts are always the numbers of zeros\n    if mask[0] == 1:\n        lengths = torch.concatenate(([0], lengths))\n\n    rle[\"counts\"] = lengths.tolist()\n\n    return rle\n</code></pre>"},{"location":"api_reference/utils/media/#pixano_inference.utils.media.extract_media_from_base64","title":"<code>extract_media_from_base64(string)</code>","text":"<p>Extract from a base64 media the actual base64 part.</p> Source code in <code>pixano_inference/utils/media.py</code> <pre><code>def extract_media_from_base64(string: str) -&gt; str:\n    \"\"\"Extract from a base64 media the actual base64 part.\"\"\"\n    match = match_base64_media(string)\n    if match is None:\n        raise ValueError(\"The string does not match the expected format.\")\n    return string[len(match.group(1)) :]\n</code></pre>"},{"location":"api_reference/utils/media/#pixano_inference.utils.media.is_base64_image","title":"<code>is_base64_image(string)</code>","text":"<p>Check if a string is a base64 image.</p> <p>The expected format is \"data:image/{image_format};base64,{base64}\".</p> Source code in <code>pixano_inference/utils/media.py</code> <pre><code>def is_base64_image(string: str) -&gt; bool:\n    \"\"\"Check if a string is a base64 image.\n\n    The expected format is \"data:image/{image_format};base64,{base64}\".\n    \"\"\"\n    return is_base64_media(string, \"image\")\n</code></pre>"},{"location":"api_reference/utils/media/#pixano_inference.utils.media.is_base64_media","title":"<code>is_base64_media(string, media)</code>","text":"<p>Check if a string is a base64 media.</p> <p>The expected format is \"data:{media}/{image_format};base64,{base64}\".</p> Source code in <code>pixano_inference/utils/media.py</code> <pre><code>def is_base64_media(string: str, media: str | None) -&gt; bool:\n    \"\"\"Check if a string is a base64 media.\n\n    The expected format is \"data:{media}/{image_format};base64,{base64}\".\n    \"\"\"\n    return match_base64_media(string, media) is not None\n</code></pre>"},{"location":"api_reference/utils/media/#pixano_inference.utils.media.is_base64_video","title":"<code>is_base64_video(string)</code>","text":"<p>Check if a string is a base64 video.</p> <p>The expected format is \"data:video/{video_format};base64,{base64}\".</p> Source code in <code>pixano_inference/utils/media.py</code> <pre><code>def is_base64_video(string: str) -&gt; bool:\n    \"\"\"Check if a string is a base64 video.\n\n    The expected format is \"data:video/{video_format};base64,{base64}\".\n    \"\"\"\n    return is_base64_media(string, \"video\")\n</code></pre>"},{"location":"api_reference/utils/media/#pixano_inference.utils.media.match_base64_media","title":"<code>match_base64_media(string, media=None)</code>","text":"<p>Match a base64 media.</p> Source code in <code>pixano_inference/utils/media.py</code> <pre><code>def match_base64_media(string: str, media: str | None = None) -&gt; re.Match[str] | None:\n    \"\"\"Match a base64 media.\"\"\"\n    regex_media_base64 = rf\"^(data:{media if media is not None else '[a-zA-Z]'}/[a-zA-Z]+;base64,)\"\n    return re.match(regex_media_base64, string)\n</code></pre>"},{"location":"api_reference/utils/package/","title":"package","text":""},{"location":"api_reference/utils/package/#pixano_inference.utils.package","title":"<code>pixano_inference.utils.package</code>","text":"<p>Utility functions for working with Python packages.</p>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.assert_lance_installed","title":"<code>assert_lance_installed()</code>","text":"<p>Assert that the lance package is installed.</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def assert_lance_installed() -&gt; None:\n    \"\"\"Assert that the lance package is installed.\"\"\"\n    assert_package_installed(\n        \"lance\", \"lance is not installed. Please install it using 'pip install pixano-inference[data]'.\"\n    )\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.assert_package_installed","title":"<code>assert_package_installed(package_name, error_message=None)</code>","text":"<p>Assert that a Python package is installed.</p> <p>Parameters:</p> Name Type Description Default <code>package_name</code> <code>str</code> <p>The name of the package to check.</p> required <code>error_message</code> <code>str | None</code> <p>The error message to raise if the package is not installed.</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the package is not installed</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def assert_package_installed(package_name: str, error_message: str | None = None) -&gt; None:\n    \"\"\"Assert that a Python package is installed.\n\n    Args:\n        package_name: The name of the package to check.\n        error_message: The error message to raise if the package is not installed.\n\n    Raises:\n        ImportError: If the package is not installed\n    \"\"\"\n    if not is_package_installed(package_name):\n        message = error_message or f\"Package '{package_name}' is not installed.\"\n        raise ImportError(message)\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.assert_sam2_installed","title":"<code>assert_sam2_installed()</code>","text":"<p>Assert that the sam2 package is installed.</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def assert_sam2_installed() -&gt; None:\n    \"\"\"Assert that the sam2 package is installed.\"\"\"\n    assert_package_installed(\n        \"sam2\",\n        \"sam2 is not installed. Please install it using 'pip install git+https://github.com/facebookresearch/sam2.git'.\",\n    )\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.assert_torch_installed","title":"<code>assert_torch_installed()</code>","text":"<p>Assert that the torch package is installed.</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def assert_torch_installed() -&gt; None:\n    \"\"\"Assert that the torch package is installed.\"\"\"\n    assert_package_installed(\n        \"torch\", \"torch is not installed. Please install it using 'pip install pixano-inference[torch]'.\"\n    )\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.assert_transformers_installed","title":"<code>assert_transformers_installed()</code>","text":"<p>Assert that the transformers package is installed.</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def assert_transformers_installed() -&gt; None:\n    \"\"\"Assert that the transformers package is installed.\"\"\"\n    assert_package_installed(\n        \"transformers\",\n        \"transformers is not installed. Please install it using 'pip install pixano-inference[transformers]'.\",\n    )\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.assert_vllm_installed","title":"<code>assert_vllm_installed()</code>","text":"<p>Assert that the vllm package is installed.</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def assert_vllm_installed() -&gt; None:\n    \"\"\"Assert that the vllm package is installed.\"\"\"\n    assert_package_installed(\n        \"vllm\", \"vLLM is not installed. Please install it using 'pip install pixano-inference[vllm]'.\"\n    )\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.is_lance_installed","title":"<code>is_lance_installed()</code>","text":"<p>Check if the lance package is installed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the lance package is installed, False otherwise</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def is_lance_installed() -&gt; bool:\n    \"\"\"Check if the lance package is installed.\n\n    Returns:\n        True if the lance package is installed, False otherwise\n    \"\"\"\n    return is_package_installed(\"lance\")\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.is_package_installed","title":"<code>is_package_installed(package_name)</code>","text":"<p>Check if a Python package is installed.</p> <p>Parameters:</p> Name Type Description Default <code>package_name</code> <code>str</code> <p>The name of the package to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the package is installed, False otherwise</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def is_package_installed(package_name: str) -&gt; bool:\n    \"\"\"Check if a Python package is installed.\n\n    Args:\n        package_name: The name of the package to check.\n\n    Returns:\n        True if the package is installed, False otherwise\n    \"\"\"\n    package_spec = importlib.util.find_spec(package_name)\n    return package_spec is not None\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.is_sam2_installed","title":"<code>is_sam2_installed()</code>","text":"<p>Check if the sam2 package is installed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the sam2 package is installed, False otherwise</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def is_sam2_installed() -&gt; bool:\n    \"\"\"Check if the sam2 package is installed.\n\n    Returns:\n        True if the sam2 package is installed, False otherwise\n    \"\"\"\n    return is_package_installed(\"sam2\")\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.is_torch_installed","title":"<code>is_torch_installed()</code>","text":"<p>Check if the torch package is installed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the torch package is installed, False otherwise</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def is_torch_installed() -&gt; bool:\n    \"\"\"Check if the torch package is installed.\n\n    Returns:\n        True if the torch package is installed, False otherwise\n    \"\"\"\n    return is_package_installed(\"torch\")\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.is_transformers_installed","title":"<code>is_transformers_installed()</code>","text":"<p>Check if the transformers package is installed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the transformers package is installed, False otherwise</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def is_transformers_installed() -&gt; bool:\n    \"\"\"Check if the transformers package is installed.\n\n    Returns:\n        True if the transformers package is installed, False otherwise\n    \"\"\"\n    return is_package_installed(\"transformers\")\n</code></pre>"},{"location":"api_reference/utils/package/#pixano_inference.utils.package.is_vllm_installed","title":"<code>is_vllm_installed()</code>","text":"<p>Check if the vllm package is installed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the vLLM package is installed, False otherwise</p> Source code in <code>pixano_inference/utils/package.py</code> <pre><code>def is_vllm_installed() -&gt; bool:\n    \"\"\"Check if the vllm package is installed.\n\n    Returns:\n        True if the vLLM package is installed, False otherwise\n    \"\"\"\n    return is_package_installed(\"vllm\")\n</code></pre>"},{"location":"api_reference/utils/url/","title":"url","text":""},{"location":"api_reference/utils/url/#pixano_inference.utils.url","title":"<code>pixano_inference.utils.url</code>","text":"<p>URL utils.</p>"},{"location":"api_reference/utils/url/#pixano_inference.utils.url.is_url","title":"<code>is_url(url)</code>","text":"<p>Check if a string is a valid URL.</p> Source code in <code>pixano_inference/utils/url.py</code> <pre><code>def is_url(url: str) -&gt; bool:\n    \"\"\"Check if a string is a valid URL.\"\"\"\n    return re.match(url_validation_regex, url) is not None\n</code></pre>"},{"location":"api_reference/utils/vector/","title":"vector","text":""},{"location":"api_reference/utils/vector/#pixano_inference.utils.vector","title":"<code>pixano_inference.utils.vector</code>","text":"<p>Utility functions for vector operations.</p>"},{"location":"api_reference/utils/vector/#pixano_inference.utils.vector.vector_to_tensor","title":"<code>vector_to_tensor(vector)</code>","text":"<p>Convert a vector to a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>vector</code> <code>NDArrayFloat | LanceVector | 'Tensor' | None</code> <p>Vector to convert.</p> required Source code in <code>pixano_inference/utils/vector.py</code> <pre><code>def vector_to_tensor(vector: NDArrayFloat | LanceVector | \"Tensor\" | None) -&gt; \"Tensor\":\n    \"\"\"Convert a vector to a tensor.\n\n    Args:\n        vector: Vector to convert.\n    \"\"\"\n    assert_torch_installed()\n    if not isinstance(vector, (NDArrayFloat, LanceVector, Tensor)) and vector is not None:\n        raise ValueError(f\"Unsupported vector type: {type(vector)}\")\n\n    if isinstance(vector, LanceVector):\n        return vector.read_vector()\n    elif isinstance(vector, NDArrayFloat):\n        return vector.to_torch()\n    return vector\n</code></pre>"},{"location":"getting_started/","title":"Index","text":""},{"location":"getting_started/#getting-started-with-pixano-inference","title":"Getting started with Pixano-Inference","text":""},{"location":"getting_started/#installation","title":"Installation","text":"<p>To install the library, simply execute the following command</p> <pre><code>pip install pixano-inference\n</code></pre> <p>If you want to dynamically make changes to the library to develop and test, make a dev install by cloning the repo and executing the following commands</p> <pre><code>cd pixano-inference\npip install -e .\n</code></pre> <p>To have access to the providers available in Pixano-Inference, make sure to install the relevant dependencies. For example to use the vLLM provider run:</p> <pre><code>pip install pixano-inference[vllm]\n</code></pre>"},{"location":"getting_started/#usage","title":"Usage","text":""},{"location":"getting_started/#run-the-application","title":"Run the application","text":"<p>Pixano-Inference requires a running Redis server. Its URL can be configured in a <code>.env</code> file at the root of the server. By default it will serve the localhost at the 6379 port.</p> <p>Pixano-Inference can invoke a server that will serve the API. To do so, simply execute the following command:</p> <pre><code>pixano-inference --port 8000\n</code></pre> <p>The default port is <code>8000</code>. You can change it by passing the <code>--port</code> argument.</p>"},{"location":"getting_started/#instantiate-the-client","title":"Instantiate the client","text":"<p>The easiest way to interact with Pixano-Inference is through the Python client.</p> <pre><code>from pixano_inference.client import PixanoInferenceClient\n\n\nclient = PixanoInferenceClient.connect(url=\"http://localhost:8000\")\n</code></pre>"},{"location":"getting_started/#instantiate-a-model","title":"Instantiate a model","text":"<p>To instantiate a model, you need to provide the path of the model file and the task that the model will perform. For example to run a Llava model from the vLLM provider:</p> <pre><code>from pixano_inference.pydantic import ModelConfig\nfrom pixano_inference.tasks import MultimodalImageNLPTask\n\n\nawait client.instantiate_model(\n    provider=\"vllm\",\n    config=ModelConfig(\n        name=\"llava-qwen\",\n        task=MultimodalImageNLPTask.CONDITIONAL_GENERATION.value,\n        path=\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\",\n        config={\n            \"dtype\": \"bfloat16\",\n        },\n    ),\n)\n</code></pre>"},{"location":"getting_started/#run-an-inference","title":"Run an inference","text":"<p>The client provide methods to run the different models on various tasks. For image-text conditional generation using Llava here is the relevant method:</p> <pre><code>from pixano_inference.pydantic import TextImageConditionalGenerationRequest, TextImageConditionalGenerationResponse\n\n\nrequest = TextImageConditionalGenerationRequest(\n    model=\"llava-qwen\",\n    prompt=[\n        {\n            \"content\": [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/9/9e/Ours_brun_parcanimalierpyrenees_1.jpg\"\n                    },\n                },\n                {\"type\": \"text\", \"text\": \"What is displayed in this image ? Answer concisely. \"},\n            ],\n            \"role\": \"user\",\n        }\n    ],\n    images=None,\n    max_new_tokens=100,\n)\n\nresponse: TextImageConditionalGenerationResponse = await client.text_image_conditional_generation(request)\nprint(response.data.generated_text)\n</code></pre>"},{"location":"home/","title":"Index","text":""},{"location":"home/#pixano-inference","title":"Pixano-Inference","text":""},{"location":"home/#overview","title":"Overview","text":"<p>Pixano-Inference is a Python library that provides an API for performing inference tasks on multi-modal data such as images, videos, and text. It includes model and API providers for various tasks, including mask generation, text-image conditional generation.</p> <p>Pixano-Inference aims to provide a simple and easy-to-use interface for developers to interact with models solving different tasks and from various providers from a common interface via its RESTful API.</p>"},{"location":"home/#how-to-use-pixano-inference","title":"How to use Pixano-Inference","text":"<p>Please refer to the Getting Started guide for instructions on how to install, configure, and use Pixano-Inference in your project.</p> <p>For going in-depth on how to use the APIs, please refer to the API Reference.</p>"},{"location":"home/#contributing","title":"Contributing","text":"<p>We welcome contributions from the community! Please open issues and pull requests for any bugs or feature requests you may have.</p>"},{"location":"home/#license","title":"License","text":"<p>Pixano-Inference is released under the terms of the CeCILL-C license.</p>"}]}