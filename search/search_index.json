{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api_reference/","title":"Pixano Inference API reference","text":"<p>Here you will find the documentation for all of our implemented models.</p> <ul> <li>The pytorch module contains models from the PyTorch library.</li> <li>The github module contains the Segment Anything Model (SAM) for Meta.</li> <li>The tensorflow module contains models from the TensorFlow library.</li> <li>The transformers module contains models from the Hugging Face Transformers library.</li> </ul>"},{"location":"api_reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>index</li> <li>github<ul> <li>groundingdino</li> <li>mobile_sam</li> <li>sam</li> </ul> </li> <li>pytorch<ul> <li>deeplabv3</li> <li>maskrcnnv2</li> <li>yolov5</li> </ul> </li> <li>tensorflow<ul> <li>efficientdet</li> <li>fasterrcnn</li> </ul> </li> <li>transformers<ul> <li>clip</li> </ul> </li> <li>utils<ul> <li>main</li> </ul> </li> </ul>"},{"location":"api_reference/github/groundingdino/","title":"groundingdino","text":""},{"location":"api_reference/github/groundingdino/#pixano_inference.github.groundingdino","title":"<code>pixano_inference.github.groundingdino</code>","text":""},{"location":"api_reference/github/groundingdino/#pixano_inference.github.groundingdino.GroundingDINO","title":"<code>GroundingDINO(checkpoint_path, config_path, model_id='', device='cuda')</code>","text":"<p>             Bases: <code>InferenceModel</code></p> <p>GroundingDINO Model</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Model name</p> <code>model_id</code> <code>str</code> <p>Model ID</p> <code>device</code> <code>str</code> <p>Model GPU or CPU device</p> <code>description</code> <code>str</code> <p>Model description</p> <code>model</code> <code>Module</code> <p>PyTorch model</p> <code>checkpoint_path</code> <code>Path</code> <p>Model checkpoint path</p> <code>config_path</code> <code>Path</code> <p>Model config path</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>Path</code> <p>Model checkpoint path (download from https://github.com/IDEA-Research/GroundingDINO)</p> required <code>config_path</code> <code>Path</code> <p>Model config path (download from https://github.com/IDEA-Research/GroundingDINO)</p> required <code>model_id</code> <code>str</code> <p>Previously used ID, generate new ID if \"\". Defaults to \"\".</p> <code>''</code> <code>device</code> <code>str</code> <p>Model GPU or CPU device (e.g. \"cuda\", \"cpu\"). Defaults to \"cuda\".</p> <code>'cuda'</code> Source code in <code>pixano_inference/github/groundingdino.py</code> <pre><code>def __init__(\n    self,\n    checkpoint_path: Path,\n    config_path: Path,\n    model_id: str = \"\",\n    device: str = \"cuda\",\n) -&gt; None:\n    \"\"\"Initialize model\n\n    Args:\n        checkpoint_path (Path): Model checkpoint path (download from https://github.com/IDEA-Research/GroundingDINO)\n        config_path (Path): Model config path (download from https://github.com/IDEA-Research/GroundingDINO)\n        model_id (str, optional): Previously used ID, generate new ID if \"\". Defaults to \"\".\n        device (str, optional): Model GPU or CPU device (e.g. \"cuda\", \"cpu\"). Defaults to \"cuda\".\n    \"\"\"\n\n    # Import GroundingDINO\n    gd_inf = attempt_import(\n        \"groundingdino.util.inference\",\n        \"groundingdino@git+https://github.com/IDEA-Research/GroundingDINO\",\n    )\n\n    super().__init__(\n        name=\"GroundingDINO\",\n        model_id=model_id,\n        device=device,\n        description=\"Fom GitHub, GroundingDINO model.\",\n    )\n\n    # Model\n    self.model = gd_inf.load_model(\n        config_path.as_posix(),\n        checkpoint_path.as_posix(),\n    )\n    self.model.to(self.device)\n</code></pre>"},{"location":"api_reference/github/groundingdino/#pixano_inference.github.groundingdino.GroundingDINO.preannotate","title":"<code>preannotate(batch, views, uri_prefix, threshold=0.0, prompt='')</code>","text":"<p>Inference pre-annotation for a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>RecordBatch</code> <p>Input batch</p> required <code>views</code> <code>list[str]</code> <p>Dataset views</p> required <code>uri_prefix</code> <code>str</code> <p>URI prefix for media files</p> required <code>threshold</code> <code>float</code> <p>Confidence threshold. Defaults to 0.0.</p> <code>0.0</code> <code>prompt</code> <code>str</code> <p>Annotation text prompt. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>Processed rows</p> Source code in <code>pixano_inference/github/groundingdino.py</code> <pre><code>def preannotate(\n    self,\n    batch: pa.RecordBatch,\n    views: list[str],\n    uri_prefix: str,\n    threshold: float = 0.0,\n    prompt: str = \"\",\n) -&gt; list[dict]:\n    \"\"\"Inference pre-annotation for a batch\n\n    Args:\n        batch (pa.RecordBatch): Input batch\n        views (list[str]): Dataset views\n        uri_prefix (str): URI prefix for media files\n        threshold (float, optional): Confidence threshold. Defaults to 0.0.\n        prompt (str, optional): Annotation text prompt. Defaults to \"\".\n\n    Returns:\n        list[dict]: Processed rows\n    \"\"\"\n\n    rows = []\n\n    # Import GroundingDINO\n    gd_inf = attempt_import(\n        \"groundingdino.util.inference\",\n        \"groundingdino@git+https://github.com/IDEA-Research/GroundingDINO\",\n    )\n\n    for view in views:\n        # Iterate manually\n        for x in range(batch.num_rows):\n            # Preprocess image\n            im: Image = Image.from_dict(batch[view][x].as_py())\n            im.uri_prefix = uri_prefix\n\n            _, image = gd_inf.load_image(im.path.as_posix())\n\n            # Inference\n            bbox_tensor, logit_tensor, category_list = gd_inf.predict(\n                model=self.model,\n                image=image,\n                caption=prompt,\n                box_threshold=0.35,\n                text_threshold=0.25,\n            )\n\n            # Convert bounding boxes from cyxcywh to xywh\n            bbox_tensor = box_convert(\n                boxes=bbox_tensor, in_fmt=\"cxcywh\", out_fmt=\"xywh\"\n            )\n            bbox_list = [[coord.item() for coord in bbox] for bbox in bbox_tensor]\n\n            # Process model outputs\n            rows.extend(\n                [\n                    {\n                        \"id\": shortuuid.uuid(),\n                        \"item_id\": batch[\"id\"][x].as_py(),\n                        \"view_id\": view,\n                        \"bbox\": BBox.from_xywh(\n                            bbox_list[i],\n                            confidence=logit_tensor[i].item(),\n                        ).to_dict(),\n                        \"category\": category_list[i],\n                    }\n                    for i in range(len(category_list))\n                    if logit_tensor[i].item() &gt; threshold\n                ]\n            )\n\n    return rows\n</code></pre>"},{"location":"api_reference/github/mobile_sam/","title":"mobile_sam","text":""},{"location":"api_reference/github/mobile_sam/#pixano_inference.github.mobile_sam","title":"<code>pixano_inference.github.mobile_sam</code>","text":""},{"location":"api_reference/github/mobile_sam/#pixano_inference.github.mobile_sam.MobileSAM","title":"<code>MobileSAM(checkpoint_path, model_id='', device='cpu')</code>","text":"<p>             Bases: <code>InferenceModel</code></p> <p>MobileSAM</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Model name</p> <code>model_id</code> <code>str</code> <p>Model ID</p> <code>device</code> <code>str</code> <p>Model GPU or CPU device (e.g. \"cuda\", \"cpu\")</p> <code>description</code> <code>str</code> <p>Model description</p> <code>model</code> <code>Module</code> <p>MobileSAM model</p> <code>checkpoint_path</code> <code>Path</code> <p>Model checkpoint path</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>Path</code> <p>Model checkpoint path.</p> required <code>model_id</code> <code>str</code> <p>Previously used ID, generate new ID if \"\". Defaults to \"\".</p> <code>''</code> <code>device</code> <code>str</code> <p>Model GPU or CPU device (e.g. \"cuda\", \"cpu\"). Defaults to \"cpu\".</p> <code>'cpu'</code> Source code in <code>pixano_inference/github/mobile_sam.py</code> <pre><code>def __init__(\n    self,\n    checkpoint_path: Path,\n    model_id: str = \"\",\n    device: str = \"cpu\",\n) -&gt; None:\n    \"\"\"Initialize model\n\n    Args:\n        checkpoint_path (Path): Model checkpoint path.\n        model_id (str, optional): Previously used ID, generate new ID if \"\". Defaults to \"\".\n        device (str, optional): Model GPU or CPU device (e.g. \"cuda\", \"cpu\"). Defaults to \"cpu\".\n    \"\"\"\n\n    # Import MobileSAM\n    mobile_sam = attempt_import(\n        \"mobile_sam\", \"mobile-sam@git+https://github.com/ChaoningZhang/MobileSAM\"\n    )\n\n    super().__init__(\n        name=\"Mobile_SAM\",\n        model_id=model_id,\n        device=device,\n        description=\"From GitHub. MobileSAM, ViT-T backbone.\",\n    )\n\n    # Model\n    self.model = mobile_sam.sam_model_registry[\"vit_t\"](checkpoint=checkpoint_path)\n    self.model.to(device=self.device)\n\n    # Model path\n    self.checkpoint_path = checkpoint_path\n</code></pre>"},{"location":"api_reference/github/mobile_sam/#pixano_inference.github.mobile_sam.MobileSAM.export_to_onnx","title":"<code>export_to_onnx(library_dir)</code>","text":"<p>Export Torch model to ONNX</p> <p>Parameters:</p> Name Type Description Default <code>library_dir</code> <code>Path</code> <p>Dataset library directory</p> required Source code in <code>pixano_inference/github/mobile_sam.py</code> <pre><code>def export_to_onnx(self, library_dir: Path):\n    \"\"\"Export Torch model to ONNX\n\n    Args:\n        library_dir (Path): Dataset library directory\n    \"\"\"\n\n    # Import MobileSAM\n    mobile_sam = attempt_import(\n        \"mobile_sam\", \"mobile-sam@git+https://github.com/ChaoningZhang/MobileSAM\"\n    )\n\n    # Model directory\n    model_dir = library_dir / \"models\"\n    model_dir.mkdir(parents=True, exist_ok=True)\n\n    # Put model to CPU for export\n    self.model.to(\"cpu\")\n\n    # Export settings\n    onnx_model = mobile_sam.utils.onnx.SamOnnxModel(\n        self.model, return_single_mask=True\n    )\n    dynamic_axes = {\n        \"point_coords\": {1: \"num_points\"},\n        \"point_labels\": {1: \"num_points\"},\n    }\n    embed_dim = self.model.prompt_encoder.embed_dim\n    embed_size = self.model.prompt_encoder.image_embedding_size\n    mask_input_size = [4 * x for x in embed_size]\n    dummy_inputs = {\n        \"image_embeddings\": torch.randn(\n            1, embed_dim, *embed_size, dtype=torch.float\n        ),\n        \"point_coords\": torch.randint(\n            low=0, high=1024, size=(1, 5, 2), dtype=torch.float\n        ),\n        \"point_labels\": torch.randint(\n            low=0, high=4, size=(1, 5), dtype=torch.float\n        ),\n        \"mask_input\": torch.randn(1, 1, *mask_input_size, dtype=torch.float),\n        \"has_mask_input\": torch.tensor([1], dtype=torch.float),\n        \"orig_im_size\": torch.tensor([1500, 2250], dtype=torch.float),\n    }\n    output_names = [\"masks\", \"iou_predictions\", \"low_res_masks\"]\n    onnx_path = model_dir / self.checkpoint_path.name.replace(\".pt\", \".onnx\")\n\n    # Export model\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        with open(onnx_path, \"wb\") as f:\n            torch.onnx.export(\n                onnx_model,\n                tuple(dummy_inputs.values()),\n                f,\n                export_params=True,\n                verbose=False,\n                opset_version=17,\n                do_constant_folding=True,\n                input_names=list(dummy_inputs.keys()),\n                output_names=output_names,\n                dynamic_axes=dynamic_axes,\n            )\n    # Quantize model\n    quantize_dynamic(\n        model_input=onnx_path,\n        model_output=onnx_path,\n        optimize_model=True,\n        per_channel=False,\n        reduce_range=False,\n        weight_type=QuantType.QUInt8,\n    )\n\n    # Put model back to device after export\n    self.model.to(self.device)\n</code></pre>"},{"location":"api_reference/github/mobile_sam/#pixano_inference.github.mobile_sam.MobileSAM.preannotate","title":"<code>preannotate(batch, views, uri_prefix, threshold=0.0, prompt='')</code>","text":"<p>Inference pre-annotation for a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>RecordBatch</code> <p>Input batch</p> required <code>views</code> <code>list[str]</code> <p>Dataset views</p> required <code>uri_prefix</code> <code>str</code> <p>URI prefix for media files</p> required <code>threshold</code> <code>float</code> <p>Confidence threshold. Defaults to 0.0.</p> <code>0.0</code> <code>prompt</code> <code>str</code> <p>Annotation text prompt. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>Processed rows</p> Source code in <code>pixano_inference/github/mobile_sam.py</code> <pre><code>def preannotate(\n    self,\n    batch: pa.RecordBatch,\n    views: list[str],\n    uri_prefix: str,\n    threshold: float = 0.0,\n    prompt: str = \"\",\n) -&gt; list[dict]:\n    \"\"\"Inference pre-annotation for a batch\n\n    Args:\n        batch (pa.RecordBatch): Input batch\n        views (list[str]): Dataset views\n        uri_prefix (str): URI prefix for media files\n        threshold (float, optional): Confidence threshold. Defaults to 0.0.\n        prompt (str, optional): Annotation text prompt. Defaults to \"\".\n\n    Returns:\n        list[dict]: Processed rows\n    \"\"\"\n\n    # Import MobileSAM\n    mobile_sam = attempt_import(\n        \"mobile_sam\", \"mobile-sam@git+https://github.com/ChaoningZhang/MobileSAM\"\n    )\n\n    rows = []\n    _ = prompt  # This model does not use prompts\n\n    for view in views:\n        # Iterate manually\n        for x in range(batch.num_rows):\n            # Preprocess image\n            im: Image = Image.from_dict(batch[view][x].as_py())\n            im.uri_prefix = uri_prefix\n            im = im.as_cv2()\n            im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n\n            # Inference\n            with torch.no_grad():\n                generator = mobile_sam.SamAutomaticMaskGenerator(self.model)\n                output = generator.generate(im)\n\n            # Process model outputs\n            h, w = im.shape[:2]\n            rows.extend(\n                [\n                    {\n                        \"id\": shortuuid.uuid(),\n                        \"item_id\": batch[\"id\"][x].as_py(),\n                        \"view_id\": view,\n                        \"bbox\": BBox.from_xywh(\n                            [int(coord) for coord in output[i][\"bbox\"]],\n                            confidence=float(output[i][\"predicted_iou\"]),\n                        )\n                        .normalize(h, w)\n                        .to_dict(),\n                        \"mask\": CompressedRLE.from_mask(\n                            output[i][\"segmentation\"]\n                        ).to_dict(),\n                    }\n                    for i in range(len(output))\n                    if output[i][\"predicted_iou\"] &gt; threshold\n                ]\n            )\n\n    return rows\n</code></pre>"},{"location":"api_reference/github/mobile_sam/#pixano_inference.github.mobile_sam.MobileSAM.precompute_embeddings","title":"<code>precompute_embeddings(batch, views, uri_prefix)</code>","text":"<p>Embedding precomputing for a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>RecordBatch</code> <p>Input batch</p> required <code>views</code> <code>list[str]</code> <p>Dataset views</p> required <code>uri_prefix</code> <code>str</code> <p>URI prefix for media files</p> required <p>Returns:</p> Type Description <code>RecordBatch</code> <p>Embedding rows</p> Source code in <code>pixano_inference/github/mobile_sam.py</code> <pre><code>def precompute_embeddings(\n    self,\n    batch: pa.RecordBatch,\n    views: list[str],\n    uri_prefix: str,\n) -&gt; list[dict]:\n    \"\"\"Embedding precomputing for a batch\n\n    Args:\n        batch (pa.RecordBatch): Input batch\n        views (list[str]): Dataset views\n        uri_prefix (str): URI prefix for media files\n\n    Returns:\n        pa.RecordBatch: Embedding rows\n    \"\"\"\n\n    # Import MobileSAM\n    mobile_sam = attempt_import(\n        \"mobile_sam\", \"mobile-sam@git+https://github.com/ChaoningZhang/MobileSAM\"\n    )\n\n    rows = [\n        {\n            \"id\": batch[\"id\"][x].as_py(),\n        }\n        for x in range(batch.num_rows)\n    ]\n\n    for view in views:\n        # Iterate manually\n        for x in range(batch.num_rows):\n            # Preprocess image\n            im: Image = Image.from_dict(batch[view][x].as_py())\n            im.uri_prefix = uri_prefix\n            im = im.as_cv2()\n            im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n\n            # Inference\n            with torch.no_grad():\n                predictor = mobile_sam.SamPredictor(self.model)\n                predictor.set_image(im)\n                img_embedding = predictor.get_image_embedding().cpu().numpy()\n\n            # Process model outputs\n            emb_bytes = BytesIO()\n            np.save(emb_bytes, img_embedding)\n            rows[x][view] = emb_bytes.getvalue()\n\n    return rows\n</code></pre>"},{"location":"api_reference/github/sam/","title":"sam","text":""},{"location":"api_reference/github/sam/#pixano_inference.github.sam","title":"<code>pixano_inference.github.sam</code>","text":""},{"location":"api_reference/github/sam/#pixano_inference.github.sam.SAM","title":"<code>SAM(checkpoint_path, size='h', model_id='', device='cuda')</code>","text":"<p>             Bases: <code>InferenceModel</code></p> <p>Segment Anything Model (SAM)</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Model name</p> <code>model_id</code> <code>str</code> <p>Model ID</p> <code>device</code> <code>str</code> <p>Model GPU or CPU device (e.g. \"cuda\", \"cpu\")</p> <code>description</code> <code>str</code> <p>Model description</p> <code>model</code> <code>Module</code> <p>SAM model</p> <code>checkpoint_path</code> <code>Path</code> <p>Model checkpoint path</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>Path</code> <p>Model checkpoint path.</p> required <code>size</code> <code>str</code> <p>Model size (\"b\", \"l\", \"h\"). Defaults to \"h\".</p> <code>'h'</code> <code>model_id</code> <code>str</code> <p>Previously used ID, generate new ID if \"\". Defaults to \"\".</p> <code>''</code> <code>device</code> <code>str</code> <p>Model GPU or CPU device (e.g. \"cuda\", \"cpu\"). Defaults to \"cuda\".</p> <code>'cuda'</code> Source code in <code>pixano_inference/github/sam.py</code> <pre><code>def __init__(\n    self,\n    checkpoint_path: Path,\n    size: str = \"h\",\n    model_id: str = \"\",\n    device: str = \"cuda\",\n) -&gt; None:\n    \"\"\"Initialize model\n\n    Args:\n        checkpoint_path (Path): Model checkpoint path.\n        size (str, optional): Model size (\"b\", \"l\", \"h\"). Defaults to \"h\".\n        model_id (str, optional): Previously used ID, generate new ID if \"\". Defaults to \"\".\n        device (str, optional): Model GPU or CPU device (e.g. \"cuda\", \"cpu\"). Defaults to \"cuda\".\n    \"\"\"\n\n    # Import SAM\n    segment_anything = attempt_import(\n        \"segment_anything\",\n        \"segment-anything@git+https://github.com/facebookresearch/segment-anything\",\n    )\n\n    super().__init__(\n        name=f\"SAM_ViT_{size.upper()}\",\n        model_id=model_id,\n        device=device,\n        description=f\"From GitHub. Segment Anything Model (SAM), ViT-{size.upper()} Backbone.\",\n    )\n\n    # Model\n    self.model = segment_anything.sam_model_registry[f\"vit_{size}\"](\n        checkpoint=checkpoint_path\n    )\n    self.model.to(device=self.device)\n\n    # Model path\n    self.checkpoint_path = checkpoint_path\n</code></pre>"},{"location":"api_reference/github/sam/#pixano_inference.github.sam.SAM.export_to_onnx","title":"<code>export_to_onnx(library_dir)</code>","text":"<p>Export Torch model to ONNX</p> <p>Parameters:</p> Name Type Description Default <code>library_dir</code> <code>Path</code> <p>Dataset library directory</p> required Source code in <code>pixano_inference/github/sam.py</code> <pre><code>def export_to_onnx(self, library_dir: Path):\n    \"\"\"Export Torch model to ONNX\n\n    Args:\n        library_dir (Path): Dataset library directory\n    \"\"\"\n\n    # Import SAM\n    segment_anything = attempt_import(\n        \"segment_anything\",\n        \"segment-anything@git+https://github.com/facebookresearch/segment-anything\",\n    )\n\n    # Model directory\n    model_dir = library_dir / \"models\"\n    model_dir.mkdir(parents=True, exist_ok=True)\n\n    # Put model to CPU for export\n    self.model.to(\"cpu\")\n\n    # Export settings\n    onnx_model = segment_anything.utils.onnx.SamOnnxModel(\n        self.model, return_single_mask=True\n    )\n    dynamic_axes = {\n        \"point_coords\": {1: \"num_points\"},\n        \"point_labels\": {1: \"num_points\"},\n    }\n    embed_dim = self.model.prompt_encoder.embed_dim\n    embed_size = self.model.prompt_encoder.image_embedding_size\n    mask_input_size = [4 * x for x in embed_size]\n    dummy_inputs = {\n        \"image_embeddings\": torch.randn(\n            1, embed_dim, *embed_size, dtype=torch.float\n        ),\n        \"point_coords\": torch.randint(\n            low=0, high=1024, size=(1, 5, 2), dtype=torch.float\n        ),\n        \"point_labels\": torch.randint(\n            low=0, high=4, size=(1, 5), dtype=torch.float\n        ),\n        \"mask_input\": torch.randn(1, 1, *mask_input_size, dtype=torch.float),\n        \"has_mask_input\": torch.tensor([1], dtype=torch.float),\n        \"orig_im_size\": torch.tensor([1500, 2250], dtype=torch.float),\n    }\n    output_names = [\"masks\", \"iou_predictions\", \"low_res_masks\"]\n    onnx_path = model_dir / self.checkpoint_path.name.replace(\".pth\", \".onnx\")\n\n    # Export model\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        with open(onnx_path, \"wb\") as f:\n            torch.onnx.export(\n                onnx_model,\n                tuple(dummy_inputs.values()),\n                f,\n                export_params=True,\n                verbose=False,\n                opset_version=17,\n                do_constant_folding=True,\n                input_names=list(dummy_inputs.keys()),\n                output_names=output_names,\n                dynamic_axes=dynamic_axes,\n            )\n    # Quantize model\n    quantize_dynamic(\n        model_input=onnx_path,\n        model_output=onnx_path,\n        optimize_model=True,\n        per_channel=False,\n        reduce_range=False,\n        weight_type=QuantType.QUInt8,\n    )\n\n    # Put model back to device after export\n    self.model.to(self.device)\n</code></pre>"},{"location":"api_reference/github/sam/#pixano_inference.github.sam.SAM.preannotate","title":"<code>preannotate(batch, views, uri_prefix, threshold=0.0, prompt='')</code>","text":"<p>Inference pre-annotation for a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>RecordBatch</code> <p>Input batch</p> required <code>views</code> <code>list[str]</code> <p>Dataset views</p> required <code>uri_prefix</code> <code>str</code> <p>URI prefix for media files</p> required <code>threshold</code> <code>float</code> <p>Confidence threshold. Defaults to 0.0.</p> <code>0.0</code> <code>prompt</code> <code>str</code> <p>Annotation text prompt. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>Processed rows</p> Source code in <code>pixano_inference/github/sam.py</code> <pre><code>def preannotate(\n    self,\n    batch: pa.RecordBatch,\n    views: list[str],\n    uri_prefix: str,\n    threshold: float = 0.0,\n    prompt: str = \"\",\n) -&gt; list[dict]:\n    \"\"\"Inference pre-annotation for a batch\n\n    Args:\n        batch (pa.RecordBatch): Input batch\n        views (list[str]): Dataset views\n        uri_prefix (str): URI prefix for media files\n        threshold (float, optional): Confidence threshold. Defaults to 0.0.\n        prompt (str, optional): Annotation text prompt. Defaults to \"\".\n\n    Returns:\n        list[dict]: Processed rows\n    \"\"\"\n\n    # Import SAM\n    segment_anything = attempt_import(\n        \"segment_anything\",\n        \"segment-anything@git+https://github.com/facebookresearch/segment-anything\",\n    )\n\n    rows = []\n    _ = prompt  # This model does not use prompts\n\n    for view in views:\n        # Iterate manually\n        for x in range(batch.num_rows):\n            # Preprocess image\n            im: Image = Image.from_dict(batch[view][x].as_py())\n            im.uri_prefix = uri_prefix\n            im = im.as_cv2()\n            im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n\n            # Inference\n            with torch.no_grad():\n                generator = segment_anything.SamAutomaticMaskGenerator(self.model)\n                output = generator.generate(im)\n\n            # Process model outputs\n            h, w = im.shape[:2]\n            rows.extend(\n                [\n                    {\n                        \"id\": shortuuid.uuid(),\n                        \"item_id\": batch[\"id\"][x].as_py(),\n                        \"view_id\": view,\n                        \"bbox\": BBox.from_xywh(\n                            [int(coord) for coord in output[i][\"bbox\"]],\n                            confidence=float(output[i][\"predicted_iou\"]),\n                        )\n                        .normalize(h, w)\n                        .to_dict(),\n                        \"mask\": CompressedRLE.from_mask(\n                            output[i][\"segmentation\"]\n                        ).to_dict(),\n                    }\n                    for i in range(len(output))\n                    if output[i][\"predicted_iou\"] &gt; threshold\n                ]\n            )\n\n    return rows\n</code></pre>"},{"location":"api_reference/github/sam/#pixano_inference.github.sam.SAM.precompute_embeddings","title":"<code>precompute_embeddings(batch, views, uri_prefix)</code>","text":"<p>Embedding precomputing for a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>RecordBatch</code> <p>Input batch</p> required <code>views</code> <code>list[str]</code> <p>Dataset views</p> required <code>uri_prefix</code> <code>str</code> <p>URI prefix for media files</p> required <p>Returns:</p> Type Description <code>RecordBatch</code> <p>Embedding rows</p> Source code in <code>pixano_inference/github/sam.py</code> <pre><code>def precompute_embeddings(\n    self,\n    batch: pa.RecordBatch,\n    views: list[str],\n    uri_prefix: str,\n) -&gt; list[dict]:\n    \"\"\"Embedding precomputing for a batch\n\n    Args:\n        batch (pa.RecordBatch): Input batch\n        views (list[str]): Dataset views\n        uri_prefix (str): URI prefix for media files\n\n    Returns:\n        pa.RecordBatch: Embedding rows\n    \"\"\"\n\n    # Import SAM\n    segment_anything = attempt_import(\n        \"segment_anything\",\n        \"segment-anything@git+https://github.com/facebookresearch/segment-anything\",\n    )\n\n    rows = [\n        {\n            \"id\": batch[\"id\"][x].as_py(),\n        }\n        for x in range(batch.num_rows)\n    ]\n\n    for view in views:\n        # Iterate manually\n        for x in range(batch.num_rows):\n            # Preprocess image\n            im: Image = Image.from_dict(batch[view][x].as_py())\n            im.uri_prefix = uri_prefix\n            im = im.as_cv2()\n            im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n\n            # Inference\n            with torch.no_grad():\n                predictor = segment_anything.SamPredictor(self.model)\n                predictor.set_image(im)\n                img_embedding = predictor.get_image_embedding().cpu().numpy()\n\n            # Process model outputs\n            emb_bytes = BytesIO()\n            np.save(emb_bytes, img_embedding)\n            rows[x][view] = emb_bytes.getvalue()\n\n    return rows\n</code></pre>"},{"location":"api_reference/pytorch/deeplabv3/","title":"deeplabv3","text":""},{"location":"api_reference/pytorch/deeplabv3/#pixano_inference.pytorch.deeplabv3","title":"<code>pixano_inference.pytorch.deeplabv3</code>","text":""},{"location":"api_reference/pytorch/deeplabv3/#pixano_inference.pytorch.deeplabv3.DeepLabV3","title":"<code>DeepLabV3(model_id='', device='cuda')</code>","text":"<p>             Bases: <code>InferenceModel</code></p> <p>PyTorch Hub DeepLabV3 Model</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Model name</p> <code>model_id</code> <code>str</code> <p>Model ID</p> <code>device</code> <code>str</code> <p>Model GPU or CPU device</p> <code>description</code> <code>str</code> <p>Model description</p> <code>model</code> <code>Module</code> <p>PyTorch model</p> <code>transforms</code> <code>Module</code> <p>PyTorch preprocessing transforms</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Previously used ID, generate new ID if \"\". Defaults to \"\".</p> <code>''</code> <code>device</code> <code>str</code> <p>Model GPU or CPU device (e.g. \"cuda\", \"cpu\"). Defaults to \"cuda\".</p> <code>'cuda'</code> Source code in <code>pixano_inference/pytorch/deeplabv3.py</code> <pre><code>def __init__(\n    self,\n    model_id: str = \"\",\n    device: str = \"cuda\",\n) -&gt; None:\n    \"\"\"Initialize model\n\n    Args:\n        model_id (str, optional): Previously used ID, generate new ID if \"\". Defaults to \"\".\n        device (str, optional): Model GPU or CPU device (e.g. \"cuda\", \"cpu\"). Defaults to \"cuda\".\n    \"\"\"\n\n    super().__init__(\n        name=\"DeepLabV3\",\n        model_id=model_id,\n        device=device,\n        description=\"From PyTorch Hub. DeepLabV3, ResNet-50 Backbone.\",\n    )\n\n    # Model\n    self.model = torch.hub.load(\n        \"pytorch/vision:v0.10.0\", \"deeplabv3_resnet50\", pretrained=True\n    )\n    self.model.eval()\n    self.model.to(self.device)\n\n    # Transforms\n    self.transforms = T.Compose(\n        [\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ]\n    )\n</code></pre>"},{"location":"api_reference/pytorch/deeplabv3/#pixano_inference.pytorch.deeplabv3.DeepLabV3.preannotate","title":"<code>preannotate(batch, views, uri_prefix, threshold=0.0, prompt='')</code>","text":"<p>Inference pre-annotation for a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>RecordBatch</code> <p>Input batch</p> required <code>views</code> <code>list[str]</code> <p>Dataset views</p> required <code>uri_prefix</code> <code>str</code> <p>URI prefix for media files</p> required <code>threshold</code> <code>float</code> <p>Confidence threshold. Defaults to 0.0.</p> <code>0.0</code> <code>prompt</code> <code>str</code> <p>Annotation text prompt. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>Processed rows</p> Source code in <code>pixano_inference/pytorch/deeplabv3.py</code> <pre><code>def preannotate(\n    self,\n    batch: pa.RecordBatch,\n    views: list[str],\n    uri_prefix: str,\n    threshold: float = 0.0,\n    prompt: str = \"\",\n) -&gt; list[dict]:\n    \"\"\"Inference pre-annotation for a batch\n\n    Args:\n        batch (pa.RecordBatch): Input batch\n        views (list[str]): Dataset views\n        uri_prefix (str): URI prefix for media files\n        threshold (float, optional): Confidence threshold. Defaults to 0.0.\n        prompt (str, optional): Annotation text prompt. Defaults to \"\".\n\n    Returns:\n        list[dict]: Processed rows\n    \"\"\"\n\n    rows = []\n    _ = prompt  # This model does not use prompts\n\n    for view in views:\n        # PyTorch Transforms don't support different-sized image batches, so iterate manually\n        for x in range(batch.num_rows):\n            # Preprocess image\n            im: Image = Image.from_dict(batch[view][x].as_py())\n            im.uri_prefix = uri_prefix\n            im = im.as_pillow()\n            im_tensor = self.transforms(im).unsqueeze(0).to(self.device)\n\n            # Inference\n            with torch.no_grad():\n                output = self.model(im_tensor)[\"out\"][0]\n\n            # Process model outputs\n            sem_mask = output.argmax(0)\n            labels = torch.unique(sem_mask)[1:]\n            masks = sem_mask == labels[:, None, None]\n\n            rows.extend(\n                [\n                    {\n                        \"id\": shortuuid.uuid(),\n                        \"item_id\": batch[\"id\"][x].as_py(),\n                        \"view_id\": view,\n                        \"mask\": CompressedRLE.from_mask(\n                            unmold_mask(mask)\n                        ).to_dict(),\n                        \"category\": voc_names(label),\n                    }\n                    for label, mask in zip(labels, masks)\n                ]\n            )\n\n    return rows\n</code></pre>"},{"location":"api_reference/pytorch/deeplabv3/#pixano_inference.pytorch.deeplabv3.unmold_mask","title":"<code>unmold_mask(mask, threshold=0.5)</code>","text":"<p>Convert mask from torch.Tensor to np.array, squeeze a dimension if needed, and treshold values</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Tensor</code> <p>Mask (1, W, H)</p> required <code>threshold</code> <code>float</code> <p>Confidence threshold. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>array</code> <p>Mask (W, H)</p> Source code in <code>pixano_inference/pytorch/deeplabv3.py</code> <pre><code>def unmold_mask(mask: torch.Tensor, threshold: float = 0.5):\n    \"\"\"Convert mask from torch.Tensor to np.array, squeeze a dimension if needed, and treshold values\n\n    Args:\n        mask (torch.Tensor): Mask (1, W, H)\n        threshold (float, optional): Confidence threshold. Defaults to 0.5.\n\n    Returns:\n        np.array: Mask (W, H)\n    \"\"\"\n\n    # Detach and convert to NumPy\n    mask = mask.cpu().numpy()\n\n    # Squeeze dimension if needed\n    if 1 in mask.shape:\n        mask = mask.squeeze(mask.shape.index(1))\n\n    # Threshold values\n    mask = np.where(mask &gt;= threshold, 1, 0).astype(np.uint8)\n\n    return mask\n</code></pre>"},{"location":"api_reference/pytorch/maskrcnnv2/","title":"maskrcnnv2","text":""},{"location":"api_reference/pytorch/maskrcnnv2/#pixano_inference.pytorch.maskrcnnv2","title":"<code>pixano_inference.pytorch.maskrcnnv2</code>","text":""},{"location":"api_reference/pytorch/maskrcnnv2/#pixano_inference.pytorch.maskrcnnv2.MaskRCNNv2","title":"<code>MaskRCNNv2(model_id='', device='cuda')</code>","text":"<p>             Bases: <code>InferenceModel</code></p> <p>PyTorch Hub MaskRCNNv2 Model</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Model name</p> <code>model_id</code> <code>str</code> <p>Model ID</p> <code>device</code> <code>str</code> <p>Model GPU or CPU device</p> <code>description</code> <code>str</code> <p>Model description</p> <code>model</code> <code>Module</code> <p>PyTorch model</p> <code>transforms</code> <code>Module</code> <p>PyTorch preprocessing transforms</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Previously used ID, generate new ID if \"\". Defaults to \"\".</p> <code>''</code> <code>device</code> <code>str</code> <p>Model GPU or CPU device (e.g. \"cuda\", \"cpu\"). Defaults to \"cuda\".</p> <code>'cuda'</code> Source code in <code>pixano_inference/pytorch/maskrcnnv2.py</code> <pre><code>def __init__(\n    self,\n    model_id: str = \"\",\n    device: str = \"cuda\",\n) -&gt; None:\n    \"\"\"Initialize model\n\n    Args:\n        model_id (str, optional): Previously used ID, generate new ID if \"\". Defaults to \"\".\n        device (str, optional): Model GPU or CPU device (e.g. \"cuda\", \"cpu\"). Defaults to \"cuda\".\n    \"\"\"\n\n    super().__init__(\n        name=\"MaskRCNNv2\",\n        model_id=model_id,\n        device=device,\n        description=\"From PyTorch Hub. MaskRCNN, ResNet-50-FPN v2 Backbone, COCO_V1 Weights.\",\n    )\n\n    # Model\n    self.model = maskrcnn_resnet50_fpn_v2(\n        weights=MaskRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n    )\n    self.model.eval()\n    self.model.to(self.device)\n\n    # Transforms\n    self.transforms = MaskRCNN_ResNet50_FPN_V2_Weights.COCO_V1.transforms()\n</code></pre>"},{"location":"api_reference/pytorch/maskrcnnv2/#pixano_inference.pytorch.maskrcnnv2.MaskRCNNv2.preannotate","title":"<code>preannotate(batch, views, uri_prefix, threshold=0.0, prompt='')</code>","text":"<p>Inference pre-annotation for a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>RecordBatch</code> <p>Input batch</p> required <code>views</code> <code>list[str]</code> <p>Dataset views</p> required <code>uri_prefix</code> <code>str</code> <p>URI prefix for media files</p> required <code>threshold</code> <code>float</code> <p>Confidence threshold. Defaults to 0.0.</p> <code>0.0</code> <code>prompt</code> <code>str</code> <p>Annotation text prompt. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>Processed rows</p> Source code in <code>pixano_inference/pytorch/maskrcnnv2.py</code> <pre><code>def preannotate(\n    self,\n    batch: pa.RecordBatch,\n    views: list[str],\n    uri_prefix: str,\n    threshold: float = 0.0,\n    prompt: str = \"\",\n) -&gt; list[dict]:\n    \"\"\"Inference pre-annotation for a batch\n\n    Args:\n        batch (pa.RecordBatch): Input batch\n        views (list[str]): Dataset views\n        uri_prefix (str): URI prefix for media files\n        threshold (float, optional): Confidence threshold. Defaults to 0.0.\n        prompt (str, optional): Annotation text prompt. Defaults to \"\".\n\n    Returns:\n        list[dict]: Processed rows\n    \"\"\"\n\n    rows = []\n    _ = prompt  # This model does not use prompts\n\n    for view in views:\n        # PyTorch Transforms don't support different-sized image batches, so iterate manually\n        for x in range(batch.num_rows):\n            # Preprocess image\n            im: Image = Image.from_dict(batch[view][x].as_py())\n            im.uri_prefix = uri_prefix\n            im = im.as_pillow()\n            im_tensor = self.transforms(im).unsqueeze(0).to(self.device)\n\n            # Inference\n            with torch.no_grad():\n                output = self.model(im_tensor)[0]\n\n            # Process model outputs\n            w, h = im.size\n            rows.extend(\n                [\n                    {\n                        \"id\": shortuuid.uuid(),\n                        \"item_id\": batch[\"id\"][x].as_py(),\n                        \"view_id\": view,\n                        \"bbox\": BBox.from_xyxy(\n                            [coord.item() for coord in output[\"boxes\"][i]],\n                            confidence=output[\"scores\"][i].item(),\n                        )\n                        .normalize(h, w)\n                        .to_dict(),\n                        \"mask\": CompressedRLE.from_mask(\n                            unmold_mask(output[\"masks\"][i])\n                        ).to_dict(),\n                        \"category\": coco_names_91(output[\"labels\"][i]),\n                    }\n                    for i in range(len(output[\"scores\"]))\n                    if output[\"scores\"][i] &gt; threshold\n                ]\n            )\n\n    return rows\n</code></pre>"},{"location":"api_reference/pytorch/maskrcnnv2/#pixano_inference.pytorch.maskrcnnv2.unmold_mask","title":"<code>unmold_mask(mask, threshold=0.5)</code>","text":"<p>Convert mask from torch.Tensor to np.array, squeeze a dimension if needed, and treshold values</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Tensor</code> <p>Mask (1, W, H)</p> required <code>threshold</code> <code>float</code> <p>Confidence threshold. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>array</code> <p>Mask (W, H)</p> Source code in <code>pixano_inference/pytorch/maskrcnnv2.py</code> <pre><code>def unmold_mask(mask: torch.Tensor, threshold: float = 0.5):\n    \"\"\"Convert mask from torch.Tensor to np.array, squeeze a dimension if needed, and treshold values\n\n    Args:\n        mask (torch.Tensor): Mask (1, W, H)\n        threshold (float, optional): Confidence threshold. Defaults to 0.5.\n\n    Returns:\n        np.array: Mask (W, H)\n    \"\"\"\n\n    # Detach and convert to NumPy\n    mask = mask.cpu().numpy()\n\n    # Squeeze dimension if needed\n    if 1 in mask.shape:\n        mask = mask.squeeze(mask.shape.index(1))\n\n    # Threshold values\n    mask = np.where(mask &gt;= threshold, 1, 0).astype(np.uint8)\n\n    return mask\n</code></pre>"},{"location":"api_reference/pytorch/yolov5/","title":"yolov5","text":""},{"location":"api_reference/pytorch/yolov5/#pixano_inference.pytorch.yolov5","title":"<code>pixano_inference.pytorch.yolov5</code>","text":""},{"location":"api_reference/pytorch/yolov5/#pixano_inference.pytorch.yolov5.YOLOv5","title":"<code>YOLOv5(size='s', model_id='', device='cuda')</code>","text":"<p>             Bases: <code>InferenceModel</code></p> <p>PyTorch Hub YOLOv5 Model</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Model name</p> <code>model_id</code> <code>str</code> <p>Model ID</p> <code>device</code> <code>str</code> <p>Model GPU or CPU device</p> <code>description</code> <code>str</code> <p>Model description</p> <code>model</code> <code>Module</code> <p>PyTorch model</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>str</code> <p>Model size (\"n\", \"s\", \"m\", \"x\"). Defaults to \"s\".</p> <code>'s'</code> <code>model_id</code> <code>str</code> <p>Previously used ID, generate new ID if \"\". Defaults to \"\".</p> <code>''</code> <code>device</code> <code>str</code> <p>Model GPU or CPU device (e.g. \"cuda\", \"cpu\"). Defaults to \"cuda\".</p> <code>'cuda'</code> Source code in <code>pixano_inference/pytorch/yolov5.py</code> <pre><code>def __init__(\n    self,\n    size: str = \"s\",\n    model_id: str = \"\",\n    device: str = \"cuda\",\n) -&gt; None:\n    \"\"\"Initialize model\n\n    Args:\n        size (str, optional): Model size (\"n\", \"s\", \"m\", \"x\"). Defaults to \"s\".\n        model_id (str, optional): Previously used ID, generate new ID if \"\". Defaults to \"\".\n        device (str, optional): Model GPU or CPU device (e.g. \"cuda\", \"cpu\"). Defaults to \"cuda\".\n    \"\"\"\n\n    super().__init__(\n        name=f\"YOLOv5{size}\",\n        model_id=model_id,\n        device=device,\n        description=f\"From PyTorch Hub. YOLOv5 model, {size.upper()} backbone.\",\n    )\n\n    # Model\n    self.model = torch.hub.load(\n        \"ultralytics/yolov5\",\n        model=f\"yolov5{size}\",\n        pretrained=True,\n    )\n    self.model.to(self.device)\n</code></pre>"},{"location":"api_reference/pytorch/yolov5/#pixano_inference.pytorch.yolov5.YOLOv5.preannotate","title":"<code>preannotate(batch, views, uri_prefix, threshold=0.0, prompt='')</code>","text":"<p>Inference pre-annotation for a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>RecordBatch</code> <p>Input batch</p> required <code>views</code> <code>list[str]</code> <p>Dataset views</p> required <code>uri_prefix</code> <code>str</code> <p>URI prefix for media files</p> required <code>threshold</code> <code>float</code> <p>Confidence threshold. Defaults to 0.0.</p> <code>0.0</code> <code>prompt</code> <code>str</code> <p>Annotation text prompt. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>Processed rows</p> Source code in <code>pixano_inference/pytorch/yolov5.py</code> <pre><code>def preannotate(\n    self,\n    batch: pa.RecordBatch,\n    views: list[str],\n    uri_prefix: str,\n    threshold: float = 0.0,\n    prompt: str = \"\",\n) -&gt; list[dict]:\n    \"\"\"Inference pre-annotation for a batch\n\n    Args:\n        batch (pa.RecordBatch): Input batch\n        views (list[str]): Dataset views\n        uri_prefix (str): URI prefix for media files\n        threshold (float, optional): Confidence threshold. Defaults to 0.0.\n        prompt (str, optional): Annotation text prompt. Defaults to \"\".\n\n    Returns:\n        list[dict]: Processed rows\n    \"\"\"\n\n    rows = []\n    _ = prompt  # This model does not use prompts\n\n    for view in views:\n        # Preprocess image batch\n        im_batch = []\n        for x in range(batch.num_rows):\n            im: Image = Image.from_dict(batch[view][x].as_py())\n            im.uri_prefix = uri_prefix\n            im_batch.append(im.as_pillow())\n\n        # Inference\n        outputs = self.model(im_batch)\n\n        # Process model outputs\n        for x, img, img_output in zip(\n            range(batch.num_rows), im_batch, outputs.xyxy\n        ):\n            w, h = img.size\n            rows.extend(\n                [\n                    {\n                        \"id\": shortuuid.uuid(),\n                        \"item_id\": batch[\"id\"][x].as_py(),\n                        \"view_id\": view,\n                        \"bbox\": BBox.from_xyxy(\n                            [coord.item() for coord in pred[0:4]],\n                            confidence=pred[4].item(),\n                        )\n                        .normalize(h, w)\n                        .to_dict(),\n                        \"category\": coco_names_91(coco_ids_80to91(pred[5] + 1)),\n                    }\n                    for pred in img_output\n                    if pred[4] &gt; threshold\n                ]\n            )\n\n    return rows\n</code></pre>"},{"location":"api_reference/tensorflow/efficientdet/","title":"efficientdet","text":""},{"location":"api_reference/tensorflow/efficientdet/#pixano_inference.tensorflow.efficientdet","title":"<code>pixano_inference.tensorflow.efficientdet</code>","text":""},{"location":"api_reference/tensorflow/efficientdet/#pixano_inference.tensorflow.efficientdet.EfficientDet","title":"<code>EfficientDet(model_id='', device='/GPU:0')</code>","text":"<p>             Bases: <code>InferenceModel</code></p> <p>TensorFlow Hub EfficientDet Model</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Model name</p> <code>model_id</code> <code>str</code> <p>Model ID</p> <code>device</code> <code>str</code> <p>Model GPU or CPU device</p> <code>description</code> <code>str</code> <p>Model description</p> <code>model</code> <code>Model</code> <p>TensorFlow model</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Previously used ID, generate new ID if \"\". Defaults to \"\".</p> <code>''</code> <code>device</code> <code>str</code> <p>Model GPU or CPU device (e.g. \"/GPU:0\", \"/CPU:0\"). Defaults to \"/GPU:0\".</p> <code>'/GPU:0'</code> Source code in <code>pixano_inference/tensorflow/efficientdet.py</code> <pre><code>def __init__(\n    self,\n    model_id: str = \"\",\n    device: str = \"/GPU:0\",\n) -&gt; None:\n    \"\"\"Initialize model\n\n    Args:\n        model_id (str, optional): Previously used ID, generate new ID if \"\". Defaults to \"\".\n        device (str, optional): Model GPU or CPU device (e.g. \"/GPU:0\", \"/CPU:0\"). Defaults to \"/GPU:0\".\n    \"\"\"\n\n    super().__init__(\n        name=\"EfficientDet_D1\",\n        model_id=model_id,\n        device=device,\n        description=\"From TensorFlow Hub. EfficientDet model, with D1 architecture.\",\n    )\n\n    # Model\n    with tf.device(self.device):\n        self.model = hub.load(\"https://tfhub.dev/tensorflow/efficientdet/d1/1\")\n</code></pre>"},{"location":"api_reference/tensorflow/efficientdet/#pixano_inference.tensorflow.efficientdet.EfficientDet.preannotate","title":"<code>preannotate(batch, views, uri_prefix, threshold=0.0, prompt='')</code>","text":"<p>Inference pre-annotation for a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>RecordBatch</code> <p>Input batch</p> required <code>views</code> <code>list[str]</code> <p>Dataset views</p> required <code>uri_prefix</code> <code>str</code> <p>URI prefix for media files</p> required <code>threshold</code> <code>float</code> <p>Confidence threshold. Defaults to 0.0.</p> <code>0.0</code> <code>prompt</code> <code>str</code> <p>Annotation text prompt. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>Processed rows</p> Source code in <code>pixano_inference/tensorflow/efficientdet.py</code> <pre><code>def preannotate(\n    self,\n    batch: pa.RecordBatch,\n    views: list[str],\n    uri_prefix: str,\n    threshold: float = 0.0,\n    prompt: str = \"\",\n) -&gt; list[dict]:\n    \"\"\"Inference pre-annotation for a batch\n\n    Args:\n        batch (pa.RecordBatch): Input batch\n        views (list[str]): Dataset views\n        uri_prefix (str): URI prefix for media files\n        threshold (float, optional): Confidence threshold. Defaults to 0.0.\n        prompt (str, optional): Annotation text prompt. Defaults to \"\".\n\n    Returns:\n        list[dict]: Processed rows\n    \"\"\"\n\n    rows = []\n    _ = prompt  # This model does not use prompts\n\n    for view in views:\n        # TF.Hub Models don't support image batches, so iterate manually\n        for x in range(batch.num_rows):\n            # Preprocess image\n            im: Image = Image.from_dict(batch[view][x].as_py())\n            im.uri_prefix = uri_prefix\n            im = im.as_pillow()\n            im_tensor = tf.expand_dims(tf.keras.utils.img_to_array(im), 0)\n            im_tensor = tf.image.convert_image_dtype(im_tensor, dtype=\"uint8\")\n\n            # Inference\n            output = self.model(im_tensor)\n\n            # Process model outputs\n            rows.extend(\n                [\n                    {\n                        \"id\": shortuuid.uuid(),\n                        \"item_id\": batch[\"id\"][x].as_py(),\n                        \"view_id\": view,\n                        \"bbox\": BBox.from_xyxy(\n                            [\n                                float(output[\"detection_boxes\"][0][i][1]),\n                                float(output[\"detection_boxes\"][0][i][0]),\n                                float(output[\"detection_boxes\"][0][i][3]),\n                                float(output[\"detection_boxes\"][0][i][2]),\n                            ],\n                            confidence=float(output[\"detection_scores\"][0][i]),\n                        ).to_dict(),\n                        \"category\": coco_names_91(\n                            output[\"detection_classes\"][0][i]\n                        ),\n                    }\n                    for i in range(int(output[\"num_detections\"]))\n                    if output[\"detection_scores\"][0][i] &gt; threshold\n                ]\n            )\n\n    return rows\n</code></pre>"},{"location":"api_reference/tensorflow/fasterrcnn/","title":"fasterrcnn","text":""},{"location":"api_reference/tensorflow/fasterrcnn/#pixano_inference.tensorflow.fasterrcnn","title":"<code>pixano_inference.tensorflow.fasterrcnn</code>","text":""},{"location":"api_reference/tensorflow/fasterrcnn/#pixano_inference.tensorflow.fasterrcnn.FasterRCNN","title":"<code>FasterRCNN(model_id='', device='/GPU:0')</code>","text":"<p>             Bases: <code>InferenceModel</code></p> <p>TensorFlow Hub FasterRCNN Model</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Model name</p> <code>model_id</code> <code>str</code> <p>Model ID</p> <code>device</code> <code>str</code> <p>Model GPU or CPU device</p> <code>description</code> <code>str</code> <p>Model description</p> <code>model</code> <code>Model</code> <p>TensorFlow model</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Previously used ID, generate new ID if \"\". Defaults to \"\".</p> <code>''</code> <code>device</code> <code>str</code> <p>Model GPU or CPU device (e.g. \"/GPU:0\", \"/CPU:0\"). Defaults to \"/GPU:0\".</p> <code>'/GPU:0'</code> Source code in <code>pixano_inference/tensorflow/fasterrcnn.py</code> <pre><code>def __init__(\n    self,\n    model_id: str = \"\",\n    device: str = \"/GPU:0\",\n) -&gt; None:\n    \"\"\"Initialize model\n\n    Args:\n        model_id (str, optional): Previously used ID, generate new ID if \"\". Defaults to \"\".\n        device (str, optional): Model GPU or CPU device (e.g. \"/GPU:0\", \"/CPU:0\"). Defaults to \"/GPU:0\".\n    \"\"\"\n\n    super().__init__(\n        name=\"FasterRCNN_R50\",\n        model_id=model_id,\n        device=device,\n        description=\"From TensorFlow Hub. FasterRCNN model, with ResNet50 architecture.\",\n    )\n\n    # Model\n    with tf.device(self.device):\n        self.model = hub.load(\n            \"https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1\"\n        )\n</code></pre>"},{"location":"api_reference/tensorflow/fasterrcnn/#pixano_inference.tensorflow.fasterrcnn.FasterRCNN.preannotate","title":"<code>preannotate(batch, views, uri_prefix, threshold=0.0, prompt='')</code>","text":"<p>Inference pre-annotation for a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>RecordBatch</code> <p>Input batch</p> required <code>views</code> <code>list[str]</code> <p>Dataset views</p> required <code>uri_prefix</code> <code>str</code> <p>URI prefix for media files</p> required <code>threshold</code> <code>float</code> <p>Confidence threshold. Defaults to 0.0.</p> <code>0.0</code> <code>prompt</code> <code>str</code> <p>Annotation text prompt. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>Processed rows</p> Source code in <code>pixano_inference/tensorflow/fasterrcnn.py</code> <pre><code>def preannotate(\n    self,\n    batch: pa.RecordBatch,\n    views: list[str],\n    uri_prefix: str,\n    threshold: float = 0.0,\n    prompt: str = \"\",\n) -&gt; list[dict]:\n    \"\"\"Inference pre-annotation for a batch\n\n    Args:\n        batch (pa.RecordBatch): Input batch\n        views (list[str]): Dataset views\n        uri_prefix (str): URI prefix for media files\n        threshold (float, optional): Confidence threshold. Defaults to 0.0.\n        prompt (str, optional): Annotation text prompt. Defaults to \"\".\n\n    Returns:\n        list[dict]: Processed rows\n    \"\"\"\n\n    rows = []\n    _ = prompt  # This model does not use prompts\n\n    for view in views:\n        # TF.Hub Models don't support image batches, so iterate manually\n        for x in range(batch.num_rows):\n            # Preprocess image\n            im: Image = Image.from_dict(batch[view][x].as_py())\n            im.uri_prefix = uri_prefix\n            im = im.as_pillow()\n            im_tensor = tf.expand_dims(tf.keras.utils.img_to_array(im), 0)\n            im_tensor = tf.image.convert_image_dtype(im_tensor, dtype=\"uint8\")\n\n            # Inference\n            output = self.model(im_tensor)\n\n            # Process model outputs\n            rows.extend(\n                [\n                    {\n                        \"id\": shortuuid.uuid(),\n                        \"item_id\": batch[\"id\"][x].as_py(),\n                        \"view_id\": view,\n                        \"bbox\": BBox.from_xyxy(\n                            [\n                                float(output[\"detection_boxes\"][0][i][1]),\n                                float(output[\"detection_boxes\"][0][i][0]),\n                                float(output[\"detection_boxes\"][0][i][3]),\n                                float(output[\"detection_boxes\"][0][i][2]),\n                            ],\n                            confidence=float(output[\"detection_scores\"][0][i]),\n                        ).to_dict(),\n                        \"category\": coco_names_91(\n                            output[\"detection_classes\"][0][i]\n                        ),\n                    }\n                    for i in range(int(output[\"num_detections\"]))\n                    if output[\"detection_scores\"][0][i] &gt; threshold\n                ]\n            )\n\n    return rows\n</code></pre>"},{"location":"api_reference/transformers/clip/","title":"clip","text":""},{"location":"api_reference/transformers/clip/#pixano_inference.transformers.clip","title":"<code>pixano_inference.transformers.clip</code>","text":""},{"location":"api_reference/transformers/clip/#pixano_inference.transformers.clip.CLIP","title":"<code>CLIP(pretrained_model='openai/clip-vit-base-patch32', model_id='')</code>","text":"<p>             Bases: <code>InferenceModel</code></p> <p>CLIP: Connecting text and images</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Model name</p> <code>model_id</code> <code>str</code> <p>Model ID</p> <code>device</code> <code>str</code> <p>Model GPU or CPU device (e.g. \"cuda\", \"cpu\")</p> <code>description</code> <code>str</code> <p>Model description</p> <code>model</code> <code>CLIPModel</code> <p>CLIP model</p> <code>processor</code> <code>CLIPProcessor</code> <p>CLIP processor</p> <code>tokenizer</code> <code>CLIPTokenizerFast</code> <p>CLIP tokenizer</p> <code>pretrained_model</code> <code>str</code> <p>Pretrained model name or path</p> <p>Parameters:</p> Name Type Description Default <code>pretrained_model</code> <code>str</code> <p>Pretrained model name or path</p> <code>'openai/clip-vit-base-patch32'</code> <code>model_id</code> <code>str</code> <p>Previously used ID, generate new ID if \"\". Defaults to \"\".</p> <code>''</code> Source code in <code>pixano_inference/transformers/clip.py</code> <pre><code>def __init__(\n    self,\n    pretrained_model: str = \"openai/clip-vit-base-patch32\",\n    model_id: str = \"\",\n) -&gt; None:\n    \"\"\"Initialize model\n\n    Args:\n        pretrained_model (str): Pretrained model name or path\n        model_id (str, optional): Previously used ID, generate new ID if \"\". Defaults to \"\".\n    \"\"\"\n\n    super().__init__(\n        name=\"CLIP\",\n        model_id=model_id,\n        device=\"cpu\",\n        description=f\"From HuggingFace Transformers. CLIP: Connecting text and images. {pretrained_model}.\",\n    )\n\n    # Model\n    self.model = CLIPModel.from_pretrained(pretrained_model)\n    self.processor = CLIPProcessor.from_pretrained(pretrained_model)\n    self.tokenizer = CLIPTokenizerFast.from_pretrained(pretrained_model)\n\n    # Model name or path\n    self.pretrained_model = pretrained_model\n</code></pre>"},{"location":"api_reference/transformers/clip/#pixano_inference.transformers.clip.CLIP.precompute_embeddings","title":"<code>precompute_embeddings(batch, views, uri_prefix)</code>","text":"<p>Embedding precomputing for a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>RecordBatch</code> <p>Input batch</p> required <code>views</code> <code>list[str]</code> <p>Dataset views</p> required <code>uri_prefix</code> <code>str</code> <p>URI prefix for media files</p> required <p>Returns:</p> Type Description <code>RecordBatch</code> <p>Embedding rows</p> Source code in <code>pixano_inference/transformers/clip.py</code> <pre><code>def precompute_embeddings(\n    self,\n    batch: pa.RecordBatch,\n    views: list[str],\n    uri_prefix: str,\n) -&gt; list[dict]:\n    \"\"\"Embedding precomputing for a batch\n\n    Args:\n        batch (pa.RecordBatch): Input batch\n        views (list[str]): Dataset views\n        uri_prefix (str): URI prefix for media files\n\n    Returns:\n        pa.RecordBatch: Embedding rows\n    \"\"\"\n\n    rows = [\n        {\n            \"id\": batch[\"id\"][x].as_py(),\n        }\n        for x in range(batch.num_rows)\n    ]\n\n    for view in views:\n        # Iterate manually\n        for x in range(batch.num_rows):\n            # Preprocess image\n            im: Image = Image.from_dict(batch[view][x].as_py())\n            im.uri_prefix = uri_prefix\n            im = im.as_pillow()\n\n            # Inference\n            inputs = self.processor(images=im, padded=True, return_tensors=\"pt\")\n            image_features = self.model.get_image_features(**inputs)\n            vect = image_features.detach().numpy()[0]\n\n            # Process model outputs\n            rows[x][view] = vect\n\n    return rows\n</code></pre>"},{"location":"api_reference/transformers/clip/#pixano_inference.transformers.clip.CLIP.semantic_search","title":"<code>semantic_search(query)</code>","text":"<p>Process semantic search query with CLIP</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query text</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Search query vector</p> Source code in <code>pixano_inference/transformers/clip.py</code> <pre><code>def semantic_search(self, query: str) -&gt; np.ndarray:\n    \"\"\"Process semantic search query with CLIP\n\n    Args:\n        query (str): Search query text\n\n    Returns:\n        np.ndarray: Search query vector\n    \"\"\"\n\n    inputs = self.tokenizer([query], padding=True, return_tensors=\"pt\")\n    text_features = self.model.get_text_features(**inputs)\n\n    return text_features.detach().numpy()[0]\n</code></pre>"},{"location":"api_reference/utils/main/","title":"main","text":""},{"location":"api_reference/utils/main/#pixano_inference.utils.main","title":"<code>pixano_inference.utils.main</code>","text":""},{"location":"api_reference/utils/main/#pixano_inference.utils.main.attempt_import","title":"<code>attempt_import(module, package=None)</code>","text":"<p>Import specified module, or raise ImportError with a helpful message</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>str</code> <p>The name of the module to import</p> required <code>package</code> <code>str</code> <p>The package to install, None if identical to module name. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ModuleType</code> <p>Imported module</p> Source code in <code>pixano_inference/utils/main.py</code> <pre><code>def attempt_import(module: str, package: str = None) -&gt; ModuleType:\n    \"\"\"Import specified module, or raise ImportError with a helpful message\n\n    Args:\n        module (str): The name of the module to import\n        package (str): The package to install, None if identical to module name. Defaults to None.\n\n    Returns:\n        ModuleType: Imported module\n    \"\"\"\n\n    try:\n        return importlib.import_module(module)\n    except ImportError as e:\n        raise ImportError(\n            f\"Please install {module.split('.')[0]} to use this model: pip install {package or module}\"\n        ) from e\n</code></pre>"},{"location":"getting_started/","title":"Getting started with Pixano Inference","text":"<ul> <li>Installing Pixano Inference</li> <li>Using pre-annotation models</li> <li>Using interactive annotation models</li> </ul>"},{"location":"getting_started/installing_pixano_inference/","title":"Installing Pixano Inference","text":"<p>As Pixano and Pixano Inference require specific versions for their dependencies, we recommend creating a new Python virtual environment to install them.</p> <p>For example, with conda:</p> <pre><code>conda create -n pixano_env python=3.10\nconda activate pixano_env\n</code></pre> <p>Then, you can install the Pixano and Pixano Inference packages inside that environment with pip:</p> <pre><code>pip install pixano\npip install pixano-inference\n</code></pre> <p>To use the inference models available through GitHub, install the following additional packages as needed:</p> <pre><code>python -m pip install segment-anything@git+https://github.com/facebookresearch/segment-anything\npython -m pip install mobile-sam@git+https://github.com/ChaoningZhang/MobileSAM\npython -m pip install groundingdino@git+https://github.com/IDEA-Research/GroundingDINO\n</code></pre>"},{"location":"getting_started/interactive_annotation/","title":"Interactive annotation","text":"<p>Please refer to this Jupyter notebook for information on how to use inference models for interactive annotation.</p>"},{"location":"getting_started/pre_annotation/","title":"Pre-annotation","text":"<p>Please refer to this Jupyter notebook for information on how to use inference models for pre-annotation.</p>"}]}